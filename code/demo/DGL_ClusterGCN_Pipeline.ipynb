{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "J-ITLZtLw8wD",
        "outputId": "9f787ee8-d86b-4742-86c0-5d11864d4afa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.dgl.ai/wheels/repo.html\n",
            "Collecting dgl-cu101\n",
            "  Downloading https://data.dgl.ai/wheels/dgl_cu101-0.8.2-cp37-cp37m-manylinux1_x86_64.whl (149.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 149.8 MB 103 kB/s \n",
            "\u001b[?25hCollecting dglgo\n",
            "  Downloading dglgo-0.0.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 2.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl-cu101) (2.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from dgl-cu101) (4.64.0)\n",
            "Collecting psutil>=5.8.0\n",
            "  Downloading psutil-5.9.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (281 kB)\n",
            "\u001b[K     |████████████████████████████████| 281 kB 23.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu101) (1.21.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu101) (2.23.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu101) (1.4.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu101) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu101) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu101) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu101) (1.24.3)\n",
            "Collecting numpydoc>=1.1.0\n",
            "  Downloading numpydoc-1.3.1-py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 9.0 kB/s \n",
            "\u001b[?25hCollecting ruamel.yaml>=0.17.20\n",
            "  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 76.0 MB/s \n",
            "\u001b[?25hCollecting autopep8>=1.6.0\n",
            "  Downloading autopep8-1.6.0-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.6 MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 81.9 MB/s \n",
            "\u001b[?25hCollecting isort>=5.10.1\n",
            "  Downloading isort-5.10.1-py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 74.2 MB/s \n",
            "\u001b[?25hCollecting pydantic>=1.9.0\n",
            "  Downloading pydantic-1.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.1 MB 79.3 MB/s \n",
            "\u001b[?25hCollecting typer>=0.4.0\n",
            "  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n",
            "Collecting toml\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Collecting pycodestyle>=2.8.0\n",
            "  Downloading pycodestyle-2.8.0-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Jinja2>=2.10 in /usr/local/lib/python3.7/dist-packages (from numpydoc>=1.1.0->dglgo) (2.11.3)\n",
            "Collecting sphinx>=3.0\n",
            "  Downloading Sphinx-5.0.1-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 73.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10->numpydoc>=1.1.0->dglgo) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic>=1.9.0->dglgo) (4.2.0)\n",
            "Collecting ruamel.yaml.clib>=0.2.6\n",
            "  Downloading ruamel.yaml.clib-0.2.6-cp37-cp37m-manylinux1_x86_64.whl (546 kB)\n",
            "\u001b[K     |████████████████████████████████| 546 kB 74.8 MB/s \n",
            "\u001b[?25hCollecting sphinxcontrib-jsmath\n",
            "  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=1.1.0->dglgo) (1.1.5)\n",
            "Collecting sphinxcontrib-applehelp\n",
            "  Downloading sphinxcontrib_applehelp-1.0.2-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[K     |████████████████████████████████| 121 kB 68.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=1.1.0->dglgo) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=1.1.0->dglgo) (2.10.1)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=1.1.0->dglgo) (1.3.0)\n",
            "Collecting sphinxcontrib-htmlhelp>=2.0.0\n",
            "  Downloading sphinxcontrib_htmlhelp-2.0.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[K     |████████████████████████████████| 100 kB 11.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=1.1.0->dglgo) (21.3)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=1.1.0->dglgo) (4.11.4)\n",
            "Collecting sphinxcontrib-devhelp\n",
            "  Downloading sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docutils<0.19,>=0.14 in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=1.1.0->dglgo) (0.17.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=1.1.0->dglgo) (0.7.12)\n",
            "Collecting sphinxcontrib-qthelp\n",
            "  Downloading sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 11.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=1.1.0->dglgo) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel>=1.3->sphinx>=3.0->numpydoc>=1.1.0->dglgo) (2022.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->sphinx>=3.0->numpydoc>=1.1.0->dglgo) (3.8.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer>=0.4.0->dglgo) (7.1.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->sphinx>=3.0->numpydoc>=1.1.0->dglgo) (3.0.9)\n",
            "Installing collected packages: sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, toml, sphinx, ruamel.yaml.clib, pycodestyle, typer, ruamel.yaml, PyYAML, pydantic, psutil, numpydoc, isort, autopep8, dglgo, dgl-cu101\n",
            "  Attempting uninstall: sphinx\n",
            "    Found existing installation: Sphinx 1.8.6\n",
            "    Uninstalling Sphinx-1.8.6:\n",
            "      Successfully uninstalled Sphinx-1.8.6\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed PyYAML-6.0 autopep8-1.6.0 dgl-cu101-0.8.2 dglgo-0.0.1 isort-5.10.1 numpydoc-1.3.1 psutil-5.9.1 pycodestyle-2.8.0 pydantic-1.9.1 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.6 sphinx-5.0.1 sphinxcontrib-applehelp-1.0.2 sphinxcontrib-devhelp-1.0.2 sphinxcontrib-htmlhelp-2.0.0 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.3 toml-0.10.2 typer-0.4.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil",
                  "sphinxcontrib"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install dgl-cu101 dglgo -f https://data.dgl.ai/wheels/repo.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(run the following block instead if there is no GPU available)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AhgQaOPvBIE3"
      },
      "outputs": [],
      "source": [
        "#!pip install dgl dglgo -f https://data.dgl.ai/wheels/repo.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtFeKWIE0KEQ",
        "outputId": "b07d9d24-2e2c-4ee6-e37f-ae262a94a6b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rdflib\n",
            "  Downloading rdflib-6.1.1-py3-none-any.whl (482 kB)\n",
            "\u001b[K     |████████████████████████████████| 482 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from rdflib) (4.11.4)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from rdflib) (3.0.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib) (57.4.0)\n",
            "Collecting isodate\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 695 kB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->rdflib) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->rdflib) (4.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from isodate->rdflib) (1.15.0)\n",
            "Installing collected packages: isodate, rdflib\n",
            "Successfully installed isodate-0.6.1 rdflib-6.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install rdflib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROjTn2-TDAig",
        "outputId": "e8d54385-8773-47f4-87f5-08e60decefb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        }
      ],
      "source": [
        "import dgl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu4OtddP2QAs"
      },
      "source": [
        "## DGI implementation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPCDfiUW5tdm",
        "outputId": "b306d980-949f-47ec-ee4e-bcf38dbcaa58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.7/dist-packages (6.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2022.5.18.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from rdflib) (4.11.4)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.7/dist-packages (from rdflib) (0.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib) (57.4.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from rdflib) (3.0.9)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->rdflib) (3.8.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests torch rdflib pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOKLIUi754mT",
        "outputId": "8d572a57-95f9-43d0-8f4b-4ce188b1c04a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'dgl'...\n",
            "remote: Enumerating objects: 30924, done.\u001b[K\n",
            "remote: Counting objects: 100% (257/257), done.\u001b[K\n",
            "remote: Compressing objects: 100% (206/206), done.\u001b[K\n",
            "remote: Total 30924 (delta 98), reused 146 (delta 47), pack-reused 30667\u001b[K\n",
            "Receiving objects: 100% (30924/30924), 17.87 MiB | 13.60 MiB/s, done.\n",
            "Resolving deltas: 100% (19870/19870), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/dmlc/dgl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVtxJKcy-jq6",
        "outputId": "a41b601b-8b1c-4032-ba9a-ef4da9f7bd03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/dgl/examples/pytorch/rgcn-hetero\n"
          ]
        }
      ],
      "source": [
        "%cd dgl/examples/pytorch/rgcn-hetero/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xG2Vz3Y8xOgF",
        "outputId": "56eb4ba2-517f-4ad5-cee8-ee6e5b1b9a8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/dgl/examples/pytorch/rgcn-hetero\n"
          ]
        }
      ],
      "source": [
        "%cd /content/dgl/examples/pytorch/rgcn-hetero/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "y6XDvJNn3O4N"
      },
      "outputs": [],
      "source": [
        "# import random\n",
        "# random.seed(123)\n",
        "# torch.manual_seed(123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_AtwCpmtGM0",
        "outputId": "7262a003-552e-4e8a-db96-dc4c1e57f62d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.9.0-py3-none-any.whl (418 kB)\n",
            "\u001b[K     |████████████████████████████████| 418 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.11.0+cu113)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uq0MQf1FPUz"
      },
      "source": [
        "### (At this step, please upload the newest version of ecm.py to dgl/examples/pytorch/rgcn-hetero/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLBAjcihGG_y"
      },
      "source": [
        "#### The first time of loading each of the datasets might sometimes trigger some data format problems. For the later experiments, these problems will disappear."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obnUQDPzGsaz",
        "outputId": "0f5bb704-3ab7-4bd9-a067-9516798f0a07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=100, cluster_size=3, data_cpu=False, dataset='aifb', dropout=0, fanout=-1, gpu=0, l2norm=0, lr=0.01, model_path=None, n_bases=-1, n_epochs=15, n_hidden=16, n_layers=2, num_parts=6, use_self_loop=False, validation=False)\n",
            "Downloading /root/.dgl/aifb-hetero.zip from https://data.dgl.ai/dataset/rdf/aifb-hetero.zip...\n",
            "Extracting file to /root/.dgl/aifb-hetero\n",
            "Parsing file aifbfixed_complete.n3 ...\n",
            "Processed 0 tuples, found 0 valid tuples.\n",
            "Processed 10000 tuples, found 8406 valid tuples.\n",
            "Processed 20000 tuples, found 16622 valid tuples.\n",
            "Adding reverse edges ...\n",
            "Creating one whole graph ...\n",
            "Total #nodes: 7262\n",
            "Total #edges: 48810\n",
            "Convert to heterograph ...\n",
            "#Node types: 7\n",
            "#Canonical edge types: 104\n",
            "#Unique edge type names: 78\n",
            "Load training/validation/testing split ...\n",
            "Done saving data into cached files.\n",
            "Traceback (most recent call last):\n",
            "  File \"ecm.py\", line 333, in <module>\n",
            "    main(args)\n",
            "  File \"ecm.py\", line 144, in main\n",
            "    hg = dgl.to_homogeneous(g, ndata=['id', 'train_mask', 'test_mask', 'labels'], edata=['eid'])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/dgl/convert.py\", line 974, in to_homogeneous\n",
            "    comb_nf = combine_frames(G._node_frames, range(len(G.ntypes)), col_names=ndata)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/dgl/heterograph.py\", line 6339, in combine_frames\n",
            "    (key, frame.schemes[key], scheme))\n",
            "dgl._ffi.base.DGLError: Cannot concatenate column train_mask with shape Scheme(shape=(), dtype=torch.bool) and shape Scheme(shape=(), dtype=torch.uint8)\n"
          ]
        }
      ],
      "source": [
        "!python ecm.py -d aifb --testing --gpu 0 --fanout=-1 --n-epochs=15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxYT6rWsHBAO",
        "outputId": "fd9fbbac-d5ea-4fbd-de62-bcfdd3d4cfb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=100, cluster_size=3, data_cpu=False, dataset='mutag', dropout=0, fanout=4, gpu=0, l2norm=0.0005, lr=0.01, model_path=None, n_bases=30, n_epochs=20, n_hidden=16, n_layers=2, num_parts=6, use_self_loop=False, validation=False)\n",
            "Downloading /root/.dgl/mutag-hetero.zip from https://data.dgl.ai/dataset/rdf/mutag-hetero.zip...\n",
            "Extracting file to /root/.dgl/mutag-hetero\n",
            "Parsing file mutag_stripped.nt ...\n",
            "Processed 0 tuples, found 0 valid tuples.\n",
            "Processed 10000 tuples, found 9855 valid tuples.\n",
            "Processed 20000 tuples, found 19855 valid tuples.\n",
            "Processed 30000 tuples, found 29845 valid tuples.\n",
            "Processed 40000 tuples, found 39845 valid tuples.\n",
            "Processed 50000 tuples, found 49845 valid tuples.\n",
            "Processed 60000 tuples, found 59845 valid tuples.\n",
            "Processed 70000 tuples, found 69845 valid tuples.\n",
            "Adding reverse edges ...\n",
            "Creating one whole graph ...\n",
            "Total #nodes: 27163\n",
            "Total #edges: 148100\n",
            "Convert to heterograph ...\n",
            "#Node types: 5\n",
            "#Canonical edge types: 50\n",
            "#Unique edge type names: 46\n",
            "Load training/validation/testing split ...\n",
            "Done saving data into cached files.\n",
            "Traceback (most recent call last):\n",
            "  File \"ecm.py\", line 333, in <module>\n",
            "    main(args)\n",
            "  File \"ecm.py\", line 144, in main\n",
            "    hg = dgl.to_homogeneous(g, ndata=['id', 'train_mask', 'test_mask', 'labels'], edata=['eid'])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/dgl/convert.py\", line 974, in to_homogeneous\n",
            "    comb_nf = combine_frames(G._node_frames, range(len(G.ntypes)), col_names=ndata)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/dgl/heterograph.py\", line 6339, in combine_frames\n",
            "    (key, frame.schemes[key], scheme))\n",
            "dgl._ffi.base.DGLError: Cannot concatenate column train_mask with shape Scheme(shape=(), dtype=torch.bool) and shape Scheme(shape=(), dtype=torch.uint8)\n"
          ]
        }
      ],
      "source": [
        "!python ecm.py -d mutag --l2norm 5e-4 --n-bases 30 --testing --gpu 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ql4Td5uIHrX",
        "outputId": "9d55dd5f-9c98-4fb1-a738-3aea08fba028"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=100, cluster_size=3, data_cpu=False, dataset='bgs', dropout=0, fanout=4, gpu=0, l2norm=0.0005, lr=0.01, model_path=None, n_bases=40, n_epochs=20, n_hidden=16, n_layers=2, num_parts=6, use_self_loop=False, validation=False)\n",
            "Downloading /root/.dgl/bgs-hetero.zip from https://data.dgl.ai/dataset/rdf/bgs-hetero.zip...\n",
            "Extracting file to /root/.dgl/bgs-hetero\n",
            "Parsing file EarthMaterialClass_ComponentRelation.nt ...\n",
            "Parsing file Geochronology_RankList.nt ...\n",
            "Parsing file Lexicon_Stratotype.nt ...\n",
            "Parsing file EarthMaterialClass.nt ...\n",
            "Parsing file Geochronology_Division.nt ...\n",
            "Parsing file Lexicon_Theme.nt ...\n",
            "Parsing file Lexicon_NamedRockUnit.nt ...\n",
            "Parsing file Geochronology.nt ...\n",
            "Parsing file EarthMaterialClass_RockName.nt ...\n",
            "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+^PYSD does not look like a valid URI, trying to serialize this will break.\n",
            "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+^PYSD does not look like a valid URI, trying to serialize this will break.\n",
            "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+#^PRS does not look like a valid URI, trying to serialize this will break.\n",
            "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+#^PRS does not look like a valid URI, trying to serialize this will break.\n",
            "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+#^PRS does not look like a valid URI, trying to serialize this will break.\n",
            "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+#^RSR does not look like a valid URI, trying to serialize this will break.\n",
            "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+#^PRS does not look like a valid URI, trying to serialize this will break.\n",
            "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+#^RSR does not look like a valid URI, trying to serialize this will break.\n",
            "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+#^RSR does not look like a valid URI, trying to serialize this will break.\n",
            "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+#^RSR does not look like a valid URI, trying to serialize this will break.\n",
            "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+#^SSR does not look like a valid URI, trying to serialize this will break.\n",
            "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+#^SSR does not look like a valid URI, trying to serialize this will break.\n",
            "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+^~VIS does not look like a valid URI, trying to serialize this will break.\n",
            "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+^PYSD does not look like a valid URI, trying to serialize this will break.\n",
            "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+#^RSR does not look like a valid URI, trying to serialize this will break.\n",
            "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+#^PRS does not look like a valid URI, trying to serialize this will break.\n",
            "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+#^SSR does not look like a valid URI, trying to serialize this will break.\n",
            "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+#^RSR does not look like a valid URI, trying to serialize this will break.\n",
            "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+#^RSR does not look like a valid URI, trying to serialize this will break.\n",
            "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+^PYSD does not look like a valid URI, trying to serialize this will break.\n",
            "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+^~VIS does not look like a valid URI, trying to serialize this will break.\n",
            "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+^*SSD does not look like a valid URI, trying to serialize this will break.\n",
            "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+^PYSD does not look like a valid URI, trying to serialize this will break.\n",
            "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+^PYSD does not look like a valid URI, trying to serialize this will break.\n",
            "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+^PYSD does not look like a valid URI, trying to serialize this will break.\n",
            "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+^PYSD does not look like a valid URI, trying to serialize this will break.\n",
            "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+^PYSD does not look like a valid URI, trying to serialize this will break.\n",
            "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+^~VIS does not look like a valid URI, trying to serialize this will break.\n",
            "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+^~VIS does not look like a valid URI, trying to serialize this will break.\n",
            "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+^~VIS does not look like a valid URI, trying to serialize this will break.\n",
            "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+^*SSD does not look like a valid URI, trying to serialize this will break.\n",
            "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+^*SSD does not look like a valid URI, trying to serialize this will break.\n",
            "Parsing file EarthMaterialClass_RockComponent.nt ...\n",
            "Parsing file Geochronology_Rank.nt ...\n",
            "Parsing file Lexicon_LithologyComponent.nt ...\n",
            "Parsing file Lexicon_RockUnitRank.nt ...\n",
            "Parsing file 625KGeologyMap_Rank.nt ...\n",
            "Parsing file EarthMaterialClass_RockDummy.nt ...\n",
            "Parsing file 625KGeologyMap_Fault.nt ...\n",
            "Parsing file EarthMaterialClass_ComponentRank.nt ...\n",
            "Parsing file Lexicon_SourceInfo.nt ...\n",
            "Parsing file Lexicon_LithogeneticType.nt ...\n",
            "Parsing file Spatial.nt ...\n",
            "Parsing file 625KGeologyMap_Dyke.nt ...\n",
            "Parsing file Lexicon_StratotypeType.nt ...\n",
            "Parsing file Lexicon.nt ...\n",
            "Parsing file Geochronology_Boundary.nt ...\n",
            "Parsing file Lexicon_ShapeType.nt ...\n",
            "Parsing file Lexicon_SpatialScope.nt ...\n",
            "Parsing file EarthMaterialClass_RockComposite.nt ...\n",
            "Parsing file Geochronology_Scheme.nt ...\n",
            "Parsing file Lexicon_EquivalentName.nt ...\n",
            "Parsing file Geochronology_AgeDeterminationType.nt ...\n",
            "Parsing file 625KGeologyMap_Unit.nt ...\n",
            "Parsing file Lexicon_Class.nt ...\n",
            "Parsing file Lexicon_DefinitionStatus.nt ...\n",
            "Parsing file Geochronology_DivisionList.nt ...\n",
            "Parsing file 625KGeologyMap.nt ...\n",
            "Parsing file Lexicon_EquivalenceType.nt ...\n",
            "Processed 0 tuples, found 0 valid tuples.\n",
            "Processed 10000 tuples, found 4357 valid tuples.\n",
            "Processed 20000 tuples, found 8693 valid tuples.\n",
            "Processed 30000 tuples, found 13042 valid tuples.\n",
            "Processed 40000 tuples, found 15081 valid tuples.\n",
            "Processed 50000 tuples, found 15749 valid tuples.\n",
            "Processed 60000 tuples, found 20125 valid tuples.\n",
            "Processed 70000 tuples, found 24499 valid tuples.\n",
            "Processed 80000 tuples, found 28859 valid tuples.\n",
            "Processed 90000 tuples, found 33222 valid tuples.\n",
            "Processed 100000 tuples, found 37556 valid tuples.\n",
            "Processed 110000 tuples, found 41897 valid tuples.\n",
            "Processed 120000 tuples, found 46237 valid tuples.\n",
            "Processed 130000 tuples, found 50562 valid tuples.\n",
            "Processed 140000 tuples, found 54917 valid tuples.\n",
            "Processed 150000 tuples, found 59231 valid tuples.\n",
            "Processed 160000 tuples, found 63584 valid tuples.\n",
            "Processed 170000 tuples, found 67824 valid tuples.\n",
            "Processed 180000 tuples, found 71516 valid tuples.\n",
            "Processed 190000 tuples, found 73895 valid tuples.\n",
            "Processed 200000 tuples, found 75403 valid tuples.\n",
            "Processed 210000 tuples, found 76995 valid tuples.\n",
            "Processed 220000 tuples, found 78650 valid tuples.\n",
            "Processed 230000 tuples, found 80092 valid tuples.\n",
            "Processed 240000 tuples, found 83088 valid tuples.\n",
            "Processed 250000 tuples, found 88088 valid tuples.\n",
            "Processed 260000 tuples, found 93089 valid tuples.\n",
            "Processed 270000 tuples, found 98088 valid tuples.\n",
            "Processed 280000 tuples, found 103088 valid tuples.\n",
            "Processed 290000 tuples, found 107016 valid tuples.\n",
            "Processed 300000 tuples, found 110350 valid tuples.\n",
            "Processed 310000 tuples, found 113682 valid tuples.\n",
            "Processed 320000 tuples, found 117016 valid tuples.\n",
            "Processed 330000 tuples, found 120350 valid tuples.\n",
            "Processed 340000 tuples, found 123682 valid tuples.\n",
            "Processed 350000 tuples, found 127016 valid tuples.\n",
            "Processed 360000 tuples, found 130350 valid tuples.\n",
            "Processed 370000 tuples, found 133682 valid tuples.\n",
            "Processed 380000 tuples, found 137016 valid tuples.\n",
            "Processed 390000 tuples, found 140350 valid tuples.\n",
            "Processed 400000 tuples, found 143682 valid tuples.\n",
            "Processed 410000 tuples, found 147016 valid tuples.\n",
            "Processed 420000 tuples, found 150719 valid tuples.\n",
            "Processed 430000 tuples, found 156047 valid tuples.\n",
            "Processed 440000 tuples, found 161229 valid tuples.\n",
            "Processed 450000 tuples, found 166320 valid tuples.\n",
            "Processed 460000 tuples, found 171438 valid tuples.\n",
            "Processed 470000 tuples, found 176640 valid tuples.\n",
            "Processed 480000 tuples, found 181918 valid tuples.\n",
            "Processed 490000 tuples, found 187147 valid tuples.\n",
            "Processed 500000 tuples, found 192335 valid tuples.\n",
            "Processed 510000 tuples, found 197477 valid tuples.\n",
            "Processed 520000 tuples, found 202658 valid tuples.\n",
            "Processed 530000 tuples, found 207808 valid tuples.\n",
            "Processed 540000 tuples, found 212961 valid tuples.\n",
            "Processed 550000 tuples, found 218138 valid tuples.\n",
            "Processed 560000 tuples, found 223248 valid tuples.\n",
            "Processed 570000 tuples, found 228396 valid tuples.\n",
            "Processed 580000 tuples, found 233574 valid tuples.\n",
            "Processed 590000 tuples, found 238751 valid tuples.\n",
            "Processed 600000 tuples, found 244002 valid tuples.\n",
            "Processed 610000 tuples, found 249226 valid tuples.\n",
            "Processed 620000 tuples, found 254375 valid tuples.\n",
            "Processed 630000 tuples, found 259451 valid tuples.\n",
            "Processed 640000 tuples, found 265063 valid tuples.\n",
            "Processed 650000 tuples, found 270129 valid tuples.\n",
            "Processed 660000 tuples, found 275215 valid tuples.\n",
            "Processed 670000 tuples, found 280521 valid tuples.\n",
            "Processed 680000 tuples, found 285751 valid tuples.\n",
            "Processed 690000 tuples, found 291078 valid tuples.\n",
            "Processed 700000 tuples, found 296206 valid tuples.\n",
            "Processed 710000 tuples, found 301331 valid tuples.\n",
            "Processed 720000 tuples, found 306456 valid tuples.\n",
            "Processed 730000 tuples, found 311553 valid tuples.\n",
            "Processed 740000 tuples, found 313980 valid tuples.\n",
            "Processed 750000 tuples, found 315980 valid tuples.\n",
            "Processed 760000 tuples, found 317980 valid tuples.\n",
            "Processed 770000 tuples, found 319980 valid tuples.\n",
            "Processed 780000 tuples, found 321980 valid tuples.\n",
            "Processed 790000 tuples, found 323980 valid tuples.\n",
            "Processed 800000 tuples, found 326574 valid tuples.\n",
            "Processed 810000 tuples, found 330574 valid tuples.\n",
            "Processed 820000 tuples, found 334574 valid tuples.\n",
            "Adding reverse edges ...\n",
            "Creating one whole graph ...\n",
            "Total #nodes: 94806\n",
            "Total #edges: 672884\n",
            "Convert to heterograph ...\n",
            "#Node types: 27\n",
            "#Canonical edge types: 122\n",
            "#Unique edge type names: 96\n",
            "Load training/validation/testing split ...\n",
            "Done saving data into cached files.\n",
            "Traceback (most recent call last):\n",
            "  File \"ecm.py\", line 333, in <module>\n",
            "    main(args)\n",
            "  File \"ecm.py\", line 144, in main\n",
            "    hg = dgl.to_homogeneous(g, ndata=['id', 'train_mask', 'test_mask', 'labels'], edata=['eid'])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/dgl/convert.py\", line 974, in to_homogeneous\n",
            "    comb_nf = combine_frames(G._node_frames, range(len(G.ntypes)), col_names=ndata)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/dgl/heterograph.py\", line 6339, in combine_frames\n",
            "    (key, frame.schemes[key], scheme))\n",
            "dgl._ffi.base.DGLError: Cannot concatenate column train_mask with shape Scheme(shape=(), dtype=torch.bool) and shape Scheme(shape=(), dtype=torch.uint8)\n"
          ]
        }
      ],
      "source": [
        "!python ecm.py -d bgs --l2norm 5e-4 --n-bases 40 --testing --gpu 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_XAkzhnPf1W",
        "outputId": "0cae231b-f4f9-4773-c346-b3d70ff44132"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=100, cluster_size=3, data_cpu=False, dataset='am', dropout=0, fanout=-1, gpu=0, l2norm=0.0005, lr=0.01, model_path=None, n_bases=40, n_epochs=20, n_hidden=16, n_layers=2, num_parts=27, use_self_loop=False, validation=False)\n",
            "Downloading /root/.dgl/am-hetero.zip from https://data.dgl.ai/dataset/rdf/am-hetero.zip...\n",
            "Extracting file to /root/.dgl/am-hetero\n",
            "Parsing file am_stripped08.nt ...\n",
            "Parsing file am_stripped03.nt ...\n",
            "Parsing file am_stripped05.nt ...\n",
            "Parsing file am_stripped00.nt ...\n",
            "Parsing file am_stripped06.nt ...\n",
            "Parsing file am_stripped01.nt ...\n",
            "Parsing file am_stripped09.nt ...\n",
            "Parsing file am_stripped04.nt ...\n",
            "Parsing file am_stripped02.nt ...\n",
            "Parsing file am_stripped07.nt ...\n",
            "Processed 0 tuples, found 0 valid tuples.\n",
            "Processed 10000 tuples, found 3872 valid tuples.\n",
            "Processed 20000 tuples, found 7721 valid tuples.\n",
            "Processed 30000 tuples, found 11613 valid tuples.\n",
            "Processed 40000 tuples, found 15392 valid tuples.\n",
            "Processed 50000 tuples, found 19200 valid tuples.\n",
            "Processed 60000 tuples, found 23062 valid tuples.\n",
            "Processed 70000 tuples, found 26883 valid tuples.\n",
            "Processed 80000 tuples, found 30754 valid tuples.\n",
            "Processed 90000 tuples, found 34574 valid tuples.\n",
            "Processed 100000 tuples, found 38411 valid tuples.\n",
            "Processed 110000 tuples, found 42262 valid tuples.\n",
            "Processed 120000 tuples, found 46029 valid tuples.\n",
            "Processed 130000 tuples, found 49881 valid tuples.\n",
            "Processed 140000 tuples, found 53727 valid tuples.\n",
            "Processed 150000 tuples, found 57686 valid tuples.\n",
            "Processed 160000 tuples, found 61532 valid tuples.\n",
            "Processed 170000 tuples, found 65367 valid tuples.\n",
            "Processed 180000 tuples, found 69218 valid tuples.\n",
            "Processed 190000 tuples, found 73019 valid tuples.\n",
            "Processed 200000 tuples, found 76882 valid tuples.\n",
            "Processed 210000 tuples, found 80615 valid tuples.\n",
            "Processed 220000 tuples, found 84426 valid tuples.\n",
            "Processed 230000 tuples, found 88242 valid tuples.\n",
            "Processed 240000 tuples, found 91983 valid tuples.\n",
            "Processed 250000 tuples, found 95879 valid tuples.\n",
            "Processed 260000 tuples, found 99714 valid tuples.\n",
            "Processed 270000 tuples, found 103524 valid tuples.\n",
            "Processed 280000 tuples, found 107296 valid tuples.\n",
            "Processed 290000 tuples, found 111044 valid tuples.\n",
            "Processed 300000 tuples, found 114876 valid tuples.\n",
            "Processed 310000 tuples, found 118769 valid tuples.\n",
            "Processed 320000 tuples, found 122453 valid tuples.\n",
            "Processed 330000 tuples, found 126256 valid tuples.\n",
            "Processed 340000 tuples, found 130062 valid tuples.\n",
            "Processed 350000 tuples, found 133884 valid tuples.\n",
            "Processed 360000 tuples, found 137701 valid tuples.\n",
            "Processed 370000 tuples, found 141498 valid tuples.\n",
            "Processed 380000 tuples, found 145295 valid tuples.\n",
            "Processed 390000 tuples, found 149126 valid tuples.\n",
            "Processed 400000 tuples, found 152939 valid tuples.\n",
            "Processed 410000 tuples, found 156791 valid tuples.\n",
            "Processed 420000 tuples, found 160642 valid tuples.\n",
            "Processed 430000 tuples, found 164508 valid tuples.\n",
            "Processed 440000 tuples, found 168402 valid tuples.\n",
            "Processed 450000 tuples, found 172205 valid tuples.\n",
            "Processed 460000 tuples, found 175985 valid tuples.\n",
            "Processed 470000 tuples, found 179776 valid tuples.\n",
            "Processed 480000 tuples, found 183539 valid tuples.\n",
            "Processed 490000 tuples, found 187292 valid tuples.\n",
            "Processed 500000 tuples, found 191131 valid tuples.\n",
            "Processed 510000 tuples, found 194907 valid tuples.\n",
            "Processed 520000 tuples, found 198630 valid tuples.\n",
            "Processed 530000 tuples, found 202485 valid tuples.\n",
            "Processed 540000 tuples, found 206252 valid tuples.\n",
            "Processed 550000 tuples, found 210095 valid tuples.\n",
            "Processed 560000 tuples, found 213960 valid tuples.\n",
            "Processed 570000 tuples, found 217796 valid tuples.\n",
            "Processed 580000 tuples, found 221630 valid tuples.\n",
            "Processed 590000 tuples, found 225451 valid tuples.\n",
            "Processed 600000 tuples, found 229163 valid tuples.\n",
            "Processed 610000 tuples, found 232989 valid tuples.\n",
            "Processed 620000 tuples, found 236805 valid tuples.\n",
            "Processed 630000 tuples, found 240550 valid tuples.\n",
            "Processed 640000 tuples, found 244399 valid tuples.\n",
            "Processed 650000 tuples, found 248175 valid tuples.\n",
            "Processed 660000 tuples, found 251997 valid tuples.\n",
            "Processed 670000 tuples, found 255852 valid tuples.\n",
            "Processed 680000 tuples, found 259658 valid tuples.\n",
            "Processed 690000 tuples, found 263437 valid tuples.\n",
            "Processed 700000 tuples, found 267249 valid tuples.\n",
            "Processed 710000 tuples, found 271084 valid tuples.\n",
            "Processed 720000 tuples, found 274939 valid tuples.\n",
            "Processed 730000 tuples, found 278753 valid tuples.\n",
            "Processed 740000 tuples, found 282570 valid tuples.\n",
            "Processed 750000 tuples, found 286389 valid tuples.\n",
            "Processed 760000 tuples, found 290218 valid tuples.\n",
            "Processed 770000 tuples, found 294057 valid tuples.\n",
            "Processed 780000 tuples, found 297874 valid tuples.\n",
            "Processed 790000 tuples, found 301735 valid tuples.\n",
            "Processed 800000 tuples, found 305529 valid tuples.\n",
            "Processed 810000 tuples, found 309311 valid tuples.\n",
            "Processed 820000 tuples, found 313101 valid tuples.\n",
            "Processed 830000 tuples, found 316956 valid tuples.\n",
            "Processed 840000 tuples, found 320820 valid tuples.\n",
            "Processed 850000 tuples, found 324607 valid tuples.\n",
            "Processed 860000 tuples, found 328416 valid tuples.\n",
            "Processed 870000 tuples, found 332190 valid tuples.\n",
            "Processed 880000 tuples, found 336036 valid tuples.\n",
            "Processed 890000 tuples, found 339808 valid tuples.\n",
            "Processed 900000 tuples, found 343605 valid tuples.\n",
            "Processed 910000 tuples, found 347399 valid tuples.\n",
            "Processed 920000 tuples, found 351182 valid tuples.\n",
            "Processed 930000 tuples, found 354945 valid tuples.\n",
            "Processed 940000 tuples, found 358768 valid tuples.\n",
            "Processed 950000 tuples, found 362610 valid tuples.\n",
            "Processed 960000 tuples, found 366466 valid tuples.\n",
            "Processed 970000 tuples, found 370306 valid tuples.\n",
            "Processed 980000 tuples, found 374085 valid tuples.\n",
            "Processed 990000 tuples, found 377935 valid tuples.\n",
            "Processed 1000000 tuples, found 381748 valid tuples.\n",
            "Processed 1010000 tuples, found 385636 valid tuples.\n",
            "Processed 1020000 tuples, found 389418 valid tuples.\n",
            "Processed 1030000 tuples, found 393335 valid tuples.\n",
            "Processed 1040000 tuples, found 397245 valid tuples.\n",
            "Processed 1050000 tuples, found 401141 valid tuples.\n",
            "Processed 1060000 tuples, found 404967 valid tuples.\n",
            "Processed 1070000 tuples, found 408707 valid tuples.\n",
            "Processed 1080000 tuples, found 412552 valid tuples.\n",
            "Processed 1090000 tuples, found 416327 valid tuples.\n",
            "Processed 1100000 tuples, found 420121 valid tuples.\n",
            "Processed 1110000 tuples, found 423985 valid tuples.\n",
            "Processed 1120000 tuples, found 427872 valid tuples.\n",
            "Processed 1130000 tuples, found 431652 valid tuples.\n",
            "Processed 1140000 tuples, found 435395 valid tuples.\n",
            "Processed 1150000 tuples, found 439246 valid tuples.\n",
            "Processed 1160000 tuples, found 443057 valid tuples.\n",
            "Processed 1170000 tuples, found 446899 valid tuples.\n",
            "Processed 1180000 tuples, found 450750 valid tuples.\n",
            "Processed 1190000 tuples, found 454524 valid tuples.\n",
            "Processed 1200000 tuples, found 458359 valid tuples.\n",
            "Processed 1210000 tuples, found 462096 valid tuples.\n",
            "Processed 1220000 tuples, found 465902 valid tuples.\n",
            "Processed 1230000 tuples, found 469776 valid tuples.\n",
            "Processed 1240000 tuples, found 473565 valid tuples.\n",
            "Processed 1250000 tuples, found 477347 valid tuples.\n",
            "Processed 1260000 tuples, found 481124 valid tuples.\n",
            "Processed 1270000 tuples, found 484964 valid tuples.\n",
            "Processed 1280000 tuples, found 488815 valid tuples.\n",
            "Processed 1290000 tuples, found 492683 valid tuples.\n",
            "Processed 1300000 tuples, found 496535 valid tuples.\n",
            "Processed 1310000 tuples, found 500406 valid tuples.\n",
            "Processed 1320000 tuples, found 504183 valid tuples.\n",
            "Processed 1330000 tuples, found 507968 valid tuples.\n",
            "Processed 1340000 tuples, found 511825 valid tuples.\n",
            "Processed 1350000 tuples, found 515616 valid tuples.\n",
            "Processed 1360000 tuples, found 519385 valid tuples.\n",
            "Processed 1370000 tuples, found 523231 valid tuples.\n",
            "Processed 1380000 tuples, found 527112 valid tuples.\n",
            "Processed 1390000 tuples, found 530894 valid tuples.\n",
            "Processed 1400000 tuples, found 534645 valid tuples.\n",
            "Processed 1410000 tuples, found 538453 valid tuples.\n",
            "Processed 1420000 tuples, found 542256 valid tuples.\n",
            "Processed 1430000 tuples, found 546064 valid tuples.\n",
            "Processed 1440000 tuples, found 549896 valid tuples.\n",
            "Processed 1450000 tuples, found 553727 valid tuples.\n",
            "Processed 1460000 tuples, found 557502 valid tuples.\n",
            "Processed 1470000 tuples, found 561342 valid tuples.\n",
            "Processed 1480000 tuples, found 565141 valid tuples.\n",
            "Processed 1490000 tuples, found 568937 valid tuples.\n",
            "Processed 1500000 tuples, found 572695 valid tuples.\n",
            "Processed 1510000 tuples, found 576451 valid tuples.\n",
            "Processed 1520000 tuples, found 580253 valid tuples.\n",
            "Processed 1530000 tuples, found 584064 valid tuples.\n",
            "Processed 1540000 tuples, found 587828 valid tuples.\n",
            "Processed 1550000 tuples, found 591637 valid tuples.\n",
            "Processed 1560000 tuples, found 595440 valid tuples.\n",
            "Processed 1570000 tuples, found 599218 valid tuples.\n",
            "Processed 1580000 tuples, found 603099 valid tuples.\n",
            "Processed 1590000 tuples, found 606888 valid tuples.\n",
            "Processed 1600000 tuples, found 610611 valid tuples.\n",
            "Processed 1610000 tuples, found 614447 valid tuples.\n",
            "Processed 1620000 tuples, found 618219 valid tuples.\n",
            "Processed 1630000 tuples, found 622003 valid tuples.\n",
            "Processed 1640000 tuples, found 625808 valid tuples.\n",
            "Processed 1650000 tuples, found 629587 valid tuples.\n",
            "Processed 1660000 tuples, found 633427 valid tuples.\n",
            "Processed 1670000 tuples, found 637243 valid tuples.\n",
            "Processed 1680000 tuples, found 641059 valid tuples.\n",
            "Processed 1690000 tuples, found 644863 valid tuples.\n",
            "Processed 1700000 tuples, found 648717 valid tuples.\n",
            "Processed 1710000 tuples, found 652541 valid tuples.\n",
            "Processed 1720000 tuples, found 656345 valid tuples.\n",
            "Processed 1730000 tuples, found 660177 valid tuples.\n",
            "Processed 1740000 tuples, found 663993 valid tuples.\n",
            "Processed 1750000 tuples, found 667781 valid tuples.\n",
            "Processed 1760000 tuples, found 671604 valid tuples.\n",
            "Processed 1770000 tuples, found 675450 valid tuples.\n",
            "Processed 1780000 tuples, found 679199 valid tuples.\n",
            "Processed 1790000 tuples, found 682995 valid tuples.\n",
            "Processed 1800000 tuples, found 686829 valid tuples.\n",
            "Processed 1810000 tuples, found 690670 valid tuples.\n",
            "Processed 1820000 tuples, found 694499 valid tuples.\n",
            "Processed 1830000 tuples, found 698332 valid tuples.\n",
            "Processed 1840000 tuples, found 702102 valid tuples.\n",
            "Processed 1850000 tuples, found 705909 valid tuples.\n",
            "Processed 1860000 tuples, found 709643 valid tuples.\n",
            "Processed 1870000 tuples, found 713422 valid tuples.\n",
            "Processed 1880000 tuples, found 717253 valid tuples.\n",
            "Processed 1890000 tuples, found 721080 valid tuples.\n",
            "Processed 1900000 tuples, found 724822 valid tuples.\n",
            "Processed 1910000 tuples, found 728655 valid tuples.\n",
            "Processed 1920000 tuples, found 732549 valid tuples.\n",
            "Processed 1930000 tuples, found 736401 valid tuples.\n",
            "Processed 1940000 tuples, found 740248 valid tuples.\n",
            "Processed 1950000 tuples, found 744065 valid tuples.\n",
            "Processed 1960000 tuples, found 747890 valid tuples.\n",
            "Processed 1970000 tuples, found 751684 valid tuples.\n",
            "Processed 1980000 tuples, found 755477 valid tuples.\n",
            "Processed 1990000 tuples, found 759340 valid tuples.\n",
            "Processed 2000000 tuples, found 763104 valid tuples.\n",
            "Processed 2010000 tuples, found 766913 valid tuples.\n",
            "Processed 2020000 tuples, found 770659 valid tuples.\n",
            "Processed 2030000 tuples, found 774453 valid tuples.\n",
            "Processed 2040000 tuples, found 778269 valid tuples.\n",
            "Processed 2050000 tuples, found 782126 valid tuples.\n",
            "Processed 2060000 tuples, found 785992 valid tuples.\n",
            "Processed 2070000 tuples, found 789762 valid tuples.\n",
            "Processed 2080000 tuples, found 793630 valid tuples.\n",
            "Processed 2090000 tuples, found 797524 valid tuples.\n",
            "Processed 2100000 tuples, found 801311 valid tuples.\n",
            "Processed 2110000 tuples, found 805120 valid tuples.\n",
            "Processed 2120000 tuples, found 808885 valid tuples.\n",
            "Processed 2130000 tuples, found 812671 valid tuples.\n",
            "Processed 2140000 tuples, found 816478 valid tuples.\n",
            "Processed 2150000 tuples, found 820334 valid tuples.\n",
            "Processed 2160000 tuples, found 824228 valid tuples.\n",
            "Processed 2170000 tuples, found 828063 valid tuples.\n",
            "Processed 2180000 tuples, found 831899 valid tuples.\n",
            "Processed 2190000 tuples, found 835757 valid tuples.\n",
            "Processed 2200000 tuples, found 839531 valid tuples.\n",
            "Processed 2210000 tuples, found 843298 valid tuples.\n",
            "Processed 2220000 tuples, found 847175 valid tuples.\n",
            "Processed 2230000 tuples, found 850958 valid tuples.\n",
            "Processed 2240000 tuples, found 854756 valid tuples.\n",
            "Processed 2250000 tuples, found 858424 valid tuples.\n",
            "Processed 2260000 tuples, found 862295 valid tuples.\n",
            "Processed 2270000 tuples, found 866132 valid tuples.\n",
            "Processed 2280000 tuples, found 869944 valid tuples.\n",
            "Processed 2290000 tuples, found 873792 valid tuples.\n",
            "Processed 2300000 tuples, found 877565 valid tuples.\n",
            "Processed 2310000 tuples, found 881433 valid tuples.\n",
            "Processed 2320000 tuples, found 885333 valid tuples.\n",
            "Processed 2330000 tuples, found 889205 valid tuples.\n",
            "Processed 2340000 tuples, found 893045 valid tuples.\n",
            "Processed 2350000 tuples, found 896860 valid tuples.\n",
            "Processed 2360000 tuples, found 900669 valid tuples.\n",
            "Processed 2370000 tuples, found 904435 valid tuples.\n",
            "Processed 2380000 tuples, found 908238 valid tuples.\n",
            "Processed 2390000 tuples, found 912133 valid tuples.\n",
            "Processed 2400000 tuples, found 915970 valid tuples.\n",
            "Processed 2410000 tuples, found 919751 valid tuples.\n",
            "Processed 2420000 tuples, found 923604 valid tuples.\n",
            "Processed 2430000 tuples, found 927378 valid tuples.\n",
            "Processed 2440000 tuples, found 931207 valid tuples.\n",
            "Processed 2450000 tuples, found 935003 valid tuples.\n",
            "Processed 2460000 tuples, found 938748 valid tuples.\n",
            "Processed 2470000 tuples, found 942559 valid tuples.\n",
            "Processed 2480000 tuples, found 946368 valid tuples.\n",
            "Processed 2490000 tuples, found 950188 valid tuples.\n",
            "Processed 2500000 tuples, found 953983 valid tuples.\n",
            "Processed 2510000 tuples, found 957817 valid tuples.\n",
            "Processed 2520000 tuples, found 961647 valid tuples.\n",
            "Processed 2530000 tuples, found 965465 valid tuples.\n",
            "Processed 2540000 tuples, found 969312 valid tuples.\n",
            "Processed 2550000 tuples, found 973154 valid tuples.\n",
            "Processed 2560000 tuples, found 977013 valid tuples.\n",
            "Processed 2570000 tuples, found 980838 valid tuples.\n",
            "Processed 2580000 tuples, found 984718 valid tuples.\n",
            "Processed 2590000 tuples, found 988484 valid tuples.\n",
            "Processed 2600000 tuples, found 992360 valid tuples.\n",
            "Processed 2610000 tuples, found 996223 valid tuples.\n",
            "Processed 2620000 tuples, found 999965 valid tuples.\n",
            "Processed 2630000 tuples, found 1003801 valid tuples.\n",
            "Processed 2640000 tuples, found 1007584 valid tuples.\n",
            "Processed 2650000 tuples, found 1011446 valid tuples.\n",
            "Processed 2660000 tuples, found 1015265 valid tuples.\n",
            "Processed 2670000 tuples, found 1019095 valid tuples.\n",
            "Processed 2680000 tuples, found 1022940 valid tuples.\n",
            "Processed 2690000 tuples, found 1026798 valid tuples.\n",
            "Processed 2700000 tuples, found 1030555 valid tuples.\n",
            "Processed 2710000 tuples, found 1034503 valid tuples.\n",
            "Processed 2720000 tuples, found 1038307 valid tuples.\n",
            "Processed 2730000 tuples, found 1042167 valid tuples.\n",
            "Processed 2740000 tuples, found 1046014 valid tuples.\n",
            "Processed 2750000 tuples, found 1049879 valid tuples.\n",
            "Processed 2760000 tuples, found 1053713 valid tuples.\n",
            "Processed 2770000 tuples, found 1057597 valid tuples.\n",
            "Processed 2780000 tuples, found 1061443 valid tuples.\n",
            "Processed 2790000 tuples, found 1065363 valid tuples.\n",
            "Processed 2800000 tuples, found 1069213 valid tuples.\n",
            "Processed 2810000 tuples, found 1073033 valid tuples.\n",
            "Processed 2820000 tuples, found 1076833 valid tuples.\n",
            "Processed 2830000 tuples, found 1080683 valid tuples.\n",
            "Processed 2840000 tuples, found 1084472 valid tuples.\n",
            "Processed 2850000 tuples, found 1088280 valid tuples.\n",
            "Processed 2860000 tuples, found 1092056 valid tuples.\n",
            "Processed 2870000 tuples, found 1095857 valid tuples.\n",
            "Processed 2880000 tuples, found 1099672 valid tuples.\n",
            "Processed 2890000 tuples, found 1103455 valid tuples.\n",
            "Processed 2900000 tuples, found 1107317 valid tuples.\n",
            "Processed 2910000 tuples, found 1111155 valid tuples.\n",
            "Processed 2920000 tuples, found 1115060 valid tuples.\n",
            "Processed 2930000 tuples, found 1118868 valid tuples.\n",
            "Processed 2940000 tuples, found 1122592 valid tuples.\n",
            "Processed 2950000 tuples, found 1126402 valid tuples.\n",
            "Processed 2960000 tuples, found 1130286 valid tuples.\n",
            "Processed 2970000 tuples, found 1134096 valid tuples.\n",
            "Processed 2980000 tuples, found 1137968 valid tuples.\n",
            "Processed 2990000 tuples, found 1141809 valid tuples.\n",
            "Processed 3000000 tuples, found 1145616 valid tuples.\n",
            "Processed 3010000 tuples, found 1149459 valid tuples.\n",
            "Processed 3020000 tuples, found 1153333 valid tuples.\n",
            "Processed 3030000 tuples, found 1157241 valid tuples.\n",
            "Processed 3040000 tuples, found 1161008 valid tuples.\n",
            "Processed 3050000 tuples, found 1164834 valid tuples.\n",
            "Processed 3060000 tuples, found 1168578 valid tuples.\n",
            "Processed 3070000 tuples, found 1172405 valid tuples.\n",
            "Processed 3080000 tuples, found 1176250 valid tuples.\n",
            "Processed 3090000 tuples, found 1176498 valid tuples.\n",
            "Processed 3100000 tuples, found 1176498 valid tuples.\n",
            "Processed 3110000 tuples, found 1176498 valid tuples.\n",
            "Processed 3120000 tuples, found 1176498 valid tuples.\n",
            "Processed 3130000 tuples, found 1176498 valid tuples.\n",
            "Processed 3140000 tuples, found 1176498 valid tuples.\n",
            "Processed 3150000 tuples, found 1176498 valid tuples.\n",
            "Processed 3160000 tuples, found 1176498 valid tuples.\n",
            "Processed 3170000 tuples, found 1176498 valid tuples.\n",
            "Processed 3180000 tuples, found 1176498 valid tuples.\n",
            "Processed 3190000 tuples, found 1176498 valid tuples.\n",
            "Processed 3200000 tuples, found 1176498 valid tuples.\n",
            "Processed 3210000 tuples, found 1176498 valid tuples.\n",
            "Processed 3220000 tuples, found 1176498 valid tuples.\n",
            "Processed 3230000 tuples, found 1177442 valid tuples.\n",
            "Processed 3240000 tuples, found 1181442 valid tuples.\n",
            "Processed 3250000 tuples, found 1185442 valid tuples.\n",
            "Processed 3260000 tuples, found 1189442 valid tuples.\n",
            "Processed 3270000 tuples, found 1193442 valid tuples.\n",
            "Processed 3280000 tuples, found 1197442 valid tuples.\n",
            "Processed 3290000 tuples, found 1201442 valid tuples.\n",
            "Processed 3300000 tuples, found 1205442 valid tuples.\n",
            "Processed 3310000 tuples, found 1209442 valid tuples.\n",
            "Processed 3320000 tuples, found 1213442 valid tuples.\n",
            "Processed 3330000 tuples, found 1217442 valid tuples.\n",
            "Processed 3340000 tuples, found 1221442 valid tuples.\n",
            "Processed 3350000 tuples, found 1225442 valid tuples.\n",
            "Processed 3360000 tuples, found 1229442 valid tuples.\n",
            "Processed 3370000 tuples, found 1233442 valid tuples.\n",
            "Processed 3380000 tuples, found 1237442 valid tuples.\n",
            "Processed 3390000 tuples, found 1241442 valid tuples.\n",
            "Processed 3400000 tuples, found 1245442 valid tuples.\n",
            "Processed 3410000 tuples, found 1249442 valid tuples.\n",
            "Processed 3420000 tuples, found 1253442 valid tuples.\n",
            "Processed 3430000 tuples, found 1257442 valid tuples.\n",
            "Processed 3440000 tuples, found 1261442 valid tuples.\n",
            "Processed 3450000 tuples, found 1265442 valid tuples.\n",
            "Processed 3460000 tuples, found 1269442 valid tuples.\n",
            "Processed 3470000 tuples, found 1273442 valid tuples.\n",
            "Processed 3480000 tuples, found 1277442 valid tuples.\n",
            "Processed 3490000 tuples, found 1281442 valid tuples.\n",
            "Processed 3500000 tuples, found 1285442 valid tuples.\n",
            "Processed 3510000 tuples, found 1289442 valid tuples.\n",
            "Processed 3520000 tuples, found 1293442 valid tuples.\n",
            "Processed 3530000 tuples, found 1297442 valid tuples.\n",
            "Processed 3540000 tuples, found 1301442 valid tuples.\n",
            "Processed 3550000 tuples, found 1305442 valid tuples.\n",
            "Processed 3560000 tuples, found 1309442 valid tuples.\n",
            "Processed 3570000 tuples, found 1313442 valid tuples.\n",
            "Processed 3580000 tuples, found 1317442 valid tuples.\n",
            "Processed 3590000 tuples, found 1321442 valid tuples.\n",
            "Processed 3600000 tuples, found 1324607 valid tuples.\n",
            "Processed 3610000 tuples, found 1327008 valid tuples.\n",
            "Processed 3620000 tuples, found 1329508 valid tuples.\n",
            "Processed 3630000 tuples, found 1331944 valid tuples.\n",
            "Processed 3640000 tuples, found 1334433 valid tuples.\n",
            "Processed 3650000 tuples, found 1338734 valid tuples.\n",
            "Processed 3660000 tuples, found 1341650 valid tuples.\n",
            "Processed 3670000 tuples, found 1343578 valid tuples.\n",
            "Processed 3680000 tuples, found 1345238 valid tuples.\n",
            "Processed 3690000 tuples, found 1346569 valid tuples.\n",
            "Processed 3700000 tuples, found 1348737 valid tuples.\n",
            "Processed 3710000 tuples, found 1353113 valid tuples.\n",
            "Processed 3720000 tuples, found 1354962 valid tuples.\n",
            "Processed 3730000 tuples, found 1357736 valid tuples.\n",
            "Processed 3740000 tuples, found 1360200 valid tuples.\n",
            "Processed 3750000 tuples, found 1362661 valid tuples.\n",
            "Processed 3760000 tuples, found 1365127 valid tuples.\n",
            "Processed 3770000 tuples, found 1367591 valid tuples.\n",
            "Processed 3780000 tuples, found 1370034 valid tuples.\n",
            "Processed 3790000 tuples, found 1372568 valid tuples.\n",
            "Processed 3800000 tuples, found 1374889 valid tuples.\n",
            "Processed 3810000 tuples, found 1377357 valid tuples.\n",
            "Processed 3820000 tuples, found 1379734 valid tuples.\n",
            "Processed 3830000 tuples, found 1382108 valid tuples.\n",
            "Processed 3840000 tuples, found 1384508 valid tuples.\n",
            "Processed 3850000 tuples, found 1387100 valid tuples.\n",
            "Processed 3860000 tuples, found 1389561 valid tuples.\n",
            "Processed 3870000 tuples, found 1391897 valid tuples.\n",
            "Processed 3880000 tuples, found 1394296 valid tuples.\n",
            "Processed 3890000 tuples, found 1396747 valid tuples.\n",
            "Processed 3900000 tuples, found 1399117 valid tuples.\n",
            "Processed 3910000 tuples, found 1400627 valid tuples.\n",
            "Processed 3920000 tuples, found 1400627 valid tuples.\n",
            "Processed 3930000 tuples, found 1400627 valid tuples.\n",
            "Processed 3940000 tuples, found 1400627 valid tuples.\n",
            "Processed 3950000 tuples, found 1400627 valid tuples.\n",
            "Processed 3960000 tuples, found 1400627 valid tuples.\n",
            "Processed 3970000 tuples, found 1400627 valid tuples.\n",
            "Processed 3980000 tuples, found 1400824 valid tuples.\n",
            "Processed 3990000 tuples, found 1407879 valid tuples.\n",
            "Processed 4000000 tuples, found 1415028 valid tuples.\n",
            "Processed 4010000 tuples, found 1422318 valid tuples.\n",
            "Processed 4020000 tuples, found 1429633 valid tuples.\n",
            "Processed 4030000 tuples, found 1437330 valid tuples.\n",
            "Processed 4040000 tuples, found 1445583 valid tuples.\n",
            "Processed 4050000 tuples, found 1453387 valid tuples.\n",
            "Processed 4060000 tuples, found 1461270 valid tuples.\n",
            "Processed 4070000 tuples, found 1468706 valid tuples.\n",
            "Processed 4080000 tuples, found 1476534 valid tuples.\n",
            "Processed 4090000 tuples, found 1483918 valid tuples.\n",
            "Processed 4100000 tuples, found 1491760 valid tuples.\n",
            "Processed 4110000 tuples, found 1499033 valid tuples.\n",
            "Processed 4120000 tuples, found 1506534 valid tuples.\n",
            "Processed 4130000 tuples, found 1514889 valid tuples.\n",
            "Processed 4140000 tuples, found 1522327 valid tuples.\n",
            "Processed 4150000 tuples, found 1529011 valid tuples.\n",
            "Processed 4160000 tuples, found 1535934 valid tuples.\n",
            "Processed 4170000 tuples, found 1542740 valid tuples.\n",
            "Processed 4180000 tuples, found 1549693 valid tuples.\n",
            "Processed 4190000 tuples, found 1555757 valid tuples.\n",
            "Processed 4200000 tuples, found 1561577 valid tuples.\n",
            "Processed 4210000 tuples, found 1567853 valid tuples.\n",
            "Processed 4220000 tuples, found 1574868 valid tuples.\n",
            "Processed 4230000 tuples, found 1581962 valid tuples.\n",
            "Processed 4240000 tuples, found 1589007 valid tuples.\n",
            "Processed 4250000 tuples, found 1597308 valid tuples.\n",
            "Processed 4260000 tuples, found 1604707 valid tuples.\n",
            "Processed 4270000 tuples, found 1612336 valid tuples.\n",
            "Processed 4280000 tuples, found 1619790 valid tuples.\n",
            "Processed 4290000 tuples, found 1626769 valid tuples.\n",
            "Processed 4300000 tuples, found 1634059 valid tuples.\n",
            "Processed 4310000 tuples, found 1641789 valid tuples.\n",
            "Processed 4320000 tuples, found 1649224 valid tuples.\n",
            "Processed 4330000 tuples, found 1656694 valid tuples.\n",
            "Processed 4340000 tuples, found 1664886 valid tuples.\n",
            "Processed 4350000 tuples, found 1672216 valid tuples.\n",
            "Processed 4360000 tuples, found 1679574 valid tuples.\n",
            "Processed 4370000 tuples, found 1686946 valid tuples.\n",
            "Processed 4380000 tuples, found 1694438 valid tuples.\n",
            "Processed 4390000 tuples, found 1701778 valid tuples.\n",
            "Processed 4400000 tuples, found 1709116 valid tuples.\n",
            "Processed 4410000 tuples, found 1717097 valid tuples.\n",
            "Processed 4420000 tuples, found 1724675 valid tuples.\n",
            "Processed 4430000 tuples, found 1732360 valid tuples.\n",
            "Processed 4440000 tuples, found 1739370 valid tuples.\n",
            "Processed 4450000 tuples, found 1746520 valid tuples.\n",
            "Processed 4460000 tuples, found 1753731 valid tuples.\n",
            "Processed 4470000 tuples, found 1761222 valid tuples.\n",
            "Processed 4480000 tuples, found 1768804 valid tuples.\n",
            "Processed 4490000 tuples, found 1776698 valid tuples.\n",
            "Processed 4500000 tuples, found 1784034 valid tuples.\n",
            "Processed 4510000 tuples, found 1791020 valid tuples.\n",
            "Processed 4520000 tuples, found 1798396 valid tuples.\n",
            "Processed 4530000 tuples, found 1806187 valid tuples.\n",
            "Processed 4540000 tuples, found 1813949 valid tuples.\n",
            "Processed 4550000 tuples, found 1823477 valid tuples.\n",
            "Processed 4560000 tuples, found 1832760 valid tuples.\n",
            "Processed 4570000 tuples, found 1840895 valid tuples.\n",
            "Processed 4580000 tuples, found 1848653 valid tuples.\n",
            "Processed 4590000 tuples, found 1856969 valid tuples.\n",
            "Processed 4600000 tuples, found 1866431 valid tuples.\n",
            "Processed 4610000 tuples, found 1875487 valid tuples.\n",
            "Processed 4620000 tuples, found 1882471 valid tuples.\n",
            "Processed 4630000 tuples, found 1890152 valid tuples.\n",
            "Processed 4640000 tuples, found 1897730 valid tuples.\n",
            "Processed 4650000 tuples, found 1905242 valid tuples.\n",
            "Processed 4660000 tuples, found 1912621 valid tuples.\n",
            "Processed 4670000 tuples, found 1920971 valid tuples.\n",
            "Processed 4680000 tuples, found 1928470 valid tuples.\n",
            "Processed 4690000 tuples, found 1935471 valid tuples.\n",
            "Processed 4700000 tuples, found 1942343 valid tuples.\n",
            "Processed 4710000 tuples, found 1948783 valid tuples.\n",
            "Processed 4720000 tuples, found 1955247 valid tuples.\n",
            "Processed 4730000 tuples, found 1961457 valid tuples.\n",
            "Processed 4740000 tuples, found 1967827 valid tuples.\n",
            "Processed 4750000 tuples, found 1974723 valid tuples.\n",
            "Processed 4760000 tuples, found 1981429 valid tuples.\n",
            "Processed 4770000 tuples, found 1988725 valid tuples.\n",
            "Processed 4780000 tuples, found 1996414 valid tuples.\n",
            "Processed 4790000 tuples, found 2004449 valid tuples.\n",
            "Processed 4800000 tuples, found 2012178 valid tuples.\n",
            "Processed 4810000 tuples, found 2020251 valid tuples.\n",
            "Processed 4820000 tuples, found 2028453 valid tuples.\n",
            "Processed 4830000 tuples, found 2036199 valid tuples.\n",
            "Processed 4840000 tuples, found 2043838 valid tuples.\n",
            "Processed 4850000 tuples, found 2051547 valid tuples.\n",
            "Processed 4860000 tuples, found 2059387 valid tuples.\n",
            "Processed 4870000 tuples, found 2067159 valid tuples.\n",
            "Processed 4880000 tuples, found 2074713 valid tuples.\n",
            "Processed 4890000 tuples, found 2081624 valid tuples.\n",
            "Processed 4900000 tuples, found 2088759 valid tuples.\n",
            "Processed 4910000 tuples, found 2096220 valid tuples.\n",
            "Processed 4920000 tuples, found 2103415 valid tuples.\n",
            "Processed 4930000 tuples, found 2109877 valid tuples.\n",
            "Processed 4940000 tuples, found 2117291 valid tuples.\n",
            "Processed 4950000 tuples, found 2124744 valid tuples.\n",
            "Processed 4960000 tuples, found 2131975 valid tuples.\n",
            "Processed 4970000 tuples, found 2139255 valid tuples.\n",
            "Processed 4980000 tuples, found 2146526 valid tuples.\n",
            "Processed 4990000 tuples, found 2154839 valid tuples.\n",
            "Processed 5000000 tuples, found 2163061 valid tuples.\n",
            "Processed 5010000 tuples, found 2170327 valid tuples.\n",
            "Processed 5020000 tuples, found 2176723 valid tuples.\n",
            "Processed 5030000 tuples, found 2183502 valid tuples.\n",
            "Processed 5040000 tuples, found 2190393 valid tuples.\n",
            "Processed 5050000 tuples, found 2198124 valid tuples.\n",
            "Processed 5060000 tuples, found 2205502 valid tuples.\n",
            "Processed 5070000 tuples, found 2212739 valid tuples.\n",
            "Processed 5080000 tuples, found 2220053 valid tuples.\n",
            "Processed 5090000 tuples, found 2227914 valid tuples.\n",
            "Processed 5100000 tuples, found 2235337 valid tuples.\n",
            "Processed 5110000 tuples, found 2242959 valid tuples.\n",
            "Processed 5120000 tuples, found 2250175 valid tuples.\n",
            "Processed 5130000 tuples, found 2257644 valid tuples.\n",
            "Processed 5140000 tuples, found 2265005 valid tuples.\n",
            "Processed 5150000 tuples, found 2272084 valid tuples.\n",
            "Processed 5160000 tuples, found 2279327 valid tuples.\n",
            "Processed 5170000 tuples, found 2286448 valid tuples.\n",
            "Processed 5180000 tuples, found 2293598 valid tuples.\n",
            "Processed 5190000 tuples, found 2300859 valid tuples.\n",
            "Processed 5200000 tuples, found 2308254 valid tuples.\n",
            "Processed 5210000 tuples, found 2315359 valid tuples.\n",
            "Processed 5220000 tuples, found 2322731 valid tuples.\n",
            "Processed 5230000 tuples, found 2329915 valid tuples.\n",
            "Processed 5240000 tuples, found 2337282 valid tuples.\n",
            "Processed 5250000 tuples, found 2344798 valid tuples.\n",
            "Processed 5260000 tuples, found 2352067 valid tuples.\n",
            "Processed 5270000 tuples, found 2359442 valid tuples.\n",
            "Processed 5280000 tuples, found 2366813 valid tuples.\n",
            "Processed 5290000 tuples, found 2374361 valid tuples.\n",
            "Processed 5300000 tuples, found 2382071 valid tuples.\n",
            "Processed 5310000 tuples, found 2389585 valid tuples.\n",
            "Processed 5320000 tuples, found 2396877 valid tuples.\n",
            "Processed 5330000 tuples, found 2404244 valid tuples.\n",
            "Processed 5340000 tuples, found 2411731 valid tuples.\n",
            "Processed 5350000 tuples, found 2419196 valid tuples.\n",
            "Processed 5360000 tuples, found 2426797 valid tuples.\n",
            "Processed 5370000 tuples, found 2434257 valid tuples.\n",
            "Processed 5380000 tuples, found 2441555 valid tuples.\n",
            "Processed 5390000 tuples, found 2448943 valid tuples.\n",
            "Processed 5400000 tuples, found 2456578 valid tuples.\n",
            "Processed 5410000 tuples, found 2464031 valid tuples.\n",
            "Processed 5420000 tuples, found 2471143 valid tuples.\n",
            "Processed 5430000 tuples, found 2478251 valid tuples.\n",
            "Processed 5440000 tuples, found 2485445 valid tuples.\n",
            "Processed 5450000 tuples, found 2492995 valid tuples.\n",
            "Processed 5460000 tuples, found 2500316 valid tuples.\n",
            "Processed 5470000 tuples, found 2507746 valid tuples.\n",
            "Processed 5480000 tuples, found 2515052 valid tuples.\n",
            "Processed 5490000 tuples, found 2522272 valid tuples.\n",
            "Processed 5500000 tuples, found 2529573 valid tuples.\n",
            "Processed 5510000 tuples, found 2537081 valid tuples.\n",
            "Processed 5520000 tuples, found 2544565 valid tuples.\n",
            "Processed 5530000 tuples, found 2552044 valid tuples.\n",
            "Processed 5540000 tuples, found 2559366 valid tuples.\n",
            "Processed 5550000 tuples, found 2566616 valid tuples.\n",
            "Processed 5560000 tuples, found 2574195 valid tuples.\n",
            "Processed 5570000 tuples, found 2579730 valid tuples.\n",
            "Processed 5580000 tuples, found 2585206 valid tuples.\n",
            "Processed 5590000 tuples, found 2592563 valid tuples.\n",
            "Processed 5600000 tuples, found 2600040 valid tuples.\n",
            "Processed 5610000 tuples, found 2607627 valid tuples.\n",
            "Processed 5620000 tuples, found 2615024 valid tuples.\n",
            "Processed 5630000 tuples, found 2622519 valid tuples.\n",
            "Processed 5640000 tuples, found 2629160 valid tuples.\n",
            "Processed 5650000 tuples, found 2635505 valid tuples.\n",
            "Processed 5660000 tuples, found 2642229 valid tuples.\n",
            "Processed 5670000 tuples, found 2648962 valid tuples.\n",
            "Processed 5680000 tuples, found 2655335 valid tuples.\n",
            "Processed 5690000 tuples, found 2661765 valid tuples.\n",
            "Processed 5700000 tuples, found 2668290 valid tuples.\n",
            "Processed 5710000 tuples, found 2674919 valid tuples.\n",
            "Processed 5720000 tuples, found 2681854 valid tuples.\n",
            "Processed 5730000 tuples, found 2689097 valid tuples.\n",
            "Processed 5740000 tuples, found 2696373 valid tuples.\n",
            "Processed 5750000 tuples, found 2703449 valid tuples.\n",
            "Processed 5760000 tuples, found 2710517 valid tuples.\n",
            "Processed 5770000 tuples, found 2717347 valid tuples.\n",
            "Processed 5780000 tuples, found 2724645 valid tuples.\n",
            "Processed 5790000 tuples, found 2731877 valid tuples.\n",
            "Processed 5800000 tuples, found 2740054 valid tuples.\n",
            "Processed 5810000 tuples, found 2747048 valid tuples.\n",
            "Processed 5820000 tuples, found 2754160 valid tuples.\n",
            "Processed 5830000 tuples, found 2760471 valid tuples.\n",
            "Processed 5840000 tuples, found 2765432 valid tuples.\n",
            "Processed 5850000 tuples, found 2770707 valid tuples.\n",
            "Processed 5860000 tuples, found 2775388 valid tuples.\n",
            "Processed 5870000 tuples, found 2779931 valid tuples.\n",
            "Processed 5880000 tuples, found 2784294 valid tuples.\n",
            "Processed 5890000 tuples, found 2788755 valid tuples.\n",
            "Processed 5900000 tuples, found 2793382 valid tuples.\n",
            "Processed 5910000 tuples, found 2798095 valid tuples.\n",
            "Processed 5920000 tuples, found 2803852 valid tuples.\n",
            "Processed 5930000 tuples, found 2808519 valid tuples.\n",
            "Processed 5940000 tuples, found 2812913 valid tuples.\n",
            "Processed 5950000 tuples, found 2817173 valid tuples.\n",
            "Processed 5960000 tuples, found 2821569 valid tuples.\n",
            "Processed 5970000 tuples, found 2826634 valid tuples.\n",
            "Processed 5980000 tuples, found 2831587 valid tuples.\n",
            "Adding reverse edges ...\n",
            "Creating one whole graph ...\n",
            "Total #nodes: 1885136\n",
            "Total #edges: 5668682\n",
            "Convert to heterograph ...\n",
            "tcmalloc: large alloc 1836654592 bytes == 0x6fd3a000 @  0x7fed7194e1e7 0x7fed6f25f0ce 0x7fed6f2b5cf5 0x7fed6f2b5e08 0x7fed6f3750f4 0x7fed6f37830c 0x7fed6f4ff3ac 0x7fed6f4ffe10 0x59588e 0x595b69 0x7fed6f26407d 0x572ade 0x511b33 0x549e0e 0x593fce 0x548ae9 0x5127f1 0x593dd7 0x511e2c 0x549e0e 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549e0e\n",
            "#Node types: 7\n",
            "#Canonical edge types: 108\n",
            "#Unique edge type names: 96\n",
            "Load training/validation/testing split ...\n",
            "Done saving data into cached files.\n",
            "Traceback (most recent call last):\n",
            "  File \"ecm.py\", line 333, in <module>\n",
            "    main(args)\n",
            "  File \"ecm.py\", line 144, in main\n",
            "    hg = dgl.to_homogeneous(g, ndata=['id', 'train_mask', 'test_mask', 'labels'], edata=['eid'])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/dgl/convert.py\", line 974, in to_homogeneous\n",
            "    comb_nf = combine_frames(G._node_frames, range(len(G.ntypes)), col_names=ndata)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/dgl/heterograph.py\", line 6339, in combine_frames\n",
            "    (key, frame.schemes[key], scheme))\n",
            "dgl._ffi.base.DGLError: Cannot concatenate column train_mask with shape Scheme(shape=(), dtype=torch.bool) and shape Scheme(shape=(), dtype=torch.uint8)\n"
          ]
        }
      ],
      "source": [
        "!python ecm.py -d am --l2norm 5e-4 --n-bases 40 --testing --gpu 0 --fanout=-1 --num-parts 27 --cluster-size 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUHLn7bfsbza"
      },
      "source": [
        "### Experiment results using the 4 datasets (with tuned hyperparamenters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2NccbvKakTP",
        "outputId": "ff22225b-738e-4836-b397-72fbf5e92e18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=100, data_cpu=False, dataset='aifb', dropout=0, fanout=4, gpu=0, l2norm=0, lr=0.01, model_path=None, n_bases=-1, n_epochs=17, n_hidden=16, n_layers=2, use_self_loop=False, validation=False)\n",
            "Done loading data from cached files.\n",
            "Count of subgraphs: 2\n",
            "start training...\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.4062 | Train Loss: 1.3116 | Time: 0.3231\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.3947 | Train Loss: 1.3174 | Time: 0.3099\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "Epoch 00000 | Valid Acc: 0.3286 | Valid loss: 1.4084 | Time: nan\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.4111 | Train Loss: 1.2777 | Time: 0.3143\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.1800 | Train Loss: 1.4709 | Time: 0.3276\n",
            "Epoch 00001 | Valid Acc: 0.3714 | Valid loss: 1.3945 | Time: nan\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.3438 | Train Loss: 1.5283 | Time: 0.3182\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.6447 | Train Loss: 1.1803 | Time: 0.3153\n",
            "Epoch 00002 | Valid Acc: 0.4786 | Valid loss: 1.3726 | Time: nan\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.7188 | Train Loss: 1.0845 | Time: 0.3302\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.6842 | Train Loss: 1.1378 | Time: 0.3269\n",
            "Epoch 00003 | Valid Acc: 0.4429 | Valid loss: 1.3599 | Time: nan\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.5922 | Train Loss: 1.1704 | Time: 0.3224\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.6757 | Train Loss: 1.0425 | Time: 0.3232\n",
            "Epoch 00004 | Valid Acc: 0.4571 | Valid loss: 1.3501 | Time: 1.2247\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.8553 | Train Loss: 1.0015 | Time: 0.3280\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.7812 | Train Loss: 0.7148 | Time: 0.3224\n",
            "Epoch 00005 | Valid Acc: 0.4786 | Valid loss: 1.3606 | Time: 1.2289\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.8293 | Train Loss: 0.8114 | Time: 0.3203\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.5152 | Train Loss: 1.2350 | Time: 0.3248\n",
            "Epoch 00006 | Valid Acc: 0.4786 | Valid loss: 1.3497 | Time: 1.2350\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.5376 | Train Loss: 1.1709 | Time: 0.3335\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.8936 | Train Loss: 0.7004 | Time: 0.3175\n",
            "Epoch 00007 | Valid Acc: 0.4786 | Valid loss: 1.3170 | Time: 1.2345\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.8375 | Train Loss: 0.7883 | Time: 0.3258\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.6667 | Train Loss: 0.9421 | Time: 0.3401\n",
            "Epoch 00008 | Valid Acc: 0.5143 | Valid loss: 1.3052 | Time: 1.2416\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.5806 | Train Loss: 1.1516 | Time: 0.3588\n",
            "Epoch 00009 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.3393 | Time: 0.3253\n",
            "Epoch 00009 | Valid Acc: 0.5429 | Valid loss: 1.2375 | Time: 1.2995\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.6364 | Train Loss: 0.9800 | Time: 0.3346\n",
            "Epoch 00010 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.1627 | Time: 0.3161\n",
            "Epoch 00010 | Valid Acc: 0.5786 | Valid loss: 1.2283 | Time: 1.2926\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.8333 | Train Loss: 0.4376 | Time: 0.3292\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.5581 | Train Loss: 1.0612 | Time: 0.3192\n",
            "Epoch 00011 | Valid Acc: 0.6071 | Valid loss: 1.2289 | Time: 1.2845\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.6974 | Train Loss: 0.8149 | Time: 0.3621\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.7812 | Train Loss: 0.4655 | Time: 0.3162\n",
            "Epoch 00012 | Valid Acc: 0.6071 | Valid loss: 1.1757 | Time: 1.2768\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.9756 | Train Loss: 0.3836 | Time: 0.3704\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.8384 | Train Loss: 0.5386 | Time: 0.3317\n",
            "Epoch 00013 | Valid Acc: 0.6286 | Valid loss: 1.1335 | Time: 1.2749\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.7000 | Train Loss: 1.1340 | Time: 0.3889\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.7125 | Train Loss: 0.7469 | Time: 0.2806\n",
            "Epoch 00014 | Valid Acc: 0.6357 | Valid loss: 1.1209 | Time: 1.2697\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.6341 | Train Loss: 0.7193 | Time: 0.3789\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.5859 | Train Loss: 1.4752 | Time: 0.3208\n",
            "Epoch 00015 | Valid Acc: 0.5571 | Valid loss: 1.1423 | Time: 1.2674\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.8835 | Train Loss: 0.4086 | Time: 0.3723\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.8649 | Train Loss: 0.3528 | Time: 0.3119\n",
            "Epoch 00016 | Valid Acc: 0.5429 | Valid loss: 1.1419 | Time: 1.2638\n",
            "\n",
            "Training time: : 1.8975s | Batch time: : 0.3306s\n",
            "100% 122/122 [00:21<00:00,  5.62it/s]\n",
            "100% 122/122 [00:23<00:00,  5.27it/s]\n",
            "Test Acc: 0.5833\n",
            "\n",
            "Total time: : 46.7968s | Batch time: : 0.3306s\n"
          ]
        }
      ],
      "source": [
        "!python ecm.py -d aifb --testing --gpu 0 --fanout=4 --n-epochs=17"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZ_2ymMhv3Ov",
        "outputId": "24926cd5-e369-46a0-ad0a-f528689b69c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=100, data_cpu=False, dataset='aifb', dropout=0, fanout=4, gpu=0, l2norm=0, lr=0.01, model_path=None, n_bases=-1, n_epochs=15, n_hidden=16, n_layers=2, use_self_loop=False, validation=False)\n",
            "Done loading data from cached files.\n",
            "Count of subgraphs: 2\n",
            "start training...\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.3000 | Train Loss: 1.3801 | Time: 0.2692\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.4500 | Train Loss: 1.3517 | Time: 0.3060\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "Epoch 00000 | Valid Acc: 0.4857 | Valid loss: 1.3097 | Time: nan\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.3500 | Train Loss: 1.2627 | Time: 0.3180\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.3125 | Train Loss: 1.2814 | Time: 0.3544\n",
            "Epoch 00001 | Valid Acc: 0.4286 | Valid loss: 1.3432 | Time: nan\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.7297 | Train Loss: 0.9974 | Time: 0.3149\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.3786 | Train Loss: 1.4453 | Time: 0.3781\n",
            "Epoch 00002 | Valid Acc: 0.4000 | Valid loss: 1.3595 | Time: nan\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.2222 | Train Loss: 1.2661 | Time: 0.3282\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.9756 | Train Loss: 0.7005 | Time: 0.3017\n",
            "Epoch 00003 | Valid Acc: 0.4714 | Valid loss: 1.3644 | Time: nan\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.4556 | Train Loss: 1.0431 | Time: 0.3174\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.4400 | Train Loss: 1.9253 | Time: 0.3637\n",
            "Epoch 00004 | Valid Acc: 0.4857 | Valid loss: 1.3805 | Time: 1.2383\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.4141 | Train Loss: 1.4594 | Time: 0.3152\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.9268 | Train Loss: 0.6195 | Time: 0.3126\n",
            "Epoch 00005 | Valid Acc: 0.5071 | Valid loss: 1.3401 | Time: 1.2197\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.4600 | Train Loss: 1.5342 | Time: 0.3205\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.8778 | Train Loss: 0.7498 | Time: 0.3489\n",
            "Epoch 00006 | Valid Acc: 0.5071 | Valid loss: 1.2604 | Time: 1.2131\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.4500 | Train Loss: 1.2787 | Time: 0.3194\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.8125 | Train Loss: 0.8356 | Time: 0.3072\n",
            "Epoch 00007 | Valid Acc: 0.5286 | Valid loss: 1.2003 | Time: 1.1934\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.6667 | Train Loss: 0.9858 | Time: 0.3300\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.7021 | Train Loss: 0.9138 | Time: 0.3665\n",
            "Epoch 00008 | Valid Acc: 0.6143 | Valid loss: 1.1287 | Time: 1.1974\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.6344 | Train Loss: 0.9955 | Time: 0.3221\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.9149 | Train Loss: 0.5677 | Time: 0.3155\n",
            "Epoch 00009 | Valid Acc: 0.6571 | Valid loss: 1.0718 | Time: 1.2040\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.5156 | Train Loss: 1.2185 | Time: 0.3209\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.7368 | Train Loss: 0.8669 | Time: 0.3545\n",
            "Epoch 00010 | Valid Acc: 0.6429 | Valid loss: 1.0694 | Time: 1.2053\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.7805 | Train Loss: 0.6512 | Time: 0.3187\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.7475 | Train Loss: 0.8208 | Time: 0.3245\n",
            "Epoch 00011 | Valid Acc: 0.6500 | Valid loss: 1.0325 | Time: 1.2082\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.4800 | Train Loss: 1.0946 | Time: 0.3133\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.7333 | Train Loss: 0.7882 | Time: 0.3004\n",
            "Epoch 00012 | Valid Acc: 0.6357 | Valid loss: 1.0510 | Time: 1.2066\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.5758 | Train Loss: 1.2322 | Time: 0.3347\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.9512 | Train Loss: 0.3296 | Time: 0.3169\n",
            "Epoch 00013 | Valid Acc: 0.5714 | Valid loss: 1.1005 | Time: 1.2091\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.7021 | Train Loss: 0.7348 | Time: 0.3114\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.7419 | Train Loss: 0.6688 | Time: 0.3165\n",
            "Epoch 00014 | Valid Acc: 0.5214 | Valid loss: 1.1153 | Time: 1.2073\n",
            "\n",
            "Training time: : 1.8708s | Batch time: : 0.3240s\n",
            "100% 43/43 [00:07<00:00,  5.49it/s]\n",
            "100% 43/43 [00:08<00:00,  5.11it/s]\n",
            "Test Acc: 0.5833\n",
            "\n",
            "Total time: : 18.1347s | Batch time: : 0.3240s\n"
          ]
        }
      ],
      "source": [
        "!python ecm.py -d aifb --testing --gpu 0 --fanout=4 --n-epochs=15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ny8A3KJa01jG",
        "outputId": "5abcadd6-0acf-4c45-a551-fe51f3707be8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=100, data_cpu=False, dataset='aifb', dropout=0, fanout=8, gpu=0, l2norm=0, lr=0.01, model_path=None, n_bases=-1, n_epochs=15, n_hidden=16, n_layers=2, use_self_loop=False, validation=False)\n",
            "Done loading data from cached files.\n",
            "Count of subgraphs: 2\n",
            "start training...\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.2556 | Train Loss: 1.5555 | Time: 0.3093\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.3000 | Train Loss: 1.4446 | Time: 0.3188\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "Epoch 00000 | Valid Acc: 0.3643 | Valid loss: 1.4005 | Time: nan\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.4815 | Train Loss: 1.2980 | Time: 0.3152\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.4767 | Train Loss: 1.3459 | Time: 0.3214\n",
            "Epoch 00001 | Valid Acc: 0.4357 | Valid loss: 1.3199 | Time: nan\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.8378 | Train Loss: 1.0398 | Time: 0.3205\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.2427 | Train Loss: 1.4196 | Time: 0.3135\n",
            "Epoch 00002 | Valid Acc: 0.4929 | Valid loss: 1.3024 | Time: nan\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.6486 | Train Loss: 1.1116 | Time: 0.3184\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.5631 | Train Loss: 1.0700 | Time: 0.3194\n",
            "Epoch 00003 | Valid Acc: 0.4786 | Valid loss: 1.3045 | Time: nan\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.4259 | Train Loss: 1.1380 | Time: 0.3271\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.6163 | Train Loss: 0.8747 | Time: 0.3066\n",
            "Epoch 00004 | Valid Acc: 0.5071 | Valid loss: 1.2987 | Time: 1.2180\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.4211 | Train Loss: 1.0454 | Time: 0.3682\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.5938 | Train Loss: 0.9357 | Time: 0.3244\n",
            "Epoch 00005 | Valid Acc: 0.5143 | Valid loss: 1.3060 | Time: 1.2323\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.7674 | Train Loss: 0.7702 | Time: 0.3163\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.4444 | Train Loss: 1.6045 | Time: 0.3223\n",
            "Epoch 00006 | Valid Acc: 0.5286 | Valid loss: 1.2842 | Time: 1.2337\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.7379 | Train Loss: 0.8328 | Time: 0.3267\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.7027 | Train Loss: 0.6237 | Time: 0.3187\n",
            "Epoch 00007 | Valid Acc: 0.5286 | Valid loss: 1.2683 | Time: 1.2337\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.5370 | Train Loss: 1.2789 | Time: 0.3637\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.6512 | Train Loss: 0.8625 | Time: 0.3126\n",
            "Epoch 00008 | Valid Acc: 0.5286 | Valid loss: 1.2063 | Time: 1.2319\n",
            "Epoch 00009 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.4237 | Time: 0.3707\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.5914 | Train Loss: 1.0404 | Time: 0.3242\n",
            "Epoch 00009 | Valid Acc: 0.5500 | Valid loss: 1.1727 | Time: 1.2337\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.7674 | Train Loss: 0.7591 | Time: 0.3157\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.5926 | Train Loss: 1.0606 | Time: 0.3128\n",
            "Epoch 00010 | Valid Acc: 0.5643 | Valid loss: 1.1296 | Time: 1.2313\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.5185 | Train Loss: 1.1821 | Time: 0.3213\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.7674 | Train Loss: 0.7576 | Time: 0.3105\n",
            "Epoch 00011 | Valid Acc: 0.6000 | Valid loss: 1.1149 | Time: 1.2305\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.7791 | Train Loss: 0.7226 | Time: 0.3320\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.9259 | Train Loss: 0.4254 | Time: 0.3078\n",
            "Epoch 00012 | Valid Acc: 0.6071 | Valid loss: 1.1030 | Time: 1.2298\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.6882 | Train Loss: 0.8441 | Time: 0.3250\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.7660 | Train Loss: 0.5752 | Time: 0.3736\n",
            "Epoch 00013 | Valid Acc: 0.6214 | Valid loss: 1.1043 | Time: 1.2306\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.7037 | Train Loss: 0.7420 | Time: 0.3216\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.8256 | Train Loss: 0.6307 | Time: 0.3047\n",
            "Epoch 00014 | Valid Acc: 0.6000 | Valid loss: 1.1223 | Time: 1.2248\n",
            "\n",
            "Training time: : 1.8846s | Batch time: : 0.3247s\n",
            "100% 47/47 [00:08<00:00,  5.51it/s]\n",
            "100% 47/47 [00:08<00:00,  5.33it/s]\n",
            "Test Acc: 0.5833\n",
            "\n",
            "Total time: : 19.2334s | Batch time: : 0.3247s\n"
          ]
        }
      ],
      "source": [
        "!python ecm.py -d aifb --testing --gpu 0 --fanout=8 --n-epochs=15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDylTmzp033K",
        "outputId": "fae411f8-7c69-4851-d4e7-ddf67dde232f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=100, data_cpu=False, dataset='aifb', dropout=0, fanout=16, gpu=0, l2norm=0, lr=0.01, model_path=None, n_bases=-1, n_epochs=20, n_hidden=16, n_layers=2, use_self_loop=False, validation=False)\n",
            "Done loading data from cached files.\n",
            "Count of subgraphs: 2\n",
            "start training...\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.1000 | Train Loss: 1.4070 | Time: 0.2059\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.1167 | Train Loss: 1.4926 | Time: 0.2328\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "Epoch 00000 | Valid Acc: 0.3000 | Valid loss: 1.3038 | Time: nan\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.4000 | Train Loss: 1.2929 | Time: 0.2389\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.4400 | Train Loss: 1.4646 | Time: 0.2417\n",
            "Epoch 00001 | Valid Acc: 0.4714 | Valid loss: 1.1980 | Time: nan\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.4500 | Train Loss: 1.2041 | Time: 0.2074\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.3667 | Train Loss: 1.4150 | Time: 0.2362\n",
            "Epoch 00002 | Valid Acc: 0.5429 | Valid loss: 1.1265 | Time: nan\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.5000 | Train Loss: 1.2187 | Time: 0.2486\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.7105 | Train Loss: 1.0838 | Time: 0.2311\n",
            "Epoch 00003 | Valid Acc: 0.5857 | Valid loss: 1.1036 | Time: nan\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.2766 | Train Loss: 1.2625 | Time: 0.2383\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.3226 | Train Loss: 1.4559 | Time: 0.2392\n",
            "Epoch 00004 | Valid Acc: 0.5286 | Valid loss: 1.1194 | Time: 1.0183\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.3800 | Train Loss: 1.7168 | Time: 0.2465\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.8778 | Train Loss: 0.7881 | Time: 0.2510\n",
            "Epoch 00005 | Valid Acc: 0.4786 | Valid loss: 1.1529 | Time: 1.0232\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.5631 | Train Loss: 1.2013 | Time: 0.2440\n",
            "Epoch 00006 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.4533 | Time: 0.2387\n",
            "Epoch 00006 | Valid Acc: 0.5071 | Valid loss: 1.1803 | Time: 1.0217\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.3737 | Train Loss: 1.3693 | Time: 0.2428\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.8537 | Train Loss: 0.6947 | Time: 0.2333\n",
            "Epoch 00007 | Valid Acc: 0.5143 | Valid loss: 1.1985 | Time: 1.0183\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.6408 | Train Loss: 1.1459 | Time: 0.2423\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.6486 | Train Loss: 0.9310 | Time: 0.2382\n",
            "Epoch 00008 | Valid Acc: 0.5500 | Valid loss: 1.2113 | Time: 1.0177\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.7568 | Train Loss: 0.8753 | Time: 0.2394\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.6214 | Train Loss: 0.9720 | Time: 0.2370\n",
            "Epoch 00009 | Valid Acc: 0.5214 | Valid loss: 1.2024 | Time: 1.0174\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.7097 | Train Loss: 0.9177 | Time: 0.2427\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.8298 | Train Loss: 0.5686 | Time: 0.2405\n",
            "Epoch 00010 | Valid Acc: 0.5000 | Valid loss: 1.1785 | Time: 1.0178\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.7670 | Train Loss: 0.7680 | Time: 0.2451\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.6486 | Train Loss: 1.1063 | Time: 0.2347\n",
            "Epoch 00011 | Valid Acc: 0.4929 | Valid loss: 1.1538 | Time: 1.0179\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.8333 | Train Loss: 0.6427 | Time: 0.2318\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.8400 | Train Loss: 0.5870 | Time: 0.2423\n",
            "Epoch 00012 | Valid Acc: 0.4643 | Valid loss: 1.1533 | Time: 1.0155\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.7447 | Train Loss: 0.6266 | Time: 0.2388\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.5806 | Train Loss: 0.9820 | Time: 0.2388\n",
            "Epoch 00013 | Valid Acc: 0.4786 | Valid loss: 1.1412 | Time: 1.0146\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.6452 | Train Loss: 0.7949 | Time: 0.2415\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.9787 | Train Loss: 0.3657 | Time: 0.2396\n",
            "Epoch 00014 | Valid Acc: 0.4929 | Valid loss: 1.1318 | Time: 1.0141\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.6094 | Train Loss: 0.9424 | Time: 0.2510\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.7632 | Train Loss: 0.6991 | Time: 0.2357\n",
            "Epoch 00015 | Valid Acc: 0.5071 | Valid loss: 1.1220 | Time: 1.0162\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.8444 | Train Loss: 0.5833 | Time: 0.2354\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.7400 | Train Loss: 0.6242 | Time: 0.2445\n",
            "Epoch 00016 | Valid Acc: 0.4929 | Valid loss: 1.1239 | Time: 1.0161\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.8111 | Train Loss: 0.6158 | Time: 0.2364\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.9200 | Train Loss: 0.3669 | Time: 0.2374\n",
            "Epoch 00017 | Valid Acc: 0.5000 | Valid loss: 1.1310 | Time: 1.0162\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.8333 | Train Loss: 0.5318 | Time: 0.2450\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.7791 | Train Loss: 0.7588 | Time: 0.2363\n",
            "Epoch 00018 | Valid Acc: 0.5000 | Valid loss: 1.1556 | Time: 1.0173\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.8625 | Train Loss: 0.4572 | Time: 0.2398\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.5667 | Train Loss: 1.1400 | Time: 0.2387\n",
            "Epoch 00019 | Valid Acc: 0.5000 | Valid loss: 1.1548 | Time: 1.0169\n",
            "\n",
            "Training time: : 1.5207s | Batch time: : 0.2382s\n",
            "100% 92/92 [00:12<00:00,  7.50it/s]\n",
            "100% 92/92 [00:13<00:00,  6.94it/s]\n",
            "Test Acc: 0.5000\n",
            "\n",
            "Total time: : 27.0454s | Batch time: : 0.2382s\n"
          ]
        }
      ],
      "source": [
        "!python ecm.py -d aifb --testing --gpu 0 --fanout=16 --n-epochs=20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcB78PRA-n26",
        "outputId": "7c507636-afb0-4353-d153-5cdd8cbed569"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "293"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bk0hlx1JC9x",
        "outputId": "b8aeedbf-0a43-42d6-a1b6-4ec49ed8343a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=100, cluster_size=3, data_cpu=False, dataset='aifb', dropout=0, fanout=16, gpu=0, l2norm=0, lr=0.01, model_path=None, n_bases=-1, n_epochs=15, n_hidden=16, n_layers=2, num_parts=6, use_self_loop=False, validation=False)\n",
            "Done loading data from cached files.\n",
            "Count of subgraphs: 2\n",
            "start training...\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.2039 | Train Loss: 1.4706 | Time: 0.2383\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.4324 | Train Loss: 1.3078 | Time: 0.2272\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "Epoch 00000 | Valid Acc: 0.2214 | Valid loss: 1.5068 | Time: nan\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.1667 | Train Loss: 1.5066 | Time: 0.2391\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.6125 | Train Loss: 1.2948 | Time: 0.2007\n",
            "Epoch 00001 | Valid Acc: 0.4071 | Valid loss: 1.3878 | Time: nan\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.6163 | Train Loss: 1.2373 | Time: 0.2403\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.2963 | Train Loss: 1.3565 | Time: 0.2333\n",
            "Epoch 00002 | Valid Acc: 0.4714 | Valid loss: 1.3448 | Time: nan\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.7222 | Train Loss: 1.0560 | Time: 0.2357\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.6000 | Train Loss: 0.9885 | Time: 0.2390\n",
            "Epoch 00003 | Valid Acc: 0.4857 | Valid loss: 1.3270 | Time: nan\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.5833 | Train Loss: 1.2008 | Time: 0.2413\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.7500 | Train Loss: 0.9456 | Time: 0.2343\n",
            "Epoch 00004 | Valid Acc: 0.5214 | Valid loss: 1.3337 | Time: 1.0285\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.4844 | Train Loss: 1.2525 | Time: 0.2518\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.7237 | Train Loss: 0.9443 | Time: 0.2414\n",
            "Epoch 00005 | Valid Acc: 0.5286 | Valid loss: 1.3382 | Time: 1.0505\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.8649 | Train Loss: 0.5539 | Time: 0.2409\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.5728 | Train Loss: 0.9843 | Time: 0.2414\n",
            "Epoch 00006 | Valid Acc: 0.5357 | Valid loss: 1.3367 | Time: 1.0577\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.7000 | Train Loss: 0.7790 | Time: 0.2978\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.8375 | Train Loss: 0.7832 | Time: 0.2371\n",
            "Epoch 00007 | Valid Acc: 0.5357 | Valid loss: 1.3456 | Time: 1.0488\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.8000 | Train Loss: 0.7877 | Time: 0.2340\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.5200 | Train Loss: 1.0402 | Time: 0.2392\n",
            "Epoch 00008 | Valid Acc: 0.5429 | Valid loss: 1.3369 | Time: 1.0423\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.7184 | Train Loss: 0.8361 | Time: 0.2929\n",
            "Epoch 00009 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.1433 | Time: 0.2345\n",
            "Epoch 00009 | Valid Acc: 0.5429 | Valid loss: 1.3271 | Time: 1.0394\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.6311 | Train Loss: 0.9198 | Time: 0.2409\n",
            "Epoch 00010 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.1459 | Time: 0.2332\n",
            "Epoch 00010 | Valid Acc: 0.5429 | Valid loss: 1.3230 | Time: 1.0390\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.9833 | Train Loss: 0.2106 | Time: 0.2376\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.8500 | Train Loss: 0.5420 | Time: 0.2289\n",
            "Epoch 00011 | Valid Acc: 0.5429 | Valid loss: 1.3156 | Time: 1.0351\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.7500 | Train Loss: 0.7812 | Time: 0.2879\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.4688 | Train Loss: 1.0947 | Time: 0.2391\n",
            "Epoch 00012 | Valid Acc: 0.5571 | Valid loss: 1.3119 | Time: 1.0348\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.6167 | Train Loss: 0.5846 | Time: 0.2368\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.8500 | Train Loss: 0.4860 | Time: 0.2319\n",
            "Epoch 00013 | Valid Acc: 0.5714 | Valid loss: 1.3156 | Time: 1.0335\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.9730 | Train Loss: 0.1833 | Time: 0.2431\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.7573 | Train Loss: 0.6970 | Time: 0.2371\n",
            "Epoch 00014 | Valid Acc: 0.5786 | Valid loss: 1.3094 | Time: 1.0348\n",
            "\n",
            "Training time: : 1.5809s | Batch time: : 0.2419s\n",
            "100% 40/40 [00:05<00:00,  7.18it/s]\n",
            "100% 40/40 [00:05<00:00,  6.74it/s]\n",
            "Test Acc: 0.5833\n",
            "\n",
            "Total time: : 13.0923s | Batch time: : 0.2419s\n"
          ]
        }
      ],
      "source": [
        "!python ecm.py -d aifb --testing --gpu 0 --fanout=16 --n-epochs=15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-X0NJCATKNPF",
        "outputId": "7e876458-876c-4684-cc63-174f8ebf9b5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=50, cluster_size=2, data_cpu=False, dataset='mutag', dropout=0, fanout=4, gpu=0, l2norm=0.0005, lr=0.01, model_path=None, n_bases=30, n_epochs=20, n_hidden=16, n_layers=2, num_parts=6, use_self_loop=False, validation=False)\n",
            "Done loading data from cached files.\n",
            "Convert a graph into a bidirected graph: 0.006 seconds, peak memory: 12.511 GB\n",
            "Construct multi-constraint weights: 0.000 seconds, peak memory: 12.511 GB\n",
            "[09:27:24] /opt/dgl/src/graph/transform/metis_partition_hetero.cc:87: Partition a graph with 27163 nodes and 150326 edges into 6 parts and get 18812 edge cuts\n",
            "Metis partitioning: 0.033 seconds, peak memory: 12.518 GB\n",
            "Count of subgraphs: 3\n",
            "start training...\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.7143 | Train Loss: 0.6852 | Time: 0.0522\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.5385 | Train Loss: 0.6913 | Time: 0.0564\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.5891 | Train Loss: 0.6785 | Time: 0.1097\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "Epoch 00000 | Valid Acc: 0.6103 | Valid loss: 0.6745 | Time: nan\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.5412 | Train Loss: 0.6677 | Time: 0.0623\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.7586 | Train Loss: 0.5957 | Time: 0.0502\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.5891 | Train Loss: 0.7048 | Time: 0.1150\n",
            "Epoch 00001 | Valid Acc: 0.6103 | Valid loss: 0.6886 | Time: nan\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.7586 | Train Loss: 0.4781 | Time: 0.0539\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.4565 | Train Loss: 0.7458 | Time: 0.0567\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.7049 | Train Loss: 0.5343 | Time: 0.1173\n",
            "Epoch 00002 | Valid Acc: 0.6103 | Valid loss: 0.6924 | Time: nan\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.7440 | Train Loss: 0.5781 | Time: 0.1219\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.6182 | Train Loss: 0.6995 | Time: 0.0468\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.5652 | Train Loss: 0.7098 | Time: 0.0581\n",
            "Epoch 00003 | Valid Acc: 0.6103 | Valid loss: 0.6911 | Time: nan\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.8295 | Train Loss: 0.4430 | Time: 0.1131\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.8103 | Train Loss: 0.2174 | Time: 0.0491\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.7882 | Train Loss: 0.3598 | Time: 0.0548\n",
            "Epoch 00004 | Valid Acc: 0.6029 | Valid loss: 0.6855 | Time: 0.8148\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.6421 | Train Loss: 0.6045 | Time: 0.0506\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.7614 | Train Loss: 0.4952 | Time: 0.0545\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.7978 | Train Loss: 0.3910 | Time: 0.1224\n",
            "Epoch 00005 | Valid Acc: 0.4485 | Valid loss: 0.7154 | Time: 0.7927\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.5517 | Train Loss: 0.6251 | Time: 0.0555\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.9882 | Train Loss: 0.2619 | Time: 0.0560\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.8295 | Train Loss: 0.4246 | Time: 0.1082\n",
            "Epoch 00006 | Valid Acc: 0.4191 | Valid loss: 0.7719 | Time: 0.7803\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.6939 | Train Loss: 0.5735 | Time: 0.0484\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.9551 | Train Loss: 0.1930 | Time: 0.1219\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.7529 | Train Loss: 0.4898 | Time: 0.0572\n",
            "Epoch 00007 | Valid Acc: 0.4485 | Valid loss: 0.7303 | Time: 0.7780\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.7143 | Train Loss: 0.5533 | Time: 0.0495\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.9438 | Train Loss: 0.1425 | Time: 0.1225\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.8235 | Train Loss: 0.3942 | Time: 0.0573\n",
            "Epoch 00008 | Valid Acc: 0.5184 | Valid loss: 0.7239 | Time: 0.7874\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.9318 | Train Loss: 0.1896 | Time: 0.0613\n",
            "Epoch 00009 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0887 | Time: 0.1252\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.9474 | Train Loss: 0.1911 | Time: 0.0476\n",
            "Epoch 00009 | Valid Acc: 0.4669 | Valid loss: 0.7508 | Time: 0.7852\n",
            "Epoch 00010 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.1292 | Time: 0.0541\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.9016 | Train Loss: 0.2810 | Time: 0.1184\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.8804 | Train Loss: 0.2530 | Time: 0.0559\n",
            "Epoch 00010 | Valid Acc: 0.4412 | Valid loss: 0.7728 | Time: 0.7825\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.5323 | Train Loss: 0.9969 | Time: 0.0549\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.8115 | Train Loss: 0.3548 | Time: 0.1164\n",
            "Epoch 00011 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0495 | Time: 0.0559\n",
            "Epoch 00011 | Valid Acc: 0.4412 | Valid loss: 0.8862 | Time: 0.7795\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.7311 | Train Loss: 0.6195 | Time: 0.1299\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.8105 | Train Loss: 0.3556 | Time: 0.0455\n",
            "Epoch 00012 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0400 | Time: 0.0495\n",
            "Epoch 00012 | Valid Acc: 0.4007 | Valid loss: 1.1879 | Time: 0.7825\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.9508 | Train Loss: 0.1600 | Time: 0.1244\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.8571 | Train Loss: 0.3354 | Time: 0.0468\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.6923 | Train Loss: 0.6556 | Time: 0.0558\n",
            "Epoch 00013 | Valid Acc: 0.3934 | Valid loss: 1.1522 | Time: 0.7829\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.6960 | Train Loss: 0.6260 | Time: 0.1255\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.8909 | Train Loss: 0.2674 | Time: 0.0460\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.8913 | Train Loss: 0.2690 | Time: 0.0566\n",
            "Epoch 00014 | Valid Acc: 0.4449 | Valid loss: 0.7794 | Time: 0.7826\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.9070 | Train Loss: 0.2395 | Time: 0.1149\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.7253 | Train Loss: 0.5387 | Time: 0.0464\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.7885 | Train Loss: 0.4501 | Time: 0.0581\n",
            "Epoch 00015 | Valid Acc: 0.5588 | Valid loss: 0.7145 | Time: 0.7862\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.9516 | Train Loss: 0.1717 | Time: 0.0529\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.9294 | Train Loss: 0.2270 | Time: 0.0576\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.7280 | Train Loss: 1.9742 | Time: 0.1199\n",
            "Epoch 00016 | Valid Acc: 0.5993 | Valid loss: 0.7431 | Time: 0.7837\n",
            "Epoch 00017 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.1179 | Time: 0.0604\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.9592 | Train Loss: 0.1513 | Time: 0.0458\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.9326 | Train Loss: 0.1787 | Time: 0.1221\n",
            "Epoch 00017 | Valid Acc: 0.6066 | Valid loss: 0.7338 | Time: 0.7819\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.9040 | Train Loss: 0.2689 | Time: 0.1278\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.9038 | Train Loss: 0.1520 | Time: 0.0564\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.9474 | Train Loss: 0.1682 | Time: 0.0474\n",
            "Epoch 00018 | Valid Acc: 0.5846 | Valid loss: 0.7234 | Time: 0.7813\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.9348 | Train Loss: 0.2414 | Time: 0.0624\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.9231 | Train Loss: 0.1950 | Time: 0.0453\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.7191 | Train Loss: 0.7204 | Time: 0.1259\n",
            "Epoch 00019 | Valid Acc: 0.5441 | Valid loss: 0.7080 | Time: 0.7803\n",
            "\n",
            "Training time: : 1.5874s | Batch time: : 0.0756s\n",
            "100% 13/13 [00:00<00:00, 13.71it/s]\n",
            "100% 13/13 [00:00<00:00, 13.32it/s]\n",
            "Test Acc: 0.6176\n",
            "\n",
            "Total time: : 3.5168s | Batch time: : 0.0756s\n"
          ]
        }
      ],
      "source": [
        "!python ecm.py -d mutag --l2norm 5e-4 --n-bases 30 --testing --gpu 0 --batch-size=50 --fanout=4 --num-parts 6 --cluster-size 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oWgc0IGKJe-",
        "outputId": "b0242f7e-092c-4315-8664-3c67cccafaa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=50, cluster_size=2, data_cpu=False, dataset='mutag', dropout=0, fanout=8, gpu=0, l2norm=0.0005, lr=0.01, model_path=None, n_bases=30, n_epochs=20, n_hidden=16, n_layers=2, num_parts=6, use_self_loop=False, validation=False)\n",
            "Done loading data from cached files.\n",
            "Count of subgraphs: 3\n",
            "start training...\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.5059 | Train Loss: 0.6944 | Time: 0.0624\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.4677 | Train Loss: 0.6962 | Time: 0.0495\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.7280 | Train Loss: 0.6582 | Time: 0.1210\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "Epoch 00000 | Valid Acc: 0.6103 | Valid loss: 0.6746 | Time: nan\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.7586 | Train Loss: 0.6452 | Time: 0.0563\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.5053 | Train Loss: 0.6590 | Time: 0.0474\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.6218 | Train Loss: 0.6413 | Time: 0.1284\n",
            "Epoch 00001 | Valid Acc: 0.6103 | Valid loss: 0.6857 | Time: nan\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.6182 | Train Loss: 0.5192 | Time: 0.0508\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.4674 | Train Loss: 0.6243 | Time: 0.0566\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.7200 | Train Loss: 0.5482 | Time: 0.1239\n",
            "Epoch 00002 | Valid Acc: 0.6103 | Valid loss: 0.7328 | Time: nan\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.6813 | Train Loss: 0.6492 | Time: 0.0529\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.4565 | Train Loss: 0.9235 | Time: 0.0568\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.6966 | Train Loss: 0.6165 | Time: 0.1253\n",
            "Epoch 00003 | Valid Acc: 0.6103 | Valid loss: 0.6862 | Time: nan\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.8545 | Train Loss: 0.3327 | Time: 0.0491\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.9239 | Train Loss: 0.4371 | Time: 0.0572\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.8800 | Train Loss: 0.4194 | Time: 0.1228\n",
            "Epoch 00004 | Valid Acc: 0.6176 | Valid loss: 0.6780 | Time: 0.8361\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.7541 | Train Loss: 0.5056 | Time: 0.1244\n",
            "Epoch 00005 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.2830 | Time: 0.0490\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.7045 | Train Loss: 0.5263 | Time: 0.1089\n",
            "Epoch 00005 | Valid Acc: 0.6140 | Valid loss: 0.6763 | Time: 0.8280\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.5769 | Train Loss: 0.7070 | Time: 0.0617\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.9895 | Train Loss: 0.2508 | Time: 0.0480\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.9920 | Train Loss: 0.2844 | Time: 0.1200\n",
            "Epoch 00006 | Valid Acc: 0.5588 | Valid loss: 0.6837 | Time: 0.8213\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.7040 | Train Loss: 0.5496 | Time: 0.1269\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.8000 | Train Loss: 0.3898 | Time: 0.0475\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.8654 | Train Loss: 0.3811 | Time: 0.0577\n",
            "Epoch 00007 | Valid Acc: 0.5074 | Valid loss: 0.6991 | Time: 0.8174\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.6735 | Train Loss: 0.4987 | Time: 0.0522\n",
            "Epoch 00008 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.1468 | Time: 0.0571\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.9551 | Train Loss: 0.2301 | Time: 0.1254\n",
            "Epoch 00008 | Valid Acc: 0.6029 | Valid loss: 0.6876 | Time: 0.8108\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.8846 | Train Loss: 0.3421 | Time: 0.0597\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.7857 | Train Loss: 0.4483 | Time: 0.0487\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.9180 | Train Loss: 0.1740 | Time: 0.1225\n",
            "Epoch 00009 | Valid Acc: 0.6140 | Valid loss: 0.7302 | Time: 0.8160\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.9684 | Train Loss: 0.1786 | Time: 0.0496\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.7731 | Train Loss: 0.6111 | Time: 0.1279\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.9483 | Train Loss: 0.1998 | Time: 0.0511\n",
            "Epoch 00010 | Valid Acc: 0.6066 | Valid loss: 0.7420 | Time: 0.8100\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.9483 | Train Loss: 0.1514 | Time: 0.0534\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.7059 | Train Loss: 0.4120 | Time: 0.0564\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.7984 | Train Loss: 0.5875 | Time: 0.1125\n",
            "Epoch 00011 | Valid Acc: 0.5294 | Valid loss: 0.7357 | Time: 0.8048\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.8992 | Train Loss: 0.2456 | Time: 0.1844\n",
            "Epoch 00012 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0287 | Time: 0.0472\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.5636 | Train Loss: 0.7295 | Time: 0.0472\n",
            "Epoch 00012 | Valid Acc: 0.4706 | Valid loss: 0.7995 | Time: 0.8088\n",
            "Epoch 00013 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0245 | Time: 0.0614\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.7519 | Train Loss: 0.4873 | Time: 0.1138\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.9890 | Train Loss: 0.0763 | Time: 0.0471\n",
            "Epoch 00013 | Valid Acc: 0.5772 | Valid loss: 0.7442 | Time: 0.8050\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.9888 | Train Loss: 0.0677 | Time: 0.1287\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.9184 | Train Loss: 0.1519 | Time: 0.0474\n",
            "Epoch 00014 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0329 | Time: 0.0577\n",
            "Epoch 00014 | Valid Acc: 0.6140 | Valid loss: 0.8041 | Time: 0.8032\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.9775 | Train Loss: 0.0501 | Time: 0.1308\n",
            "Epoch 00015 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0314 | Time: 0.0481\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.8636 | Train Loss: 0.3955 | Time: 0.0574\n",
            "Epoch 00015 | Valid Acc: 0.6029 | Valid loss: 0.9183 | Time: 0.8029\n",
            "Epoch 00016 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0232 | Time: 0.0548\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.8864 | Train Loss: 0.3301 | Time: 0.1094\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.8539 | Train Loss: 0.3973 | Time: 0.1262\n",
            "Epoch 00016 | Valid Acc: 0.5956 | Valid loss: 0.8983 | Time: 0.8062\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.9147 | Train Loss: 0.1897 | Time: 0.1213\n",
            "Epoch 00017 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0472 | Time: 0.0585\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.9138 | Train Loss: 0.1640 | Time: 0.0510\n",
            "Epoch 00017 | Valid Acc: 0.5699 | Valid loss: 0.7856 | Time: 0.8044\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.9882 | Train Loss: 0.0414 | Time: 0.0623\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.9839 | Train Loss: 0.0662 | Time: 0.0509\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.9760 | Train Loss: 0.0689 | Time: 0.1278\n",
            "Epoch 00018 | Valid Acc: 0.5404 | Valid loss: 0.7685 | Time: 0.8039\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.9839 | Train Loss: 0.0236 | Time: 0.0529\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.8960 | Train Loss: 0.2553 | Time: 0.1212\n",
            "Epoch 00019 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0142 | Time: 0.0644\n",
            "Epoch 00019 | Valid Acc: 0.4779 | Valid loss: 0.7983 | Time: 0.8054\n",
            "\n",
            "Training time: : 1.6584s | Batch time: : 0.0799s\n",
            "100% 8/8 [00:00<00:00, 13.81it/s]\n",
            "100% 8/8 [00:00<00:00, 12.34it/s]\n",
            "Test Acc: 0.6029\n",
            "\n",
            "Total time: : 2.8907s | Batch time: : 0.0799s\n"
          ]
        }
      ],
      "source": [
        "!python ecm.py -d mutag --l2norm 5e-4 --n-bases 30 --testing --gpu 0 --batch-size=50 --fanout=8 --num-parts 6 --cluster-size 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJNT-jp4Jww0",
        "outputId": "79585860-0b95-4c3f-f517-c7586fa509e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=50, cluster_size=5, data_cpu=False, dataset='mutag', dropout=0, fanout=16, gpu=0, l2norm=0.0005, lr=0.01, model_path=None, n_bases=30, n_epochs=20, n_hidden=16, n_layers=2, num_parts=15, use_self_loop=False, validation=False)\n",
            "Done loading data from cached files.\n",
            "Convert a graph into a bidirected graph: 0.005 seconds, peak memory: 12.518 GB\n",
            "Construct multi-constraint weights: 0.000 seconds, peak memory: 12.518 GB\n",
            "[09:28:44] /opt/dgl/src/graph/transform/metis_partition_hetero.cc:87: Partition a graph with 27163 nodes and 150326 edges into 15 parts and get 22384 edge cuts\n",
            "Metis partitioning: 0.037 seconds, peak memory: 12.520 GB\n",
            "Count of subgraphs: 3\n",
            "start training...\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.5619 | Train Loss: 0.6865 | Time: 0.1620\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.5283 | Train Loss: 0.6994 | Time: 0.1037\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.7049 | Train Loss: 0.6512 | Time: 0.0661\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "Epoch 00000 | Valid Acc: 0.6103 | Valid loss: 0.6697 | Time: nan\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.6174 | Train Loss: 0.6757 | Time: 0.1387\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.6500 | Train Loss: 0.6460 | Time: 0.0613\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.5773 | Train Loss: 0.6910 | Time: 0.0495\n",
            "Epoch 00001 | Valid Acc: 0.6103 | Valid loss: 0.6752 | Time: nan\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.5974 | Train Loss: 0.6820 | Time: 0.0480\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.5250 | Train Loss: 0.7234 | Time: 0.1226\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.5652 | Train Loss: 0.6803 | Time: 0.1020\n",
            "Epoch 00002 | Valid Acc: 0.6103 | Valid loss: 0.6751 | Time: nan\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.6203 | Train Loss: 0.6613 | Time: 0.1176\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.6705 | Train Loss: 0.5941 | Time: 0.1288\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.5714 | Train Loss: 0.6499 | Time: 0.0449\n",
            "Epoch 00003 | Valid Acc: 0.6103 | Valid loss: 0.6706 | Time: nan\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.6970 | Train Loss: 0.6304 | Time: 0.1231\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.5882 | Train Loss: 0.6634 | Time: 0.1207\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.5070 | Train Loss: 0.7373 | Time: 0.0498\n",
            "Epoch 00004 | Valid Acc: 0.6103 | Valid loss: 0.6673 | Time: 0.8593\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.5526 | Train Loss: 0.7063 | Time: 0.0609\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.6824 | Train Loss: 0.6536 | Time: 0.1021\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.6126 | Train Loss: 0.6654 | Time: 0.1108\n",
            "Epoch 00005 | Valid Acc: 0.6103 | Valid loss: 0.6701 | Time: 0.8537\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.6667 | Train Loss: 0.6241 | Time: 0.1769\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.5948 | Train Loss: 0.6829 | Time: 0.1065\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.5513 | Train Loss: 0.6665 | Time: 0.0592\n",
            "Epoch 00006 | Valid Acc: 0.6103 | Valid loss: 0.6758 | Time: 0.8761\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.5949 | Train Loss: 0.6445 | Time: 0.0581\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.6238 | Train Loss: 0.6400 | Time: 0.1222\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.5326 | Train Loss: 0.6797 | Time: 0.0427\n",
            "Epoch 00007 | Valid Acc: 0.6103 | Valid loss: 0.6814 | Time: 0.8497\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.6239 | Train Loss: 0.6393 | Time: 0.1204\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.6988 | Train Loss: 0.5953 | Time: 0.1182\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.5417 | Train Loss: 0.6547 | Time: 0.0409\n",
            "Epoch 00008 | Valid Acc: 0.6103 | Valid loss: 0.6804 | Time: 0.8500\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.6796 | Train Loss: 0.6079 | Time: 0.1276\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.5340 | Train Loss: 0.6712 | Time: 0.0478\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.6212 | Train Loss: 0.6687 | Time: 0.1077\n",
            "Epoch 00009 | Valid Acc: 0.6103 | Valid loss: 0.6783 | Time: 0.8498\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.6237 | Train Loss: 0.6649 | Time: 0.1212\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.5977 | Train Loss: 0.6836 | Time: 0.0558\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.6413 | Train Loss: 0.6429 | Time: 0.1040\n",
            "Epoch 00010 | Valid Acc: 0.6140 | Valid loss: 0.6753 | Time: 0.8511\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.6438 | Train Loss: 0.6483 | Time: 0.1239\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.6900 | Train Loss: 0.5925 | Time: 0.1020\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.5354 | Train Loss: 0.7022 | Time: 0.0564\n",
            "Epoch 00011 | Valid Acc: 0.6140 | Valid loss: 0.6773 | Time: 0.8565\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.6875 | Train Loss: 0.5989 | Time: 0.0649\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.6263 | Train Loss: 0.6605 | Time: 0.0477\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.5699 | Train Loss: 0.6829 | Time: 0.1148\n",
            "Epoch 00012 | Valid Acc: 0.6140 | Valid loss: 0.6739 | Time: 0.8474\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.7333 | Train Loss: 0.5882 | Time: 0.0634\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.5802 | Train Loss: 0.6898 | Time: 0.0957\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.6466 | Train Loss: 0.6292 | Time: 0.1074\n",
            "Epoch 00013 | Valid Acc: 0.6140 | Valid loss: 0.6740 | Time: 0.8460\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.6744 | Train Loss: 0.5963 | Time: 0.1014\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.6250 | Train Loss: 0.6783 | Time: 0.0506\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.5976 | Train Loss: 0.6379 | Time: 0.1106\n",
            "Epoch 00014 | Valid Acc: 0.6140 | Valid loss: 0.6735 | Time: 0.8447\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.7241 | Train Loss: 0.5612 | Time: 0.0543\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.5804 | Train Loss: 0.6578 | Time: 0.1084\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.5784 | Train Loss: 0.7033 | Time: 0.1238\n",
            "Epoch 00015 | Valid Acc: 0.6103 | Valid loss: 0.6779 | Time: 0.8457\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.7568 | Train Loss: 0.5665 | Time: 0.1056\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.6512 | Train Loss: 0.6565 | Time: 0.1623\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.5733 | Train Loss: 0.6604 | Time: 0.0602\n",
            "Epoch 00016 | Valid Acc: 0.4963 | Valid loss: 0.6942 | Time: 0.8502\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.7228 | Train Loss: 0.5411 | Time: 0.1267\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.6055 | Train Loss: 0.6118 | Time: 0.1159\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.6452 | Train Loss: 0.6176 | Time: 0.0490\n",
            "Epoch 00017 | Valid Acc: 0.4449 | Valid loss: 0.7054 | Time: 0.8515\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.7356 | Train Loss: 0.5266 | Time: 0.1117\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.7089 | Train Loss: 0.5788 | Time: 0.1148\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.6415 | Train Loss: 0.6551 | Time: 0.0504\n",
            "Epoch 00018 | Valid Acc: 0.5588 | Valid loss: 0.6949 | Time: 0.8500\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.7288 | Train Loss: 0.5426 | Time: 0.1220\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.4355 | Train Loss: 0.7608 | Time: 0.0474\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.6630 | Train Loss: 0.5459 | Time: 0.0553\n",
            "Epoch 00019 | Valid Acc: 0.5919 | Valid loss: 0.6935 | Time: 0.8458\n",
            "\n",
            "Training time: : 1.6313s | Batch time: : 0.0928s\n",
            "100% 8/8 [00:00<00:00, 14.04it/s]\n",
            "100% 8/8 [00:00<00:00, 13.08it/s]\n",
            "Test Acc: 0.5882\n",
            "\n",
            "Total time: : 2.8179s | Batch time: : 0.0928s\n"
          ]
        }
      ],
      "source": [
        "!python ecm.py -d mutag --l2norm 5e-4 --n-bases 30 --testing --gpu 0 --batch-size=50 --fanout=16 --num-parts 15 --cluster-size 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TE9Q1DVGJI2c",
        "outputId": "f6b00b98-098d-40c3-832b-c6e4d647f298"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=100, cluster_size=5, data_cpu=False, dataset='bgs', dropout=0, fanout=4, gpu=0, l2norm=0.0005, lr=0.01, model_path=None, n_bases=40, n_epochs=20, n_hidden=16, n_layers=2, num_parts=10, use_self_loop=False, validation=False)\n",
            "Done loading data from cached files.\n",
            "Convert a graph into a bidirected graph: 0.026 seconds, peak memory: 12.588 GB\n",
            "Construct multi-constraint weights: 0.000 seconds, peak memory: 12.588 GB\n",
            "[09:30:18] /opt/dgl/src/graph/transform/metis_partition_hetero.cc:87: Partition a graph with 94806 nodes and 784348 edges into 10 parts and get 105284 edge cuts\n",
            "Metis partitioning: 0.164 seconds, peak memory: 12.639 GB\n",
            "Count of subgraphs: 2\n",
            "start training...\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.5556 | Train Loss: 0.6501 | Time: 0.2401\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.7778 | Train Loss: 0.5754 | Time: 0.2276\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "Epoch 00000 | Valid Acc: 0.6325 | Valid loss: 1.2394 | Time: nan\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.6216 | Train Loss: 1.1843 | Time: 0.2302\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.8333 | Train Loss: 0.4691 | Time: 0.2239\n",
            "Epoch 00001 | Valid Acc: 0.6239 | Valid loss: 0.8892 | Time: nan\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.4909 | Train Loss: 0.9252 | Time: 0.2498\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.7143 | Train Loss: 0.5606 | Time: 0.2271\n",
            "Epoch 00002 | Valid Acc: 0.6068 | Valid loss: 0.8800 | Time: nan\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.8174 | Train Loss: 0.4094 | Time: 0.2927\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.5000 | Train Loss: 0.6933 | Time: 0.2286\n",
            "Epoch 00003 | Valid Acc: 0.6239 | Valid loss: 0.9786 | Time: nan\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.8000 | Train Loss: 0.5547 | Time: 0.2366\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.6250 | Train Loss: 0.8975 | Time: 0.2371\n",
            "Epoch 00004 | Valid Acc: 0.6239 | Valid loss: 1.0562 | Time: 2.0484\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.6182 | Train Loss: 0.7996 | Time: 0.2385\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.7143 | Train Loss: 0.6705 | Time: 0.2588\n",
            "Epoch 00005 | Valid Acc: 0.6239 | Valid loss: 0.9409 | Time: 2.0594\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.8000 | Train Loss: 0.5270 | Time: 0.2405\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.6250 | Train Loss: 0.8896 | Time: 0.2302\n",
            "Epoch 00006 | Valid Acc: 0.5812 | Valid loss: 0.8250 | Time: 2.0595\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.6139 | Train Loss: 0.6330 | Time: 0.2264\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.7500 | Train Loss: 0.5478 | Time: 0.2394\n",
            "Epoch 00007 | Valid Acc: 0.5470 | Valid loss: 0.7717 | Time: 2.0541\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.8421 | Train Loss: 0.5259 | Time: 0.2354\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.5714 | Train Loss: 0.7023 | Time: 0.2443\n",
            "Epoch 00008 | Valid Acc: 0.5983 | Valid loss: 0.7290 | Time: 2.0604\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.6161 | Train Loss: 0.6309 | Time: 0.2911\n",
            "Epoch 00009 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.2769 | Time: 0.2314\n",
            "Epoch 00009 | Valid Acc: 0.6581 | Valid loss: 0.6880 | Time: 2.0597\n",
            "Epoch 00010 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.2964 | Time: 0.2467\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.8214 | Train Loss: 0.4634 | Time: 0.2476\n",
            "Epoch 00010 | Valid Acc: 0.6325 | Valid loss: 0.6612 | Time: 2.0626\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.8333 | Train Loss: 0.4587 | Time: 0.2345\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.5405 | Train Loss: 0.6787 | Time: 0.2282\n",
            "Epoch 00011 | Valid Acc: 0.6410 | Valid loss: 0.6489 | Time: 2.0613\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.8125 | Train Loss: 0.2962 | Time: 0.2333\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.5743 | Train Loss: 0.7639 | Time: 0.2285\n",
            "Epoch 00012 | Valid Acc: 0.6667 | Valid loss: 0.6354 | Time: 2.0622\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.6535 | Train Loss: 0.7295 | Time: 0.2517\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.6875 | Train Loss: 0.5614 | Time: 0.2825\n",
            "Epoch 00013 | Valid Acc: 0.6667 | Valid loss: 0.6291 | Time: 2.0646\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.7647 | Train Loss: 0.4572 | Time: 0.2338\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.5900 | Train Loss: 0.7270 | Time: 0.2412\n",
            "Epoch 00014 | Valid Acc: 0.6752 | Valid loss: 0.6423 | Time: 2.0637\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.8235 | Train Loss: 0.4287 | Time: 0.2330\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.4800 | Train Loss: 1.0367 | Time: 0.2276\n",
            "Epoch 00015 | Valid Acc: 0.6581 | Valid loss: 0.6547 | Time: 2.0653\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.7500 | Train Loss: 0.5993 | Time: 0.2364\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.7257 | Train Loss: 0.5429 | Time: 0.2255\n",
            "Epoch 00016 | Valid Acc: 0.6581 | Valid loss: 0.6777 | Time: 2.0651\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.8235 | Train Loss: 0.3714 | Time: 0.2443\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.8000 | Train Loss: 0.2839 | Time: 0.2338\n",
            "Epoch 00017 | Valid Acc: 0.6325 | Valid loss: 0.6496 | Time: 2.0651\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.7895 | Train Loss: 0.5109 | Time: 0.2506\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.9286 | Train Loss: 0.1753 | Time: 0.2371\n",
            "Epoch 00018 | Valid Acc: 0.6410 | Valid loss: 0.6322 | Time: 2.0650\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.6518 | Train Loss: 0.6390 | Time: 0.2428\n",
            "Epoch 00019 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.1941 | Time: 0.2383\n",
            "Epoch 00019 | Valid Acc: 0.6239 | Valid loss: 0.6332 | Time: 2.0669\n",
            "\n",
            "Training time: : 2.8033s | Batch time: : 0.2407s\n",
            "100% 11/11 [00:02<00:00,  4.82it/s]\n",
            "100% 11/11 [00:02<00:00,  4.58it/s]\n",
            "Test Acc: 0.6897\n",
            "\n",
            "Total time: : 7.4937s | Batch time: : 0.2407s\n"
          ]
        }
      ],
      "source": [
        "!python ecm.py -d bgs --l2norm 5e-4 --n-bases 40 --testing --gpu 0 --fanout=4 --num-parts 10 --cluster-size 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChejItOaQ4SC",
        "outputId": "f5500028-fce3-43c1-a0d5-25b8aa0e8f2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=100, cluster_size=5, data_cpu=False, dataset='bgs', dropout=0, fanout=8, gpu=0, l2norm=0.0005, lr=0.01, model_path=None, n_bases=40, n_epochs=20, n_hidden=16, n_layers=2, num_parts=10, use_self_loop=False, validation=False)\n",
            "Done loading data from cached files.\n",
            "Count of subgraphs: 2\n",
            "start training...\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.0000 | Train Loss: 0.9325 | Time: 0.2269\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.4425 | Train Loss: 0.7454 | Time: 0.2684\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "Epoch 00000 | Valid Acc: 0.5470 | Valid loss: 1.0918 | Time: nan\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.5333 | Train Loss: 0.7033 | Time: 0.2467\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.5980 | Train Loss: 1.1980 | Time: 0.2349\n",
            "Epoch 00001 | Valid Acc: 0.6239 | Valid loss: 1.4440 | Time: nan\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.6162 | Train Loss: 1.2093 | Time: 0.2423\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.7222 | Train Loss: 0.4837 | Time: 0.2450\n",
            "Epoch 00002 | Valid Acc: 0.6239 | Valid loss: 1.1780 | Time: nan\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.5000 | Train Loss: 0.6463 | Time: 0.2841\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.6372 | Train Loss: 0.8925 | Time: 0.2357\n",
            "Epoch 00003 | Valid Acc: 0.6410 | Valid loss: 0.9375 | Time: nan\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.7500 | Train Loss: 0.5164 | Time: 0.2298\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.5487 | Train Loss: 0.8519 | Time: 0.3135\n",
            "Epoch 00004 | Valid Acc: 0.5128 | Valid loss: 0.8508 | Time: 2.1030\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.7449 | Train Loss: 0.5414 | Time: 0.2457\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.7368 | Train Loss: 0.6358 | Time: 0.2314\n",
            "Epoch 00005 | Valid Acc: 0.4359 | Valid loss: 0.8717 | Time: 2.1000\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.8000 | Train Loss: 0.5562 | Time: 0.2526\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.8039 | Train Loss: 0.5081 | Time: 0.2364\n",
            "Epoch 00006 | Valid Acc: 0.4017 | Valid loss: 0.8976 | Time: 2.1000\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.8667 | Train Loss: 0.2564 | Time: 0.2423\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.4314 | Train Loss: 1.2387 | Time: 0.2692\n",
            "Epoch 00007 | Valid Acc: 0.4872 | Valid loss: 0.8534 | Time: 2.1043\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.7895 | Train Loss: 0.6345 | Time: 0.2444\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.5204 | Train Loss: 1.1817 | Time: 0.2347\n",
            "Epoch 00008 | Valid Acc: 0.5641 | Valid loss: 0.8066 | Time: 2.0979\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.5000 | Train Loss: 0.6406 | Time: 0.2289\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.6372 | Train Loss: 0.6192 | Time: 0.2526\n",
            "Epoch 00009 | Valid Acc: 0.6325 | Valid loss: 0.8743 | Time: 2.0944\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.8367 | Train Loss: 0.2905 | Time: 0.2559\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.8947 | Train Loss: 0.4602 | Time: 0.2248\n",
            "Epoch 00010 | Valid Acc: 0.6325 | Valid loss: 1.0444 | Time: 2.0906\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.6400 | Train Loss: 0.6396 | Time: 0.2542\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.8824 | Train Loss: 0.2765 | Time: 0.2372\n",
            "Epoch 00011 | Valid Acc: 0.6239 | Valid loss: 1.2372 | Time: 2.0910\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.8000 | Train Loss: 0.5626 | Time: 0.2384\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.9018 | Train Loss: 0.2594 | Time: 0.2636\n",
            "Epoch 00012 | Valid Acc: 0.6239 | Valid loss: 1.4242 | Time: 2.0902\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.5000 | Train Loss: 0.6785 | Time: 0.2267\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.6372 | Train Loss: 0.8286 | Time: 0.2507\n",
            "Epoch 00013 | Valid Acc: 0.6239 | Valid loss: 1.5373 | Time: 2.0904\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.8333 | Train Loss: 0.3772 | Time: 0.2720\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.6396 | Train Loss: 0.8353 | Time: 0.2592\n",
            "Epoch 00014 | Valid Acc: 0.6239 | Valid loss: 1.5742 | Time: 2.0902\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.7778 | Train Loss: 0.5928 | Time: 0.2409\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.9091 | Train Loss: 0.1997 | Time: 0.2249\n",
            "Epoch 00015 | Valid Acc: 0.6068 | Valid loss: 1.5345 | Time: 2.0823\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.7456 | Train Loss: 0.4269 | Time: 0.3078\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.6667 | Train Loss: 0.3935 | Time: 0.2331\n",
            "Epoch 00016 | Valid Acc: 0.5641 | Valid loss: 1.4899 | Time: 2.0856\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.6214 | Train Loss: 1.0966 | Time: 0.2461\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.7857 | Train Loss: 0.4785 | Time: 0.2795\n",
            "Epoch 00017 | Valid Acc: 0.5556 | Valid loss: 1.3440 | Time: 2.0830\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.7143 | Train Loss: 0.4754 | Time: 0.2899\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.6000 | Train Loss: 0.6919 | Time: 0.2490\n",
            "Epoch 00018 | Valid Acc: 0.5385 | Valid loss: 1.2454 | Time: 2.0846\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.5500 | Train Loss: 0.9940 | Time: 0.2436\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.8235 | Train Loss: 0.4786 | Time: 0.2348\n",
            "Epoch 00019 | Valid Acc: 0.5385 | Valid loss: 1.1598 | Time: 2.0845\n",
            "\n",
            "Training time: : 2.7984s | Batch time: : 0.2499s\n",
            "100% 10/10 [00:02<00:00,  4.85it/s]\n",
            "100% 10/10 [00:02<00:00,  4.56it/s]\n",
            "Test Acc: 0.6207\n",
            "\n",
            "Total time: : 7.0615s | Batch time: : 0.2499s\n"
          ]
        }
      ],
      "source": [
        "!python ecm.py -d bgs --l2norm 5e-4 --n-bases 40 --testing --gpu 0 --fanout=8 --num-parts 10 --cluster-size 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdxeLnnVJJHX",
        "outputId": "da263c93-3ac1-4dc0-8c03-dd191093273f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=100, cluster_size=5, data_cpu=False, dataset='bgs', dropout=0, fanout=16, gpu=0, l2norm=0.0005, lr=0.01, model_path=None, n_bases=40, n_epochs=20, n_hidden=16, n_layers=2, num_parts=10, use_self_loop=False, validation=False)\n",
            "Done loading data from cached files.\n",
            "Count of subgraphs: 2\n",
            "start training...\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.5405 | Train Loss: 0.7024 | Time: 0.2640\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.1667 | Train Loss: 0.7215 | Time: 0.2633\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "Epoch 00000 | Valid Acc: 0.6325 | Valid loss: 1.3447 | Time: nan\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.8333 | Train Loss: 0.3991 | Time: 0.2528\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.6216 | Train Loss: 1.3093 | Time: 0.2391\n",
            "Epoch 00001 | Valid Acc: 0.6325 | Valid loss: 1.8654 | Time: nan\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.6061 | Train Loss: 1.0801 | Time: 0.3073\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.7778 | Train Loss: 0.6290 | Time: 0.2296\n",
            "Epoch 00002 | Valid Acc: 0.6325 | Valid loss: 1.4901 | Time: nan\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.6195 | Train Loss: 0.7694 | Time: 0.2533\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.5000 | Train Loss: 0.6877 | Time: 0.2772\n",
            "Epoch 00003 | Valid Acc: 0.6410 | Valid loss: 1.1113 | Time: nan\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.6667 | Train Loss: 0.4565 | Time: 0.2291\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.6228 | Train Loss: 0.7189 | Time: 0.2391\n",
            "Epoch 00004 | Valid Acc: 0.6325 | Valid loss: 0.9025 | Time: 2.0914\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.6667 | Train Loss: 0.6015 | Time: 0.2832\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.5766 | Train Loss: 0.6530 | Time: 0.2666\n",
            "Epoch 00005 | Valid Acc: 0.6154 | Valid loss: 0.8327 | Time: 2.0888\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.6667 | Train Loss: 0.6088 | Time: 0.2617\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.5980 | Train Loss: 0.6372 | Time: 0.2318\n",
            "Epoch 00006 | Valid Acc: 0.5897 | Valid loss: 0.8045 | Time: 2.0657\n",
            "Epoch 00007 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.3583 | Time: 0.2375\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.5179 | Train Loss: 0.7797 | Time: 0.2390\n",
            "Epoch 00007 | Valid Acc: 0.5641 | Valid loss: 0.8127 | Time: 2.0683\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.8750 | Train Loss: 0.3775 | Time: 0.2398\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.4037 | Train Loss: 1.0102 | Time: 0.2533\n",
            "Epoch 00008 | Valid Acc: 0.5812 | Valid loss: 0.9203 | Time: 2.0731\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.7157 | Train Loss: 0.5294 | Time: 0.3044\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.6000 | Train Loss: 0.6286 | Time: 0.2346\n",
            "Epoch 00009 | Valid Acc: 0.5983 | Valid loss: 1.0764 | Time: 2.0716\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.5000 | Train Loss: 0.7828 | Time: 0.2479\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.8421 | Train Loss: 0.4050 | Time: 0.2463\n",
            "Epoch 00010 | Valid Acc: 0.6154 | Valid loss: 1.2894 | Time: 2.0784\n",
            "Epoch 00011 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.1133 | Time: 0.2278\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.5766 | Train Loss: 0.7047 | Time: 0.2482\n",
            "Epoch 00011 | Valid Acc: 0.6239 | Valid loss: 1.5252 | Time: 2.0763\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.6875 | Train Loss: 0.6936 | Time: 0.2525\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.7129 | Train Loss: 0.6853 | Time: 0.2524\n",
            "Epoch 00012 | Valid Acc: 0.6154 | Valid loss: 1.6161 | Time: 2.0812\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.8000 | Train Loss: 0.5646 | Time: 0.2558\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.6275 | Train Loss: 1.2819 | Time: 0.2931\n",
            "Epoch 00013 | Valid Acc: 0.6068 | Valid loss: 1.4584 | Time: 2.0819\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.8125 | Train Loss: 0.4885 | Time: 0.2472\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.6634 | Train Loss: 0.6426 | Time: 0.2534\n",
            "Epoch 00014 | Valid Acc: 0.5726 | Valid loss: 1.2110 | Time: 2.0849\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.7500 | Train Loss: 0.6248 | Time: 0.2427\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.6634 | Train Loss: 0.6455 | Time: 0.2215\n",
            "Epoch 00015 | Valid Acc: 0.5641 | Valid loss: 1.0105 | Time: 2.0843\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.6071 | Train Loss: 0.6655 | Time: 0.2421\n",
            "Epoch 00016 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.1813 | Time: 0.2422\n",
            "Epoch 00016 | Valid Acc: 0.5726 | Valid loss: 0.8898 | Time: 2.0826\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.3333 | Train Loss: 0.8428 | Time: 0.2974\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.6768 | Train Loss: 0.6554 | Time: 0.2439\n",
            "Epoch 00017 | Valid Acc: 0.5726 | Valid loss: 0.8572 | Time: 2.0851\n",
            "Epoch 00018 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.1076 | Time: 0.2501\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.5893 | Train Loss: 0.7599 | Time: 0.2639\n",
            "Epoch 00018 | Valid Acc: 0.5641 | Valid loss: 0.8692 | Time: 2.0875\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.6842 | Train Loss: 0.7084 | Time: 0.2425\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.6429 | Train Loss: 0.6594 | Time: 0.2441\n",
            "Epoch 00019 | Valid Acc: 0.5641 | Valid loss: 0.8693 | Time: 2.0871\n",
            "\n",
            "Training time: : 2.8150s | Batch time: : 0.2530s\n",
            "100% 13/13 [00:02<00:00,  4.86it/s]\n",
            "100% 13/13 [00:02<00:00,  4.52it/s]\n",
            "Test Acc: 0.6552\n",
            "\n",
            "Total time: : 8.3777s | Batch time: : 0.2530s\n"
          ]
        }
      ],
      "source": [
        "!python ecm.py -d bgs --l2norm 5e-4 --n-bases 40 --testing --gpu 0 --fanout=16 --num-parts 10 --cluster-size 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZU-Afee-2RBK",
        "outputId": "41cb1e1a-e643-4e35-ee7a-72fc08aae0d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "579"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpD52ZLOJf3x",
        "outputId": "aa5d15b1-c381-4fec-d36e-3f33e3df10d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=50, cluster_size=5, data_cpu=False, dataset='mutag', dropout=0, fanout=4, gpu=0, l2norm=0.0005, lr=0.01, model_path=None, n_bases=30, n_epochs=20, n_hidden=16, n_layers=2, num_parts=15, use_self_loop=False, validation=False)\n",
            "Done loading data from cached files.\n",
            "Count of subgraphs: 3\n",
            "start training...\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.5714 | Train Loss: 0.6894 | Time: 0.0598\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.6091 | Train Loss: 0.6811 | Time: 0.1192\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.6061 | Train Loss: 0.6739 | Time: 0.1058\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "Epoch 00000 | Valid Acc: 0.6103 | Valid loss: 0.6705 | Time: nan\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.5417 | Train Loss: 0.6905 | Time: 0.0480\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.5972 | Train Loss: 0.6684 | Time: 0.0591\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.6562 | Train Loss: 0.6568 | Time: 0.1245\n",
            "Epoch 00001 | Valid Acc: 0.6103 | Valid loss: 0.6747 | Time: nan\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.5663 | Train Loss: 0.6965 | Time: 0.1156\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.5800 | Train Loss: 0.6779 | Time: 0.0477\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.6966 | Train Loss: 0.6401 | Time: 0.1256\n",
            "Epoch 00002 | Valid Acc: 0.6103 | Valid loss: 0.6753 | Time: nan\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.6471 | Train Loss: 0.6626 | Time: 0.0664\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.5049 | Train Loss: 0.7085 | Time: 0.1055\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.6119 | Train Loss: 0.6691 | Time: 0.1114\n",
            "Epoch 00003 | Valid Acc: 0.6103 | Valid loss: 0.6720 | Time: nan\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.6667 | Train Loss: 0.6593 | Time: 0.1321\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.5190 | Train Loss: 0.7151 | Time: 0.0435\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.6200 | Train Loss: 0.6687 | Time: 0.1129\n",
            "Epoch 00004 | Valid Acc: 0.6103 | Valid loss: 0.6704 | Time: 0.8556\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.6176 | Train Loss: 0.6426 | Time: 0.1095\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.5833 | Train Loss: 0.6225 | Time: 0.0504\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.6182 | Train Loss: 0.6614 | Time: 0.1209\n",
            "Epoch 00005 | Valid Acc: 0.6103 | Valid loss: 0.6707 | Time: 0.8777\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.5652 | Train Loss: 0.6574 | Time: 0.1172\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.5273 | Train Loss: 0.6809 | Time: 0.0493\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.4314 | Train Loss: 0.7773 | Time: 0.1286\n",
            "Epoch 00006 | Valid Acc: 0.6103 | Valid loss: 0.6722 | Time: 0.8688\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.6465 | Train Loss: 0.6304 | Time: 0.1291\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.6064 | Train Loss: 0.6764 | Time: 0.0468\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.5696 | Train Loss: 0.6610 | Time: 0.0501\n",
            "Epoch 00007 | Valid Acc: 0.6103 | Valid loss: 0.6747 | Time: 0.8411\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.5050 | Train Loss: 0.6873 | Time: 0.0507\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.7222 | Train Loss: 0.5991 | Time: 0.1323\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.6790 | Train Loss: 0.6209 | Time: 0.1278\n",
            "Epoch 00008 | Valid Acc: 0.6103 | Valid loss: 0.6768 | Time: 0.8522\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.7059 | Train Loss: 0.5750 | Time: 0.1155\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.5823 | Train Loss: 0.6757 | Time: 0.0497\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.5833 | Train Loss: 0.6727 | Time: 0.1155\n",
            "Epoch 00009 | Valid Acc: 0.6103 | Valid loss: 0.6803 | Time: 0.8509\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.5385 | Train Loss: 0.6426 | Time: 0.1079\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.6250 | Train Loss: 0.6078 | Time: 0.1126\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.6883 | Train Loss: 0.5461 | Time: 0.1290\n",
            "Epoch 00010 | Valid Acc: 0.6103 | Valid loss: 0.6732 | Time: 0.8593\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.5978 | Train Loss: 0.6600 | Time: 0.0549\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.6579 | Train Loss: 0.6352 | Time: 0.0595\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.6538 | Train Loss: 0.6252 | Time: 0.1132\n",
            "Epoch 00011 | Valid Acc: 0.5919 | Valid loss: 0.6781 | Time: 0.8462\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.6575 | Train Loss: 0.6065 | Time: 0.0532\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.5604 | Train Loss: 0.7137 | Time: 0.1111\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.7222 | Train Loss: 0.5658 | Time: 0.1314\n",
            "Epoch 00012 | Valid Acc: 0.4596 | Valid loss: 0.7024 | Time: 0.8472\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.7345 | Train Loss: 0.5071 | Time: 0.1366\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.5143 | Train Loss: 0.6928 | Time: 0.0983\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.5000 | Train Loss: 0.7501 | Time: 0.0499\n",
            "Epoch 00013 | Valid Acc: 0.4375 | Valid loss: 0.7342 | Time: 0.8469\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.6506 | Train Loss: 0.6185 | Time: 0.0561\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.5133 | Train Loss: 0.7332 | Time: 0.1169\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.5526 | Train Loss: 0.6816 | Time: 0.0572\n",
            "Epoch 00014 | Valid Acc: 0.4890 | Valid loss: 0.6992 | Time: 0.8413\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.6814 | Train Loss: 0.6986 | Time: 0.1333\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.7108 | Train Loss: 0.6010 | Time: 0.0484\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.5263 | Train Loss: 0.7188 | Time: 0.0503\n",
            "Epoch 00015 | Valid Acc: 0.5110 | Valid loss: 0.6943 | Time: 0.8356\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.6436 | Train Loss: 0.6034 | Time: 0.1268\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.5679 | Train Loss: 0.6327 | Time: 0.0503\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.6778 | Train Loss: 0.5642 | Time: 0.0583\n",
            "Epoch 00016 | Valid Acc: 0.5699 | Valid loss: 0.6836 | Time: 0.8322\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.6613 | Train Loss: 0.5751 | Time: 0.0552\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.6903 | Train Loss: 0.6238 | Time: 0.1164\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.6804 | Train Loss: 0.5876 | Time: 0.0601\n",
            "Epoch 00017 | Valid Acc: 0.5184 | Valid loss: 0.6899 | Time: 0.8286\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.5714 | Train Loss: 0.6795 | Time: 0.0489\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.7500 | Train Loss: 0.5886 | Time: 0.1247\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.8058 | Train Loss: 0.4827 | Time: 0.0612\n",
            "Epoch 00018 | Valid Acc: 0.4926 | Valid loss: 0.6885 | Time: 0.8322\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.5536 | Train Loss: 0.6392 | Time: 0.1097\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.6835 | Train Loss: 0.5571 | Time: 0.0601\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.6543 | Train Loss: 0.6274 | Time: 0.1145\n",
            "Epoch 00019 | Valid Acc: 0.5735 | Valid loss: 0.6827 | Time: 0.8333\n",
            "\n",
            "Training time: : 1.7041s | Batch time: : 0.0896s\n",
            "100% 10/10 [00:00<00:00, 14.00it/s]\n",
            "100% 10/10 [00:00<00:00, 13.24it/s]\n",
            "Test Acc: 0.6029\n",
            "\n",
            "Total time: : 3.1780s | Batch time: : 0.0896s\n"
          ]
        }
      ],
      "source": [
        "!python ecm.py -d mutag --l2norm 5e-4 --n-bases 30 --testing --gpu 0 --batch-size=50 --fanout=4 --num-parts 15 --cluster-size 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krhHWDWHJgXV",
        "outputId": "959fec54-52af-439a-869a-cee44325c89b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=50, cluster_size=5, data_cpu=False, dataset='mutag', dropout=0, fanout=8, gpu=0, l2norm=0.0005, lr=0.01, model_path=None, n_bases=30, n_epochs=20, n_hidden=16, n_layers=2, num_parts=15, use_self_loop=False, validation=False)\n",
            "Done loading data from cached files.\n",
            "Count of subgraphs: 3\n",
            "start training...\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.6404 | Train Loss: 0.6816 | Time: 0.0533\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.6092 | Train Loss: 0.6711 | Time: 0.1283\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.5833 | Train Loss: 0.6898 | Time: 0.1122\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "Epoch 00000 | Valid Acc: 0.6103 | Valid loss: 0.6786 | Time: nan\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.5385 | Train Loss: 0.7124 | Time: 0.0537\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.6697 | Train Loss: 0.6343 | Time: 0.1247\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.5856 | Train Loss: 0.7163 | Time: 0.1192\n",
            "Epoch 00001 | Valid Acc: 0.6103 | Valid loss: 0.6801 | Time: nan\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.5981 | Train Loss: 0.6397 | Time: 0.1163\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.5652 | Train Loss: 0.7131 | Time: 0.0568\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.6986 | Train Loss: 0.6315 | Time: 0.1289\n",
            "Epoch 00002 | Valid Acc: 0.4779 | Valid loss: 0.6985 | Time: nan\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.5660 | Train Loss: 0.6637 | Time: 0.0576\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.6984 | Train Loss: 0.5563 | Time: 0.1072\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.5631 | Train Loss: 0.6920 | Time: 0.1233\n",
            "Epoch 00003 | Valid Acc: 0.3676 | Valid loss: 0.7204 | Time: nan\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.3978 | Train Loss: 0.7895 | Time: 0.1195\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.4904 | Train Loss: 0.6869 | Time: 0.0468\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.5600 | Train Loss: 0.6798 | Time: 0.0618\n",
            "Epoch 00004 | Valid Acc: 0.4522 | Valid loss: 0.6972 | Time: 0.7681\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.5096 | Train Loss: 0.6953 | Time: 0.0652\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.6329 | Train Loss: 0.5989 | Time: 0.1092\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.5506 | Train Loss: 0.7651 | Time: 0.1086\n",
            "Epoch 00005 | Valid Acc: 0.6103 | Valid loss: 0.6791 | Time: 0.8144\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.6250 | Train Loss: 0.6343 | Time: 0.1291\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.5784 | Train Loss: 0.6573 | Time: 0.0457\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.6212 | Train Loss: 0.6356 | Time: 0.1096\n",
            "Epoch 00006 | Valid Acc: 0.6103 | Valid loss: 0.6766 | Time: 0.8292\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.6410 | Train Loss: 0.6541 | Time: 0.0596\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.6000 | Train Loss: 0.6833 | Time: 0.0617\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.5965 | Train Loss: 0.7297 | Time: 0.1181\n",
            "Epoch 00007 | Valid Acc: 0.6103 | Valid loss: 0.6789 | Time: 0.8213\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.6418 | Train Loss: 0.6231 | Time: 0.0596\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.6535 | Train Loss: 0.7026 | Time: 0.1173\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.5481 | Train Loss: 0.6389 | Time: 0.0457\n",
            "Epoch 00008 | Valid Acc: 0.6103 | Valid loss: 0.6761 | Time: 0.8088\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.5938 | Train Loss: 0.6516 | Time: 0.0624\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.6495 | Train Loss: 0.5805 | Time: 0.1166\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.5823 | Train Loss: 0.6904 | Time: 0.1156\n",
            "Epoch 00009 | Valid Acc: 0.6103 | Valid loss: 0.6801 | Time: 0.8243\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.5467 | Train Loss: 0.6654 | Time: 0.0617\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.5600 | Train Loss: 0.6630 | Time: 0.0479\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.7113 | Train Loss: 0.5634 | Time: 0.1168\n",
            "Epoch 00010 | Valid Acc: 0.6103 | Valid loss: 0.6825 | Time: 0.8248\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.6346 | Train Loss: 0.6539 | Time: 0.1327\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.6200 | Train Loss: 0.6668 | Time: 0.1070\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.6029 | Train Loss: 0.6683 | Time: 0.0601\n",
            "Epoch 00011 | Valid Acc: 0.6066 | Valid loss: 0.6818 | Time: 0.8363\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.8434 | Train Loss: 0.5298 | Time: 0.1143\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.6161 | Train Loss: 0.6474 | Time: 0.1202\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.5455 | Train Loss: 0.6730 | Time: 0.0479\n",
            "Epoch 00012 | Valid Acc: 0.6103 | Valid loss: 0.6822 | Time: 0.8421\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.7253 | Train Loss: 0.5517 | Time: 0.1303\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.6133 | Train Loss: 0.6170 | Time: 0.0511\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.5755 | Train Loss: 0.6339 | Time: 0.0533\n",
            "Epoch 00013 | Valid Acc: 0.6103 | Valid loss: 0.6849 | Time: 0.8350\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.6346 | Train Loss: 0.6053 | Time: 0.1127\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.6020 | Train Loss: 0.6294 | Time: 0.1074\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.5857 | Train Loss: 0.7621 | Time: 0.0571\n",
            "Epoch 00014 | Valid Acc: 0.6103 | Valid loss: 0.6853 | Time: 0.8338\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.7108 | Train Loss: 0.5747 | Time: 0.0525\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.5862 | Train Loss: 0.7824 | Time: 0.1152\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.6569 | Train Loss: 0.6038 | Time: 0.1011\n",
            "Epoch 00015 | Valid Acc: 0.6103 | Valid loss: 0.6863 | Time: 0.8332\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.6163 | Train Loss: 0.6213 | Time: 0.1240\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.7222 | Train Loss: 0.6548 | Time: 0.1210\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.5938 | Train Loss: 0.6045 | Time: 0.0471\n",
            "Epoch 00016 | Valid Acc: 0.5919 | Valid loss: 0.6896 | Time: 0.8341\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.5893 | Train Loss: 0.7235 | Time: 0.1117\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.7470 | Train Loss: 0.5900 | Time: 0.1252\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.7013 | Train Loss: 0.6055 | Time: 0.0515\n",
            "Epoch 00017 | Valid Acc: 0.5368 | Valid loss: 0.6918 | Time: 0.8351\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.6078 | Train Loss: 0.6575 | Time: 0.1099\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.5286 | Train Loss: 0.6813 | Time: 0.0590\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.6900 | Train Loss: 0.5716 | Time: 0.1220\n",
            "Epoch 00018 | Valid Acc: 0.4890 | Valid loss: 0.6978 | Time: 0.8367\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.6935 | Train Loss: 0.6772 | Time: 0.1181\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.5816 | Train Loss: 0.6426 | Time: 0.0474\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.5179 | Train Loss: 0.8170 | Time: 0.1219\n",
            "Epoch 00019 | Valid Acc: 0.5809 | Valid loss: 0.6928 | Time: 0.8374\n",
            "\n",
            "Training time: : 1.6689s | Batch time: : 0.0913s\n",
            "100% 8/8 [00:00<00:00, 13.06it/s]\n",
            "100% 8/8 [00:00<00:00, 12.91it/s]\n",
            "Test Acc: 0.6471\n",
            "\n",
            "Total time: : 2.9057s | Batch time: : 0.0913s\n"
          ]
        }
      ],
      "source": [
        "!python ecm.py -d mutag --l2norm 5e-4 --n-bases 30 --testing --gpu 0 --batch-size=50 --fanout=8 --num-parts 15 --cluster-size 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWrKoq64Jg4u",
        "outputId": "db7e61bf-056b-4f25-9a47-d84437d8948e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=50, cluster_size=5, data_cpu=False, dataset='mutag', dropout=0, fanout=16, gpu=0, l2norm=0.0005, lr=0.01, model_path=None, n_bases=30, n_epochs=20, n_hidden=16, n_layers=2, num_parts=15, use_self_loop=False, validation=False)\n",
            "Done loading data from cached files.\n",
            "Count of subgraphs: 3\n",
            "start training...\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.5446 | Train Loss: 0.6908 | Time: 0.0571\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.5802 | Train Loss: 0.6891 | Time: 0.1222\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.5222 | Train Loss: 0.7044 | Time: 0.1044\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "Epoch 00000 | Valid Acc: 0.6103 | Valid loss: 0.6717 | Time: nan\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.5658 | Train Loss: 0.6422 | Time: 0.0539\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.6000 | Train Loss: 0.6679 | Time: 0.1251\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.6293 | Train Loss: 0.6687 | Time: 0.1189\n",
            "Epoch 00001 | Valid Acc: 0.6103 | Valid loss: 0.6831 | Time: nan\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.6356 | Train Loss: 0.6725 | Time: 0.1204\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.5556 | Train Loss: 0.7502 | Time: 0.0446\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.6406 | Train Loss: 0.6385 | Time: 0.0560\n",
            "Epoch 00002 | Valid Acc: 0.6103 | Valid loss: 0.6741 | Time: nan\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.5577 | Train Loss: 0.7130 | Time: 0.1070\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.6250 | Train Loss: 0.6643 | Time: 0.1082\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.6477 | Train Loss: 0.6452 | Time: 0.0601\n",
            "Epoch 00003 | Valid Acc: 0.6103 | Valid loss: 0.6776 | Time: nan\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.6147 | Train Loss: 0.6375 | Time: 0.1146\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.4839 | Train Loss: 0.7314 | Time: 0.1066\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.5857 | Train Loss: 0.6586 | Time: 0.0558\n",
            "Epoch 00004 | Valid Acc: 0.6103 | Valid loss: 0.6820 | Time: 0.8657\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.6883 | Train Loss: 0.6247 | Time: 0.0610\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.6585 | Train Loss: 0.6483 | Time: 0.1087\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.6460 | Train Loss: 0.6708 | Time: 0.1185\n",
            "Epoch 00005 | Valid Acc: 0.6103 | Valid loss: 0.6830 | Time: 0.8722\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.6667 | Train Loss: 0.7045 | Time: 0.1149\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.5306 | Train Loss: 0.7047 | Time: 0.0570\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.7727 | Train Loss: 0.6288 | Time: 0.0611\n",
            "Epoch 00006 | Valid Acc: 0.6066 | Valid loss: 0.6866 | Time: 0.8411\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.6598 | Train Loss: 0.6579 | Time: 0.1206\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.6355 | Train Loss: 0.6586 | Time: 0.0594\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.6471 | Train Loss: 0.6634 | Time: 0.0489\n",
            "Epoch 00007 | Valid Acc: 0.6103 | Valid loss: 0.6863 | Time: 0.8289\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.6711 | Train Loss: 0.6481 | Time: 0.0529\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.6395 | Train Loss: 0.6435 | Time: 0.1143\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.5818 | Train Loss: 0.6534 | Time: 0.1052\n",
            "Epoch 00008 | Valid Acc: 0.6103 | Valid loss: 0.6846 | Time: 0.8313\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.6860 | Train Loss: 0.6069 | Time: 0.1252\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.6458 | Train Loss: 0.6477 | Time: 0.0568\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.5778 | Train Loss: 0.6557 | Time: 0.1145\n",
            "Epoch 00009 | Valid Acc: 0.6103 | Valid loss: 0.6887 | Time: 0.8386\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.6133 | Train Loss: 0.6506 | Time: 0.0506\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.6771 | Train Loss: 0.5697 | Time: 0.1259\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.7228 | Train Loss: 0.5591 | Time: 0.1231\n",
            "Epoch 00010 | Valid Acc: 0.6140 | Valid loss: 0.6844 | Time: 0.8421\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.6737 | Train Loss: 0.7697 | Time: 0.1095\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.6633 | Train Loss: 0.6417 | Time: 0.1192\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.5949 | Train Loss: 0.6064 | Time: 0.0595\n",
            "Epoch 00011 | Valid Acc: 0.5221 | Valid loss: 0.6974 | Time: 0.8452\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.5970 | Train Loss: 0.6750 | Time: 0.0672\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.5258 | Train Loss: 0.7302 | Time: 0.1178\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.6667 | Train Loss: 0.5886 | Time: 0.1143\n",
            "Epoch 00012 | Valid Acc: 0.4706 | Valid loss: 0.7115 | Time: 0.8494\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.5565 | Train Loss: 0.7205 | Time: 0.1178\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.8868 | Train Loss: 0.4498 | Time: 0.0564\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.6154 | Train Loss: 0.6583 | Time: 0.0527\n",
            "Epoch 00013 | Valid Acc: 0.5331 | Valid loss: 0.6969 | Time: 0.8428\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.5135 | Train Loss: 0.7449 | Time: 0.0541\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.5542 | Train Loss: 0.7478 | Time: 0.0451\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.6087 | Train Loss: 0.5906 | Time: 0.1269\n",
            "Epoch 00014 | Valid Acc: 0.5993 | Valid loss: 0.6901 | Time: 0.8383\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.6386 | Train Loss: 0.6450 | Time: 0.0630\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.6789 | Train Loss: 0.6161 | Time: 0.1239\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.5000 | Train Loss: 0.6318 | Time: 0.1150\n",
            "Epoch 00015 | Valid Acc: 0.5956 | Valid loss: 0.6933 | Time: 0.8404\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.6731 | Train Loss: 0.5936 | Time: 0.1128\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.4590 | Train Loss: 0.7418 | Time: 0.0588\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.6822 | Train Loss: 0.6443 | Time: 0.1044\n",
            "Epoch 00016 | Valid Acc: 0.5956 | Valid loss: 0.6957 | Time: 0.8445\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.6076 | Train Loss: 0.5560 | Time: 0.0626\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.6404 | Train Loss: 0.6304 | Time: 0.1143\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.5865 | Train Loss: 0.6300 | Time: 0.1081\n",
            "Epoch 00017 | Valid Acc: 0.6029 | Valid loss: 0.7082 | Time: 0.8457\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.6111 | Train Loss: 0.6732 | Time: 0.0511\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.6900 | Train Loss: 0.6250 | Time: 0.1175\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.6700 | Train Loss: 0.6608 | Time: 0.0611\n",
            "Epoch 00018 | Valid Acc: 0.6066 | Valid loss: 0.7145 | Time: 0.8420\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.6706 | Train Loss: 0.5534 | Time: 0.0518\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.6860 | Train Loss: 0.5917 | Time: 0.0570\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.6832 | Train Loss: 0.6247 | Time: 0.1273\n",
            "Epoch 00019 | Valid Acc: 0.6066 | Valid loss: 0.7181 | Time: 0.8391\n",
            "\n",
            "Training time: : 1.6630s | Batch time: : 0.0890s\n",
            "100% 10/10 [00:00<00:00, 13.77it/s]\n",
            "100% 10/10 [00:00<00:00, 13.26it/s]\n",
            "Test Acc: 0.6618\n",
            "\n",
            "Total time: : 3.1492s | Batch time: : 0.0890s\n"
          ]
        }
      ],
      "source": [
        "!python ecm.py -d mutag --l2norm 5e-4 --n-bases 30 --testing --gpu 0 --batch-size=50 --fanout=16 --num-parts 15 --cluster-size 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyfGPF2W2cv6",
        "outputId": "fd141aa1-513e-44a2-a879-ac4968518406"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "140"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de4FCrsgO5q9",
        "outputId": "1e23e30c-32d2-4746-c035-bb06cfe1d14d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=100, cluster_size=3, data_cpu=False, dataset='am', dropout=0, fanout=4, gpu=0, l2norm=0.0005, lr=0.01, model_path=None, n_bases=40, n_epochs=20, n_hidden=16, n_layers=2, num_parts=27, use_self_loop=False, validation=False)\n",
            "Done loading data from cached files.\n",
            "Count of subgraphs: 9\n",
            "start training...\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.0909 | Train Loss: 2.3963 | Time: 0.2356\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.0952 | Train Loss: 2.3721 | Time: 0.2207\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.2121 | Train Loss: 2.3876 | Time: 0.2604\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.1959 | Train Loss: 2.3538 | Time: 0.2659\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.2667 | Train Loss: 2.3237 | Time: 0.2074\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.4875 | Train Loss: 2.1761 | Time: 0.2317\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.1091 | Train Loss: 2.4444 | Time: 0.2263\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.2987 | Train Loss: 1.9292 | Time: 0.2363\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.2877 | Train Loss: 2.1859 | Time: 0.2329\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "Epoch 00000 | Valid Acc: 0.3466 | Valid loss: 2.0548 | Time: nan\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.2222 | Train Loss: 2.0944 | Time: 0.1767\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.7364 | Train Loss: 1.2335 | Time: 0.2242\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.1881 | Train Loss: 2.3518 | Time: 0.2315\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.2045 | Train Loss: 2.3430 | Time: 0.3113\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.5342 | Train Loss: 2.0137 | Time: 0.2314\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.1616 | Train Loss: 2.5358 | Time: 0.2367\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.2317 | Train Loss: 2.0533 | Time: 0.2370\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.3494 | Train Loss: 1.7361 | Time: 0.2385\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.4771 | Train Loss: 1.6158 | Time: 0.2414\n",
            "Epoch 00001 | Valid Acc: 0.3778 | Valid loss: 1.9629 | Time: nan\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.1905 | Train Loss: 2.1353 | Time: 0.2583\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.2105 | Train Loss: 2.1069 | Time: 0.2579\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.2857 | Train Loss: 1.9449 | Time: 0.2247\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.3710 | Train Loss: 2.0570 | Time: 0.2458\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.2148 | Train Loss: 2.2579 | Time: 0.2261\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.2317 | Train Loss: 2.3111 | Time: 0.2151\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.2587 | Train Loss: 2.0532 | Time: 0.2462\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.2946 | Train Loss: 2.3710 | Time: 0.2431\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.2000 | Train Loss: 2.1715 | Time: 0.2402\n",
            "Epoch 00002 | Valid Acc: 0.4027 | Valid loss: 2.0713 | Time: nan\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.2317 | Train Loss: 2.1450 | Time: 0.2662\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.3692 | Train Loss: 1.8713 | Time: 0.2354\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.4691 | Train Loss: 1.7428 | Time: 0.2298\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.3980 | Train Loss: 2.0236 | Time: 0.2301\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.1786 | Train Loss: 2.1411 | Time: 0.2300\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.2268 | Train Loss: 2.3947 | Time: 0.2402\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.6054 | Train Loss: 1.4289 | Time: 0.2843\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.2614 | Train Loss: 2.0376 | Time: 0.2298\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.5000 | Train Loss: 1.5447 | Time: 0.1912\n",
            "Epoch 00003 | Valid Acc: 0.4040 | Valid loss: 1.7878 | Time: nan\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.4444 | Train Loss: 1.8126 | Time: 0.2275\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.4688 | Train Loss: 1.8037 | Time: 0.2359\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.5474 | Train Loss: 1.5758 | Time: 0.2495\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.3440 | Train Loss: 1.8511 | Time: 0.2539\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.3913 | Train Loss: 2.0071 | Time: 0.2432\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.7632 | Train Loss: 0.6702 | Time: 0.2603\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.5481 | Train Loss: 1.8153 | Time: 0.2279\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.4091 | Train Loss: 2.0695 | Time: 0.2858\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.4745 | Train Loss: 1.4702 | Time: 0.2447\n",
            "Epoch 00004 | Valid Acc: 0.4115 | Valid loss: 1.5524 | Time: 11.4448\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.3968 | Train Loss: 1.8996 | Time: 0.2289\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.5140 | Train Loss: 1.4350 | Time: 0.2343\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.6364 | Train Loss: 1.3697 | Time: 0.1940\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.2222 | Train Loss: 1.9174 | Time: 0.2360\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.3908 | Train Loss: 1.5933 | Time: 0.2387\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.2720 | Train Loss: 2.6521 | Time: 0.2755\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.5476 | Train Loss: 1.5511 | Time: 0.2409\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.4583 | Train Loss: 1.4685 | Time: 0.2319\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.8317 | Train Loss: 0.9162 | Time: 0.2388\n",
            "Epoch 00005 | Valid Acc: 0.4489 | Valid loss: 1.6419 | Time: 11.2830\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.6277 | Train Loss: 1.2772 | Time: 0.2444\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.3453 | Train Loss: 2.1351 | Time: 0.2570\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.6855 | Train Loss: 0.9495 | Time: 0.2745\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.5506 | Train Loss: 1.4609 | Time: 0.2727\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.6296 | Train Loss: 1.0833 | Time: 0.2294\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.4583 | Train Loss: 1.4738 | Time: 0.2109\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.4853 | Train Loss: 1.6728 | Time: 0.2217\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.5133 | Train Loss: 1.7666 | Time: 0.2756\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.6857 | Train Loss: 1.1291 | Time: 0.2080\n",
            "Epoch 00006 | Valid Acc: 0.4190 | Valid loss: 1.7527 | Time: 11.2536\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.6667 | Train Loss: 1.3385 | Time: 0.1843\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.2024 | Train Loss: 2.6982 | Time: 0.2591\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.5000 | Train Loss: 1.8949 | Time: 0.2675\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.3478 | Train Loss: 1.9550 | Time: 0.2631\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.5970 | Train Loss: 1.3355 | Time: 0.2298\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.8537 | Train Loss: 0.5401 | Time: 0.2159\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.7257 | Train Loss: 0.8940 | Time: 0.2191\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.2564 | Train Loss: 2.5868 | Time: 0.2654\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.4675 | Train Loss: 1.7310 | Time: 0.2503\n",
            "Epoch 00007 | Valid Acc: 0.4589 | Valid loss: 1.6450 | Time: 11.2862\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.5138 | Train Loss: 1.2671 | Time: 0.2485\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.4400 | Train Loss: 1.5601 | Time: 0.2233\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.3611 | Train Loss: 1.8853 | Time: 0.2424\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.5860 | Train Loss: 1.2949 | Time: 0.2389\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.5929 | Train Loss: 1.3519 | Time: 0.2446\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.3281 | Train Loss: 1.9321 | Time: 0.2567\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.3646 | Train Loss: 1.8671 | Time: 0.2345\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.3750 | Train Loss: 1.9014 | Time: 0.1874\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.4713 | Train Loss: 1.5430 | Time: 0.2664\n",
            "Epoch 00008 | Valid Acc: 0.4626 | Valid loss: 1.6961 | Time: 11.3480\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.4371 | Train Loss: 1.9777 | Time: 0.2448\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.5224 | Train Loss: 1.2804 | Time: 0.2546\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.3010 | Train Loss: 2.0811 | Time: 0.2904\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.4727 | Train Loss: 1.5582 | Time: 0.2553\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.5970 | Train Loss: 1.3328 | Time: 0.2350\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.4359 | Train Loss: 1.9802 | Time: 0.2199\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.4310 | Train Loss: 1.8447 | Time: 0.2455\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.7273 | Train Loss: 0.7453 | Time: 0.2113\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.3043 | Train Loss: 2.3164 | Time: 0.2443\n",
            "Epoch 00009 | Valid Acc: 0.4165 | Valid loss: 1.7065 | Time: 11.3217\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.7411 | Train Loss: 0.8083 | Time: 0.2310\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.3700 | Train Loss: 2.0846 | Time: 0.2391\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.5294 | Train Loss: 1.9371 | Time: 0.2208\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.7500 | Train Loss: 0.7179 | Time: 0.2302\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.6087 | Train Loss: 1.1427 | Time: 0.2145\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.3019 | Train Loss: 2.0934 | Time: 0.2628\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.3273 | Train Loss: 2.2623 | Time: 0.2425\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.5638 | Train Loss: 1.2190 | Time: 0.2912\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.1818 | Train Loss: 2.0597 | Time: 0.2342\n",
            "Epoch 00010 | Valid Acc: 0.4850 | Valid loss: 1.5838 | Time: 11.2962\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.4483 | Train Loss: 1.3345 | Time: 0.2180\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.3853 | Train Loss: 1.7349 | Time: 0.2375\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.2800 | Train Loss: 2.4832 | Time: 0.2487\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.6204 | Train Loss: 1.2166 | Time: 0.2465\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.2946 | Train Loss: 2.1548 | Time: 0.2349\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.3107 | Train Loss: 2.0605 | Time: 0.1973\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.3400 | Train Loss: 1.8431 | Time: 0.2571\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.5906 | Train Loss: 1.2067 | Time: 0.2773\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.6000 | Train Loss: 1.2474 | Time: 0.1882\n",
            "Epoch 00011 | Valid Acc: 0.4489 | Valid loss: 1.5677 | Time: 11.2767\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.7569 | Train Loss: 0.7281 | Time: 0.2315\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.4741 | Train Loss: 1.5072 | Time: 0.2389\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.3604 | Train Loss: 1.6798 | Time: 0.2686\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.3571 | Train Loss: 1.2437 | Time: 0.1928\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.7209 | Train Loss: 0.8537 | Time: 0.2330\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.2836 | Train Loss: 2.4038 | Time: 0.2299\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.7349 | Train Loss: 0.8607 | Time: 0.2380\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.5421 | Train Loss: 1.4776 | Time: 0.2381\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.7350 | Train Loss: 0.8206 | Time: 0.2476\n",
            "Epoch 00012 | Valid Acc: 0.4514 | Valid loss: 1.6589 | Time: 11.2755\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.4733 | Train Loss: 1.9504 | Time: 0.2377\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.4286 | Train Loss: 1.7729 | Time: 0.2330\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.6429 | Train Loss: 1.3676 | Time: 0.2236\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.5985 | Train Loss: 1.5604 | Time: 0.2927\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.5435 | Train Loss: 1.6594 | Time: 0.2438\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.5603 | Train Loss: 1.7541 | Time: 0.2415\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.4935 | Train Loss: 1.8254 | Time: 0.2371\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.4257 | Train Loss: 1.2648 | Time: 0.1929\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.5882 | Train Loss: 1.6715 | Time: 0.2626\n",
            "Epoch 00013 | Valid Acc: 0.4638 | Valid loss: 1.6399 | Time: 11.2557\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.8657 | Train Loss: 0.4411 | Time: 0.2097\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.5673 | Train Loss: 1.3101 | Time: 0.2369\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.7429 | Train Loss: 0.9449 | Time: 0.2270\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.4262 | Train Loss: 1.7995 | Time: 0.2362\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.7115 | Train Loss: 0.9191 | Time: 0.2428\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.2857 | Train Loss: 1.9458 | Time: 0.2335\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.7273 | Train Loss: 0.7372 | Time: 0.2262\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.6262 | Train Loss: 1.3394 | Time: 0.2735\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.6863 | Train Loss: 1.0523 | Time: 0.2213\n",
            "Epoch 00014 | Valid Acc: 0.4302 | Valid loss: 1.7380 | Time: 11.2437\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.5268 | Train Loss: 1.5394 | Time: 0.2638\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.5462 | Train Loss: 1.4384 | Time: 0.2469\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.2857 | Train Loss: 2.7607 | Time: 0.2361\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.4390 | Train Loss: 1.9354 | Time: 0.2394\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.4945 | Train Loss: 1.8048 | Time: 0.2403\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.5938 | Train Loss: 1.3531 | Time: 0.2367\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.6962 | Train Loss: 1.0924 | Time: 0.2119\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.5610 | Train Loss: 1.5557 | Time: 0.2285\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.8024 | Train Loss: 0.6494 | Time: 0.2296\n",
            "Epoch 00015 | Valid Acc: 0.4489 | Valid loss: 1.5560 | Time: 11.2383\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.6494 | Train Loss: 1.0621 | Time: 0.2719\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.4706 | Train Loss: 1.7726 | Time: 0.2206\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.8333 | Train Loss: 0.5947 | Time: 0.1856\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.7403 | Train Loss: 0.6062 | Time: 0.2671\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.4948 | Train Loss: 1.4675 | Time: 0.2414\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.8487 | Train Loss: 0.5696 | Time: 0.2387\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.7770 | Train Loss: 0.7199 | Time: 0.2294\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.4375 | Train Loss: 1.7137 | Time: 0.2788\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.7368 | Train Loss: 0.9726 | Time: 0.2039\n",
            "Epoch 00016 | Valid Acc: 0.4102 | Valid loss: 1.7593 | Time: 11.2263\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.7200 | Train Loss: 1.0713 | Time: 0.1928\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.4500 | Train Loss: 1.7502 | Time: 0.2471\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.4806 | Train Loss: 1.3839 | Time: 0.2501\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.7018 | Train Loss: 0.8665 | Time: 0.2410\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.2473 | Train Loss: 2.4687 | Time: 0.2381\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.4795 | Train Loss: 2.4638 | Time: 0.2388\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.6325 | Train Loss: 1.2148 | Time: 0.2397\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.7583 | Train Loss: 0.8521 | Time: 0.2677\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.5641 | Train Loss: 1.1558 | Time: 0.2605\n",
            "Epoch 00017 | Valid Acc: 0.4002 | Valid loss: 1.6542 | Time: 11.2299\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.8824 | Train Loss: 0.5346 | Time: 0.2307\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.9068 | Train Loss: 0.3357 | Time: 0.2149\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.5325 | Train Loss: 1.5178 | Time: 0.2616\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.5221 | Train Loss: 1.4731 | Time: 0.2905\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.5862 | Train Loss: 1.6004 | Time: 0.2054\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.8205 | Train Loss: 0.6048 | Time: 0.2338\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.4662 | Train Loss: 1.6201 | Time: 0.2340\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.5833 | Train Loss: 1.2810 | Time: 0.2729\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.6598 | Train Loss: 1.0693 | Time: 0.2355\n",
            "Epoch 00018 | Valid Acc: 0.4489 | Valid loss: 1.5282 | Time: 11.2319\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.8444 | Train Loss: 0.7219 | Time: 0.2285\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.5385 | Train Loss: 1.5068 | Time: 0.2429\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.6250 | Train Loss: 1.4342 | Time: 0.2312\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.3506 | Train Loss: 1.8743 | Time: 0.2624\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.8125 | Train Loss: 0.5758 | Time: 0.2641\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.4902 | Train Loss: 1.6457 | Time: 0.2356\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.8533 | Train Loss: 0.4920 | Time: 0.2294\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.7913 | Train Loss: 0.7119 | Time: 0.2139\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.5000 | Train Loss: 1.6210 | Time: 0.2255\n",
            "Epoch 00019 | Valid Acc: 0.4888 | Valid loss: 1.5521 | Time: 11.2224\n",
            "\n",
            "Training time: : 13.6698s | Batch time: : 0.2389s\n",
            "100% 406/406 [01:03<00:00,  6.38it/s]\n",
            "100% 406/406 [01:08<00:00,  5.91it/s]\n",
            "Test Acc: 0.4848\n",
            "\n",
            "Total time: : 146.0421s | Batch time: : 0.2389s\n"
          ]
        }
      ],
      "source": [
        "!python ecm.py -d am --l2norm 5e-4 --n-bases 40 --testing --gpu 0 --fanout=4 --num-parts 27 --cluster-size 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNuyHMx7O6FA",
        "outputId": "2aae7771-5afc-4276-efeb-a77a91f459c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=100, cluster_size=3, data_cpu=False, dataset='am', dropout=0, fanout=8, gpu=0, l2norm=0.0005, lr=0.01, model_path=None, n_bases=40, n_epochs=20, n_hidden=16, n_layers=2, num_parts=27, use_self_loop=False, validation=False)\n",
            "Done loading data from cached files.\n",
            "Count of subgraphs: 9\n",
            "start training...\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.0519 | Train Loss: 2.4026 | Time: 0.2330\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.2632 | Train Loss: 2.3933 | Time: 0.2459\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.5278 | Train Loss: 2.3619 | Time: 0.2431\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.1136 | Train Loss: 2.3911 | Time: 0.2371\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.2471 | Train Loss: 2.3668 | Time: 0.2366\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.0800 | Train Loss: 2.3916 | Time: 0.2567\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.2200 | Train Loss: 2.2961 | Time: 0.2479\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.1754 | Train Loss: 2.3538 | Time: 0.2329\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.2414 | Train Loss: 2.1752 | Time: 0.2269\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "Epoch 00000 | Valid Acc: 0.3466 | Valid loss: 2.0046 | Time: nan\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.3457 | Train Loss: 1.8953 | Time: 0.2428\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.0400 | Train Loss: 2.3520 | Time: 0.2343\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.5410 | Train Loss: 1.6713 | Time: 0.2638\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.1650 | Train Loss: 2.5452 | Time: 0.2838\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.2821 | Train Loss: 2.4206 | Time: 0.2169\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.3409 | Train Loss: 2.0769 | Time: 0.2513\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.3774 | Train Loss: 1.8695 | Time: 0.2295\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.3165 | Train Loss: 2.1328 | Time: 0.2811\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.1538 | Train Loss: 2.3284 | Time: 0.2353\n",
            "Epoch 00001 | Valid Acc: 0.3915 | Valid loss: 1.9458 | Time: nan\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.5101 | Train Loss: 1.6606 | Time: 0.2506\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.2400 | Train Loss: 2.1151 | Time: 0.2064\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.3750 | Train Loss: 1.8486 | Time: 0.2527\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.4054 | Train Loss: 1.9988 | Time: 0.2695\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.3542 | Train Loss: 1.8846 | Time: 0.2501\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.4750 | Train Loss: 2.1743 | Time: 0.2340\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.3871 | Train Loss: 1.8897 | Time: 0.2384\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.3059 | Train Loss: 1.9896 | Time: 0.2682\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.3333 | Train Loss: 1.8287 | Time: 0.2075\n",
            "Epoch 00002 | Valid Acc: 0.4077 | Valid loss: 1.7286 | Time: nan\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.5278 | Train Loss: 1.2719 | Time: 0.2529\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.3816 | Train Loss: 2.3293 | Time: 0.2175\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.3443 | Train Loss: 1.5023 | Time: 0.2263\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.3418 | Train Loss: 1.8848 | Time: 0.2905\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.3038 | Train Loss: 2.3099 | Time: 0.2362\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.6193 | Train Loss: 0.9748 | Time: 0.2393\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.3636 | Train Loss: 1.9893 | Time: 0.2200\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.4545 | Train Loss: 1.5658 | Time: 0.2615\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.4904 | Train Loss: 1.5622 | Time: 0.2510\n",
            "Epoch 00003 | Valid Acc: 0.4713 | Valid loss: 1.5978 | Time: nan\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.4804 | Train Loss: 1.4843 | Time: 0.2413\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.4615 | Train Loss: 1.8214 | Time: 0.2431\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.3696 | Train Loss: 1.5735 | Time: 0.2753\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.4571 | Train Loss: 1.6932 | Time: 0.2457\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.4035 | Train Loss: 1.8155 | Time: 0.2413\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.3377 | Train Loss: 1.5179 | Time: 0.2371\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.4615 | Train Loss: 1.5827 | Time: 0.2385\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.1467 | Train Loss: 2.1011 | Time: 0.2340\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.6364 | Train Loss: 1.2168 | Time: 0.2126\n",
            "Epoch 00004 | Valid Acc: 0.3828 | Valid loss: 1.6845 | Time: 11.0184\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.6905 | Train Loss: 1.1282 | Time: 0.2191\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.6163 | Train Loss: 1.0082 | Time: 0.2367\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.6889 | Train Loss: 0.8754 | Time: 0.2357\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.5263 | Train Loss: 1.5757 | Time: 0.2800\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.3902 | Train Loss: 1.9470 | Time: 0.2663\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.6875 | Train Loss: 1.0565 | Time: 0.2000\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.6711 | Train Loss: 1.1347 | Time: 0.2386\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.2941 | Train Loss: 1.8476 | Time: 0.2691\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.3000 | Train Loss: 2.1991 | Time: 0.2437\n",
            "Epoch 00005 | Valid Acc: 0.4439 | Valid loss: 1.6524 | Time: 10.9245\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.2600 | Train Loss: 2.2021 | Time: 0.2431\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.4306 | Train Loss: 1.8278 | Time: 0.2309\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.7895 | Train Loss: 0.7859 | Time: 0.1854\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.6306 | Train Loss: 1.2740 | Time: 0.2598\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.4472 | Train Loss: 1.9110 | Time: 0.2428\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.7476 | Train Loss: 0.7374 | Time: 0.2018\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.7674 | Train Loss: 0.5931 | Time: 0.2425\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.6028 | Train Loss: 1.1863 | Time: 0.2702\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.3814 | Train Loss: 1.8680 | Time: 0.2468\n",
            "Epoch 00006 | Valid Acc: 0.4726 | Valid loss: 1.5328 | Time: 10.9865\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.4091 | Train Loss: 1.5192 | Time: 0.2550\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.7500 | Train Loss: 0.7534 | Time: 0.1895\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.4000 | Train Loss: 1.5920 | Time: 0.2441\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.7927 | Train Loss: 1.0257 | Time: 0.2639\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.4038 | Train Loss: 1.6465 | Time: 0.2288\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.4444 | Train Loss: 2.2061 | Time: 0.2281\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.7680 | Train Loss: 1.6683 | Time: 0.2373\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.6529 | Train Loss: 0.9600 | Time: 0.2472\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.3009 | Train Loss: 2.1705 | Time: 0.2897\n",
            "Epoch 00007 | Valid Acc: 0.4401 | Valid loss: 1.7198 | Time: 10.9601\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.2500 | Train Loss: 2.3983 | Time: 0.2328\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.7162 | Train Loss: 0.8847 | Time: 0.2139\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.4062 | Train Loss: 1.7642 | Time: 0.2075\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.3333 | Train Loss: 1.7083 | Time: 0.2436\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.4277 | Train Loss: 1.4389 | Time: 0.2613\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.5606 | Train Loss: 1.5729 | Time: 0.2340\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.5155 | Train Loss: 1.6070 | Time: 0.2451\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.8202 | Train Loss: 0.7721 | Time: 0.2632\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.3091 | Train Loss: 1.8582 | Time: 0.3174\n",
            "Epoch 00008 | Valid Acc: 0.4850 | Valid loss: 1.6209 | Time: 11.0163\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.7113 | Train Loss: 0.9265 | Time: 0.2623\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.7143 | Train Loss: 0.9779 | Time: 0.1970\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.8839 | Train Loss: 0.5491 | Time: 0.2001\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.6250 | Train Loss: 1.1950 | Time: 0.2446\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.4762 | Train Loss: 1.5794 | Time: 0.2656\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.9036 | Train Loss: 0.5099 | Time: 0.2734\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.7500 | Train Loss: 0.7945 | Time: 0.2404\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.6526 | Train Loss: 1.0731 | Time: 0.2397\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.3721 | Train Loss: 2.1110 | Time: 0.2463\n",
            "Epoch 00009 | Valid Acc: 0.4850 | Valid loss: 1.5181 | Time: 11.0043\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.4176 | Train Loss: 1.7598 | Time: 0.2780\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.8252 | Train Loss: 0.6695 | Time: 0.2501\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.6622 | Train Loss: 0.9134 | Time: 0.2400\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.5040 | Train Loss: 1.5946 | Time: 0.2443\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.7640 | Train Loss: 0.6991 | Time: 0.2373\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.9333 | Train Loss: 0.2733 | Time: 0.1981\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.5349 | Train Loss: 1.4412 | Time: 0.2380\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.3651 | Train Loss: 1.7122 | Time: 0.2198\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.4286 | Train Loss: 1.9539 | Time: 0.2161\n",
            "Epoch 00010 | Valid Acc: 0.4177 | Valid loss: 1.6518 | Time: 10.9812\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.4750 | Train Loss: 1.6317 | Time: 0.2263\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.4918 | Train Loss: 1.9183 | Time: 0.2293\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.4915 | Train Loss: 1.7257 | Time: 0.2421\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.6579 | Train Loss: 1.1372 | Time: 0.2498\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.7419 | Train Loss: 0.8227 | Time: 0.2314\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.3482 | Train Loss: 1.8870 | Time: 0.2314\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.5345 | Train Loss: 1.3006 | Time: 0.2760\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.5938 | Train Loss: 1.2905 | Time: 0.2674\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.3438 | Train Loss: 2.1266 | Time: 0.2421\n",
            "Epoch 00011 | Valid Acc: 0.4352 | Valid loss: 1.6518 | Time: 10.9900\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.5161 | Train Loss: 1.7996 | Time: 0.2274\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.7800 | Train Loss: 1.1161 | Time: 0.2281\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.7486 | Train Loss: 0.7922 | Time: 0.2342\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.5882 | Train Loss: 1.6605 | Time: 0.2054\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.4122 | Train Loss: 1.8950 | Time: 0.2458\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.4966 | Train Loss: 1.5847 | Time: 0.2520\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.4286 | Train Loss: 2.0299 | Time: 0.2698\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.6667 | Train Loss: 1.1461 | Time: 0.2340\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.3673 | Train Loss: 2.4659 | Time: 0.2424\n",
            "Epoch 00012 | Valid Acc: 0.4676 | Valid loss: 1.5534 | Time: 10.9708\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.7500 | Train Loss: 0.8635 | Time: 0.2545\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.4524 | Train Loss: 1.9458 | Time: 0.2838\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.8774 | Train Loss: 0.4163 | Time: 0.2541\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.9506 | Train Loss: 0.3135 | Time: 0.2339\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.5333 | Train Loss: 2.8783 | Time: 0.2305\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.3836 | Train Loss: 2.0732 | Time: 0.2352\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.9000 | Train Loss: 0.3578 | Time: 0.2526\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.4286 | Train Loss: 1.7055 | Time: 0.1777\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.4667 | Train Loss: 1.6498 | Time: 0.2448\n",
            "Epoch 00013 | Valid Acc: 0.3915 | Valid loss: 1.7410 | Time: 10.9731\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.8333 | Train Loss: 0.5399 | Time: 0.2491\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.6489 | Train Loss: 1.1676 | Time: 0.2568\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.3478 | Train Loss: 2.1989 | Time: 0.2408\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.7321 | Train Loss: 0.7039 | Time: 0.2302\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.4615 | Train Loss: 1.5569 | Time: 0.1986\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.5000 | Train Loss: 1.6749 | Time: 0.2356\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.7755 | Train Loss: 0.5956 | Time: 0.2405\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.8889 | Train Loss: 0.3441 | Time: 0.2391\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.2791 | Train Loss: 2.2826 | Time: 0.2365\n",
            "Epoch 00014 | Valid Acc: 0.4302 | Valid loss: 1.6106 | Time: 10.9696\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.6800 | Train Loss: 1.4397 | Time: 0.1983\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.7812 | Train Loss: 0.6364 | Time: 0.2455\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.4425 | Train Loss: 1.6949 | Time: 0.2545\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.8197 | Train Loss: 0.5337 | Time: 0.2701\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.7582 | Train Loss: 0.7905 | Time: 0.2434\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.5854 | Train Loss: 1.0548 | Time: 0.2238\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.7667 | Train Loss: 0.8882 | Time: 0.2501\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.4000 | Train Loss: 2.3218 | Time: 0.2319\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.4932 | Train Loss: 1.7722 | Time: 0.2391\n",
            "Epoch 00015 | Valid Acc: 0.4514 | Valid loss: 1.6165 | Time: 10.9667\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.4468 | Train Loss: 1.7541 | Time: 0.2774\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.6667 | Train Loss: 1.2892 | Time: 0.2126\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.8571 | Train Loss: 0.4276 | Time: 0.2354\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.4277 | Train Loss: 1.6740 | Time: 0.2500\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.3139 | Train Loss: 2.1891 | Time: 0.2471\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.3014 | Train Loss: 2.2177 | Time: 0.2355\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.8081 | Train Loss: 0.7168 | Time: 0.2242\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.9694 | Train Loss: 0.1249 | Time: 0.2445\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.7115 | Train Loss: 0.7868 | Time: 0.2412\n",
            "Epoch 00016 | Valid Acc: 0.5337 | Valid loss: 1.4876 | Time: 10.9527\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.5046 | Train Loss: 1.6638 | Time: 0.2322\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.5889 | Train Loss: 1.1411 | Time: 0.2324\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.6584 | Train Loss: 0.9615 | Time: 0.2423\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.3667 | Train Loss: 1.6347 | Time: 0.2210\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.5833 | Train Loss: 1.8342 | Time: 0.2120\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.7561 | Train Loss: 0.7569 | Time: 0.2473\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.4286 | Train Loss: 1.5772 | Time: 0.2203\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.7045 | Train Loss: 0.9656 | Time: 0.2966\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.4216 | Train Loss: 1.8852 | Time: 0.2817\n",
            "Epoch 00017 | Valid Acc: 0.5399 | Valid loss: 1.4550 | Time: 10.9597\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.7746 | Train Loss: 0.9625 | Time: 0.2170\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.3457 | Train Loss: 1.8808 | Time: 0.2380\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.6981 | Train Loss: 0.8391 | Time: 0.2561\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.4490 | Train Loss: 1.5306 | Time: 0.3031\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.8370 | Train Loss: 0.4445 | Time: 0.2288\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.3167 | Train Loss: 1.9787 | Time: 0.2460\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.6757 | Train Loss: 1.1400 | Time: 0.2377\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.4615 | Train Loss: 1.7366 | Time: 0.2425\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.6400 | Train Loss: 1.0635 | Time: 0.1939\n",
            "Epoch 00018 | Valid Acc: 0.5262 | Valid loss: 1.4290 | Time: 10.9578\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.5040 | Train Loss: 2.0155 | Time: 0.2578\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.5632 | Train Loss: 1.3010 | Time: 0.2381\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.7812 | Train Loss: 0.7091 | Time: 0.2378\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.7797 | Train Loss: 0.5743 | Time: 0.2344\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.8462 | Train Loss: 0.7869 | Time: 0.2149\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.8839 | Train Loss: 0.3367 | Time: 0.2278\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.5306 | Train Loss: 1.4623 | Time: 0.2445\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.8416 | Train Loss: 0.5534 | Time: 0.2677\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.5000 | Train Loss: 1.4788 | Time: 0.2310\n",
            "Epoch 00019 | Valid Acc: 0.4701 | Valid loss: 1.5300 | Time: 10.9653\n",
            "\n",
            "Training time: : 13.7100s | Batch time: : 0.2411s\n",
            "100% 141/141 [00:24<00:00,  5.83it/s]\n",
            "100% 141/141 [00:26<00:00,  5.30it/s]\n",
            "Test Acc: 0.4798\n",
            "\n",
            "Total time: : 64.6218s | Batch time: : 0.2411s\n"
          ]
        }
      ],
      "source": [
        "!python ecm.py -d am --l2norm 5e-4 --n-bases 40 --testing --gpu 0 --fanout=8 --num-parts 27 --cluster-size 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-A-3-KzpO7o-",
        "outputId": "bc323221-a501-400c-f53b-eeba9680d29d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=100, cluster_size=3, data_cpu=False, dataset='am', dropout=0, fanout=16, gpu=0, l2norm=0.0005, lr=0.01, model_path=None, n_bases=40, n_epochs=20, n_hidden=16, n_layers=2, num_parts=27, use_self_loop=False, validation=False)\n",
            "Done loading data from cached files.\n",
            "Count of subgraphs: 9\n",
            "start training...\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.1765 | Train Loss: 2.3962 | Time: 0.1996\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.0693 | Train Loss: 2.3964 | Time: 0.2307\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.0370 | Train Loss: 2.3853 | Time: 0.2097\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.0097 | Train Loss: 2.3692 | Time: 0.2253\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.0714 | Train Loss: 2.3883 | Time: 0.2506\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.2119 | Train Loss: 2.3366 | Time: 0.2420\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.1122 | Train Loss: 2.4663 | Time: 0.2308\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.2778 | Train Loss: 2.3251 | Time: 0.2624\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.4204 | Train Loss: 2.1700 | Time: 0.2456\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "Epoch 00000 | Valid Acc: 0.3491 | Valid loss: 2.0775 | Time: nan\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.1688 | Train Loss: 2.0966 | Time: 0.2450\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.2976 | Train Loss: 2.0070 | Time: 0.2422\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.2963 | Train Loss: 2.2803 | Time: 0.2311\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.2188 | Train Loss: 2.2871 | Time: 0.2350\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.1667 | Train Loss: 2.5778 | Time: 0.2381\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.6977 | Train Loss: 1.2947 | Time: 0.2248\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.3929 | Train Loss: 2.0584 | Time: 0.2634\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.1224 | Train Loss: 2.1381 | Time: 0.2582\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.4191 | Train Loss: 1.8088 | Time: 0.2229\n",
            "Epoch 00001 | Valid Acc: 0.3641 | Valid loss: 1.8484 | Time: nan\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.3030 | Train Loss: 2.0927 | Time: 0.2620\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.7206 | Train Loss: 1.3121 | Time: 0.2275\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.3974 | Train Loss: 2.0135 | Time: 0.2328\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.3404 | Train Loss: 1.9954 | Time: 0.2166\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.3191 | Train Loss: 1.9961 | Time: 0.2297\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.5802 | Train Loss: 1.4211 | Time: 0.2317\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.3333 | Train Loss: 1.9156 | Time: 0.2737\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.3511 | Train Loss: 2.0116 | Time: 0.2408\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.3691 | Train Loss: 1.7997 | Time: 0.2407\n",
            "Epoch 00002 | Valid Acc: 0.4227 | Valid loss: 1.6646 | Time: nan\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.5100 | Train Loss: 1.8735 | Time: 0.2413\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.3818 | Train Loss: 1.8376 | Time: 0.2209\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.3361 | Train Loss: 2.0361 | Time: 0.2529\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.5694 | Train Loss: 1.2893 | Time: 0.2354\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.3269 | Train Loss: 2.1039 | Time: 0.2589\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.3659 | Train Loss: 1.8578 | Time: 0.2336\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.5570 | Train Loss: 1.3615 | Time: 0.2514\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.4286 | Train Loss: 1.6227 | Time: 0.1989\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.7013 | Train Loss: 1.2183 | Time: 0.2404\n",
            "Epoch 00003 | Valid Acc: 0.3429 | Valid loss: 2.1065 | Time: nan\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.6715 | Train Loss: 1.5155 | Time: 0.2472\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.4096 | Train Loss: 1.9696 | Time: 0.2451\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.2750 | Train Loss: 2.3151 | Time: 0.2358\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.3359 | Train Loss: 2.2810 | Time: 0.2785\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.5625 | Train Loss: 1.6650 | Time: 0.2561\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.2857 | Train Loss: 2.6004 | Time: 0.2310\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.4583 | Train Loss: 1.5917 | Time: 0.2110\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.3731 | Train Loss: 1.6717 | Time: 0.2234\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.6606 | Train Loss: 1.1115 | Time: 0.2008\n",
            "Epoch 00004 | Valid Acc: 0.4688 | Valid loss: 1.5138 | Time: 10.7443\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.7273 | Train Loss: 0.9173 | Time: 0.2357\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.1831 | Train Loss: 2.2352 | Time: 0.2215\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.7419 | Train Loss: 0.8811 | Time: 0.2212\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.3696 | Train Loss: 1.8345 | Time: 0.2919\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.3333 | Train Loss: 1.8036 | Time: 0.2555\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.8472 | Train Loss: 0.8795 | Time: 0.2343\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.8250 | Train Loss: 0.6560 | Time: 0.2391\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.3790 | Train Loss: 1.8614 | Time: 0.2442\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.4219 | Train Loss: 2.1486 | Time: 0.2343\n",
            "Epoch 00005 | Valid Acc: 0.4676 | Valid loss: 1.6152 | Time: 10.8503\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.3387 | Train Loss: 1.8124 | Time: 0.2367\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.7500 | Train Loss: 0.9217 | Time: 0.2278\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.3043 | Train Loss: 2.0761 | Time: 0.2382\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.6988 | Train Loss: 1.0723 | Time: 0.2505\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.4000 | Train Loss: 1.9076 | Time: 0.2674\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.6812 | Train Loss: 1.5292 | Time: 0.2068\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.5647 | Train Loss: 1.5339 | Time: 0.2483\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.5000 | Train Loss: 1.5822 | Time: 0.2469\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.8496 | Train Loss: 0.6761 | Time: 0.2348\n",
            "Epoch 00006 | Valid Acc: 0.4451 | Valid loss: 1.7561 | Time: 10.8483\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.6800 | Train Loss: 0.8918 | Time: 0.2232\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.6923 | Train Loss: 1.1327 | Time: 0.2193\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.4253 | Train Loss: 1.8914 | Time: 0.2365\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.3840 | Train Loss: 1.8595 | Time: 0.2758\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.6800 | Train Loss: 1.0260 | Time: 0.2187\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.2656 | Train Loss: 2.8321 | Time: 0.2316\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.4625 | Train Loss: 1.6387 | Time: 0.2410\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.8952 | Train Loss: 0.3843 | Time: 0.2401\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.4156 | Train Loss: 1.9019 | Time: 0.2458\n",
            "Epoch 00007 | Valid Acc: 0.4688 | Valid loss: 1.8409 | Time: 10.8665\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.6800 | Train Loss: 1.4764 | Time: 0.2033\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.3444 | Train Loss: 2.2252 | Time: 0.2341\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.7917 | Train Loss: 0.5343 | Time: 0.2374\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.2676 | Train Loss: 2.6402 | Time: 0.2776\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.8333 | Train Loss: 0.7875 | Time: 0.2135\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.8794 | Train Loss: 0.4238 | Time: 0.2310\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.3529 | Train Loss: 2.3747 | Time: 0.2225\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.5400 | Train Loss: 1.4626 | Time: 0.2659\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.3488 | Train Loss: 2.1080 | Time: 0.2526\n",
            "Epoch 00008 | Valid Acc: 0.4451 | Valid loss: 1.6535 | Time: 10.8883\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.8750 | Train Loss: 0.3685 | Time: 0.2403\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.3023 | Train Loss: 2.0626 | Time: 0.2619\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.4898 | Train Loss: 1.5996 | Time: 0.2279\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.2923 | Train Loss: 2.2306 | Time: 0.2543\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.7547 | Train Loss: 0.8727 | Time: 0.2322\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.7222 | Train Loss: 0.6828 | Time: 0.1982\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.4211 | Train Loss: 1.8131 | Time: 0.2330\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.5278 | Train Loss: 1.4596 | Time: 0.2404\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.5278 | Train Loss: 1.4438 | Time: 0.2379\n",
            "Epoch 00009 | Valid Acc: 0.3878 | Valid loss: 1.9290 | Time: 10.8684\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.3023 | Train Loss: 2.1591 | Time: 0.2517\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.8974 | Train Loss: 0.5042 | Time: 0.2394\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.6721 | Train Loss: 1.1768 | Time: 0.2442\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.3516 | Train Loss: 2.1682 | Time: 0.2661\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.5030 | Train Loss: 1.5481 | Time: 0.2343\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.3111 | Train Loss: 1.8678 | Time: 0.2361\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.6471 | Train Loss: 1.1270 | Time: 0.2623\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.6809 | Train Loss: 1.0092 | Time: 0.2443\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.9012 | Train Loss: 0.3366 | Time: 0.2164\n",
            "Epoch 00010 | Valid Acc: 0.4327 | Valid loss: 1.8127 | Time: 10.8956\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.6364 | Train Loss: 1.3983 | Time: 0.1786\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.5385 | Train Loss: 2.9809 | Time: 0.2370\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.4194 | Train Loss: 2.1227 | Time: 0.2250\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.3535 | Train Loss: 1.9156 | Time: 0.2768\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.7022 | Train Loss: 0.9502 | Time: 0.2478\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.4722 | Train Loss: 1.6113 | Time: 0.2660\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.9103 | Train Loss: 0.2373 | Time: 0.1989\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.6937 | Train Loss: 1.0792 | Time: 0.2330\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.6737 | Train Loss: 1.1570 | Time: 0.2384\n",
            "Epoch 00011 | Valid Acc: 0.4165 | Valid loss: 1.6861 | Time: 10.8915\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.2157 | Train Loss: 2.3267 | Time: 0.2303\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.9342 | Train Loss: 0.2994 | Time: 0.2357\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.2162 | Train Loss: 3.2546 | Time: 0.2295\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.3950 | Train Loss: 1.7393 | Time: 0.2559\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.5244 | Train Loss: 1.5392 | Time: 0.2385\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.4651 | Train Loss: 1.9004 | Time: 0.2377\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.3483 | Train Loss: 2.2359 | Time: 0.2274\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.4262 | Train Loss: 1.6504 | Time: 0.2632\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.6875 | Train Loss: 0.8894 | Time: 0.2614\n",
            "Epoch 00012 | Valid Acc: 0.4514 | Valid loss: 1.6298 | Time: 10.8900\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.5652 | Train Loss: 0.9611 | Time: 0.2000\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.7000 | Train Loss: 0.9045 | Time: 0.2259\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.5556 | Train Loss: 1.3238 | Time: 0.2419\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.5500 | Train Loss: 1.7026 | Time: 0.1978\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.7667 | Train Loss: 0.7878 | Time: 0.2572\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.7353 | Train Loss: 0.7750 | Time: 0.2349\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.5622 | Train Loss: 1.3597 | Time: 0.2326\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.6484 | Train Loss: 1.3112 | Time: 0.2701\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.3600 | Train Loss: 2.0387 | Time: 0.2291\n",
            "Epoch 00013 | Valid Acc: 0.4613 | Valid loss: 1.6963 | Time: 10.8810\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.6429 | Train Loss: 1.1419 | Time: 0.2142\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.4390 | Train Loss: 1.7294 | Time: 0.2428\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.4248 | Train Loss: 1.9109 | Time: 0.2428\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.8558 | Train Loss: 0.5953 | Time: 0.2354\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.8655 | Train Loss: 0.4569 | Time: 0.2685\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.7422 | Train Loss: 0.8636 | Time: 0.2490\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.4506 | Train Loss: 1.8330 | Time: 0.2541\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.6250 | Train Loss: 1.4171 | Time: 0.2099\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.4800 | Train Loss: 1.7103 | Time: 0.2240\n",
            "Epoch 00014 | Valid Acc: 0.4676 | Valid loss: 1.6203 | Time: 10.8993\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.5283 | Train Loss: 1.3322 | Time: 0.2373\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.8021 | Train Loss: 0.7098 | Time: 0.2291\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.8837 | Train Loss: 0.3963 | Time: 0.2361\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.5926 | Train Loss: 1.1660 | Time: 0.1913\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.7667 | Train Loss: 0.6321 | Time: 0.2223\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.4324 | Train Loss: 1.8836 | Time: 0.2663\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.5056 | Train Loss: 1.5314 | Time: 0.2419\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.5200 | Train Loss: 1.6650 | Time: 0.2549\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.8506 | Train Loss: 0.4149 | Time: 0.2374\n",
            "Epoch 00015 | Valid Acc: 0.4626 | Valid loss: 1.6859 | Time: 10.9196\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.7768 | Train Loss: 0.6499 | Time: 0.2472\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.4231 | Train Loss: 1.7006 | Time: 0.2326\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.6838 | Train Loss: 1.0787 | Time: 0.2241\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.6147 | Train Loss: 1.1297 | Time: 0.2721\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.8319 | Train Loss: 0.5309 | Time: 0.2159\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.6270 | Train Loss: 1.0718 | Time: 0.2265\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.3607 | Train Loss: 1.9593 | Time: 0.2397\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.3125 | Train Loss: 1.9505 | Time: 0.2197\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.5714 | Train Loss: 1.3438 | Time: 0.2506\n",
            "Epoch 00016 | Valid Acc: 0.5025 | Valid loss: 1.5789 | Time: 10.9076\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.5204 | Train Loss: 1.4883 | Time: 0.2427\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.6352 | Train Loss: 0.9393 | Time: 0.2426\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.8797 | Train Loss: 0.5383 | Time: 0.1905\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.3846 | Train Loss: 1.9963 | Time: 0.2627\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.2857 | Train Loss: 2.1876 | Time: 0.2681\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.4737 | Train Loss: 1.2636 | Time: 0.1924\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.5556 | Train Loss: 1.2932 | Time: 0.2080\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.7434 | Train Loss: 0.7923 | Time: 0.2447\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.8028 | Train Loss: 0.7308 | Time: 0.2202\n",
            "Epoch 00017 | Valid Acc: 0.5062 | Valid loss: 1.5943 | Time: 10.9110\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.6038 | Train Loss: 1.0074 | Time: 0.2655\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.4429 | Train Loss: 1.8354 | Time: 0.2378\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.4536 | Train Loss: 1.9753 | Time: 0.2471\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.5610 | Train Loss: 1.3315 | Time: 0.2731\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.7264 | Train Loss: 1.1146 | Time: 0.2387\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.4815 | Train Loss: 2.0710 | Time: 0.2220\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.3934 | Train Loss: 1.8310 | Time: 0.2218\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.6180 | Train Loss: 1.2540 | Time: 0.2700\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.5545 | Train Loss: 1.4084 | Time: 0.2377\n",
            "Epoch 00018 | Valid Acc: 0.4875 | Valid loss: 1.5618 | Time: 10.8992\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.7407 | Train Loss: 0.7804 | Time: 0.2332\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.7091 | Train Loss: 1.0806 | Time: 0.2371\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.5000 | Train Loss: 1.3139 | Time: 0.2061\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.5868 | Train Loss: 1.2602 | Time: 0.2588\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.6087 | Train Loss: 1.0849 | Time: 0.1955\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.7127 | Train Loss: 0.9261 | Time: 0.2432\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.6560 | Train Loss: 1.1094 | Time: 0.2647\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.7170 | Train Loss: 1.0826 | Time: 0.2233\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.4068 | Train Loss: 1.9021 | Time: 0.2329\n",
            "Epoch 00019 | Valid Acc: 0.4751 | Valid loss: 1.6215 | Time: 10.8933\n",
            "\n",
            "Training time: : 13.4093s | Batch time: : 0.2374s\n",
            "100% 391/391 [01:00<00:00,  6.43it/s]\n",
            "100% 391/391 [01:06<00:00,  5.92it/s]\n",
            "Test Acc: 0.4949\n",
            "\n",
            "Total time: : 140.3816s | Batch time: : 0.2374s\n"
          ]
        }
      ],
      "source": [
        "!python ecm.py -d am --l2norm 5e-4 --n-bases 40 --testing --gpu 0 --fanout=16 --num-parts 27 --cluster-size 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viGB435b2Wzx"
      },
      "source": [
        "### This section is specifically for \"--fanout=-1\"\n",
        "(which means after the clustring process, we use full batches to train.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ollsB3io2FZL",
        "outputId": "70fb258f-97cd-479e-b108-f92d3899decc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=50, data_cpu=False, dataset='mutag', dropout=0, fanout=-1, gpu=0, l2norm=0.0005, lr=0.01, model_path=None, n_bases=30, n_epochs=20, n_hidden=16, n_layers=2, use_self_loop=False, validation=False)\n",
            "Done loading data from cached files.\n",
            "Count of subgraphs: 3\n",
            "start training...\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.4000 | Train Loss: 0.6985 | Time: 0.1771\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.5493 | Train Loss: 0.6869 | Time: 0.0666\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.5938 | Train Loss: 0.6809 | Time: 0.1398\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "Epoch 00000 | Valid Acc: 0.6103 | Valid loss: 0.6723 | Time: nan\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.6329 | Train Loss: 0.6469 | Time: 0.1555\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.5532 | Train Loss: 0.6830 | Time: 0.0796\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.6465 | Train Loss: 0.6612 | Time: 0.1596\n",
            "Epoch 00001 | Valid Acc: 0.6103 | Valid loss: 0.6797 | Time: nan\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.6582 | Train Loss: 0.6074 | Time: 0.0817\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.6000 | Train Loss: 0.7585 | Time: 0.1355\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.5841 | Train Loss: 0.7958 | Time: 0.1601\n",
            "Epoch 00002 | Valid Acc: 0.6103 | Valid loss: 0.6780 | Time: nan\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.6146 | Train Loss: 0.7103 | Time: 0.1809\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.6329 | Train Loss: 0.5699 | Time: 0.0778\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.5876 | Train Loss: 0.6723 | Time: 0.1374\n",
            "Epoch 00003 | Valid Acc: 0.6103 | Valid loss: 0.6724 | Time: nan\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.6733 | Train Loss: 0.6366 | Time: 0.1590\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.5844 | Train Loss: 0.6739 | Time: 0.0795\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.5745 | Train Loss: 0.6816 | Time: 0.0652\n",
            "Epoch 00004 | Valid Acc: 0.6066 | Valid loss: 0.6833 | Time: 0.8657\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.6410 | Train Loss: 0.6468 | Time: 0.0792\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.6111 | Train Loss: 0.6595 | Time: 0.1637\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.6442 | Train Loss: 0.6596 | Time: 0.1443\n",
            "Epoch 00005 | Valid Acc: 0.5441 | Valid loss: 0.6890 | Time: 0.9352\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.4200 | Train Loss: 0.7194 | Time: 0.2173\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.6329 | Train Loss: 0.6649 | Time: 0.0814\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.5914 | Train Loss: 0.7725 | Time: 0.1470\n",
            "Epoch 00006 | Valid Acc: 0.5956 | Valid loss: 0.6830 | Time: 0.9724\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.6931 | Train Loss: 0.6356 | Time: 0.1624\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.5500 | Train Loss: 0.6989 | Time: 0.0663\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.6264 | Train Loss: 0.6342 | Time: 0.0789\n",
            "Epoch 00007 | Valid Acc: 0.6103 | Valid loss: 0.6781 | Time: 0.9450\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.6040 | Train Loss: 0.6621 | Time: 0.1470\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.7385 | Train Loss: 0.5704 | Time: 0.1566\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.6604 | Train Loss: 0.5688 | Time: 0.0732\n",
            "Epoch 00008 | Valid Acc: 0.6103 | Valid loss: 0.6800 | Time: 0.9462\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.5876 | Train Loss: 0.6731 | Time: 0.1439\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.6625 | Train Loss: 0.6356 | Time: 0.0650\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.6737 | Train Loss: 0.5897 | Time: 0.1770\n",
            "Epoch 00009 | Valid Acc: 0.6103 | Valid loss: 0.6812 | Time: 0.9472\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.6634 | Train Loss: 0.5543 | Time: 0.1472\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.6139 | Train Loss: 0.6129 | Time: 0.1532\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.6000 | Train Loss: 0.6423 | Time: 0.0844\n",
            "Epoch 00010 | Valid Acc: 0.5956 | Valid loss: 0.6845 | Time: 0.9509\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.6333 | Train Loss: 0.5936 | Time: 0.0703\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.6522 | Train Loss: 0.6304 | Time: 0.1577\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.5773 | Train Loss: 0.6460 | Time: 0.1733\n",
            "Epoch 00011 | Valid Acc: 0.5625 | Valid loss: 0.6904 | Time: 0.9635\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.7419 | Train Loss: 0.5832 | Time: 0.0861\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.5660 | Train Loss: 0.5947 | Time: 0.1396\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.6442 | Train Loss: 0.6099 | Time: 0.1587\n",
            "Epoch 00012 | Valid Acc: 0.5294 | Valid loss: 0.6959 | Time: 0.9637\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.6456 | Train Loss: 0.6265 | Time: 0.1528\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.6883 | Train Loss: 0.5417 | Time: 0.0775\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.6034 | Train Loss: 0.6044 | Time: 0.1573\n",
            "Epoch 00013 | Valid Acc: 0.5184 | Valid loss: 0.7008 | Time: 0.9662\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.6495 | Train Loss: 0.5820 | Time: 0.1520\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.5600 | Train Loss: 0.7155 | Time: 0.0764\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.7400 | Train Loss: 0.5364 | Time: 0.1431\n",
            "Epoch 00014 | Valid Acc: 0.4669 | Valid loss: 0.7093 | Time: 0.9641\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.7105 | Train Loss: 0.5701 | Time: 0.0692\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.6292 | Train Loss: 0.6453 | Time: 0.1697\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.5981 | Train Loss: 0.6595 | Time: 0.0720\n",
            "Epoch 00015 | Valid Acc: 0.4265 | Valid loss: 0.7206 | Time: 0.9562\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.7143 | Train Loss: 0.5446 | Time: 0.1824\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.6396 | Train Loss: 0.5912 | Time: 0.1468\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.6825 | Train Loss: 0.5227 | Time: 0.0689\n",
            "Epoch 00016 | Valid Acc: 0.4816 | Valid loss: 0.7062 | Time: 0.9591\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.7126 | Train Loss: 0.5350 | Time: 0.1817\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.6449 | Train Loss: 0.6419 | Time: 0.1361\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.6538 | Train Loss: 0.5677 | Time: 0.0632\n",
            "Epoch 00017 | Valid Acc: 0.5699 | Valid loss: 0.6930 | Time: 0.9599\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.6737 | Train Loss: 0.5435 | Time: 0.1532\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.5464 | Train Loss: 0.6483 | Time: 0.1405\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.6375 | Train Loss: 0.6038 | Time: 0.0828\n",
            "Epoch 00018 | Valid Acc: 0.5956 | Valid loss: 0.6898 | Time: 0.9587\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.5849 | Train Loss: 0.6501 | Time: 0.0821\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.7521 | Train Loss: 0.5206 | Time: 0.1552\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.7059 | Train Loss: 0.5555 | Time: 0.1425\n",
            "Epoch 00019 | Valid Acc: 0.5956 | Valid loss: 0.6865 | Time: 0.9587\n",
            "\n",
            "Training time: : 2.0113s | Batch time: : 0.1255s\n",
            "100% 8/8 [00:00<00:00, 10.54it/s]\n",
            "100% 8/8 [00:00<00:00, 10.48it/s]\n",
            "Test Acc: 0.6324\n",
            "\n",
            "Total time: : 3.5407s | Batch time: : 0.1255s\n"
          ]
        }
      ],
      "source": [
        "!python ecm.py -d mutag --l2norm 5e-4 --n-bases 30 --testing --gpu 0 --batch-size=50 --fanout=-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jv3jVugNMrln",
        "outputId": "a4483e0c-d3d2-4ee0-b02e-15cd4c0cb509"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=50, cluster_size=5, data_cpu=False, dataset='mutag', dropout=0, fanout=-1, gpu=0, l2norm=0.0005, lr=0.01, model_path=None, n_bases=30, n_epochs=20, n_hidden=16, n_layers=2, num_parts=15, use_self_loop=False, validation=False)\n",
            "Done loading data from cached files.\n",
            "Convert a graph into a bidirected graph: 0.006 seconds, peak memory: 12.514 GB\n",
            "Construct multi-constraint weights: 0.000 seconds, peak memory: 12.514 GB\n",
            "[19:26:25] /opt/dgl/src/graph/transform/metis_partition_hetero.cc:87: Partition a graph with 27163 nodes and 150326 edges into 15 parts and get 22384 edge cuts\n",
            "Metis partitioning: 0.038 seconds, peak memory: 12.519 GB\n",
            "Count of subgraphs: 3\n",
            "start training...\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.5571 | Train Loss: 0.6861 | Time: 0.0654\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.5914 | Train Loss: 0.6785 | Time: 0.1125\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.6514 | Train Loss: 0.6479 | Time: 0.1321\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "Epoch 00000 | Valid Acc: 0.6103 | Valid loss: 0.6664 | Time: nan\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.6020 | Train Loss: 0.6820 | Time: 0.0657\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.6559 | Train Loss: 0.6540 | Time: 0.1122\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.5679 | Train Loss: 0.6987 | Time: 0.0510\n",
            "Epoch 00001 | Valid Acc: 0.6103 | Valid loss: 0.6745 | Time: nan\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.6053 | Train Loss: 0.6550 | Time: 0.1246\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.6667 | Train Loss: 0.6244 | Time: 0.1258\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.5541 | Train Loss: 0.6907 | Time: 0.0486\n",
            "Epoch 00002 | Valid Acc: 0.6103 | Valid loss: 0.6688 | Time: nan\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.6111 | Train Loss: 0.6702 | Time: 0.0635\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.6262 | Train Loss: 0.6729 | Time: 0.1122\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.6000 | Train Loss: 0.6803 | Time: 0.0586\n",
            "Epoch 00003 | Valid Acc: 0.6103 | Valid loss: 0.6803 | Time: nan\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.5119 | Train Loss: 0.6981 | Time: 0.0600\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.6903 | Train Loss: 0.6516 | Time: 0.1203\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.6000 | Train Loss: 0.6811 | Time: 0.0590\n",
            "Epoch 00004 | Valid Acc: 0.5846 | Valid loss: 0.6859 | Time: 0.8458\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.6577 | Train Loss: 0.6319 | Time: 0.1359\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.6867 | Train Loss: 0.6598 | Time: 0.0479\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.5641 | Train Loss: 0.6768 | Time: 0.0648\n",
            "Epoch 00005 | Valid Acc: 0.6066 | Valid loss: 0.6828 | Time: 0.8229\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.5833 | Train Loss: 0.6817 | Time: 0.1362\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.5217 | Train Loss: 0.6935 | Time: 0.0615\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.5972 | Train Loss: 0.6593 | Time: 0.0463\n",
            "Epoch 00006 | Valid Acc: 0.6103 | Valid loss: 0.6780 | Time: 0.8137\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.6034 | Train Loss: 0.6565 | Time: 0.0581\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.6190 | Train Loss: 0.6550 | Time: 0.0624\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.6697 | Train Loss: 0.6196 | Time: 0.1134\n",
            "Epoch 00007 | Valid Acc: 0.6103 | Valid loss: 0.6746 | Time: 0.8054\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.4921 | Train Loss: 0.6746 | Time: 0.0559\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.6373 | Train Loss: 0.6379 | Time: 0.0614\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.6822 | Train Loss: 0.5925 | Time: 0.1107\n",
            "Epoch 00008 | Valid Acc: 0.6103 | Valid loss: 0.6751 | Time: 0.8115\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.6458 | Train Loss: 0.6516 | Time: 0.0657\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.6702 | Train Loss: 0.6316 | Time: 0.1138\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.5000 | Train Loss: 0.7572 | Time: 0.0514\n",
            "Epoch 00009 | Valid Acc: 0.6103 | Valid loss: 0.6768 | Time: 0.8068\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.5670 | Train Loss: 0.7320 | Time: 0.0502\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.6991 | Train Loss: 0.6208 | Time: 0.1355\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.5968 | Train Loss: 0.6106 | Time: 0.0492\n",
            "Epoch 00010 | Valid Acc: 0.6103 | Valid loss: 0.6756 | Time: 0.8029\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.5875 | Train Loss: 0.6152 | Time: 0.0506\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.6571 | Train Loss: 0.6453 | Time: 0.1224\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.6207 | Train Loss: 0.5954 | Time: 0.1144\n",
            "Epoch 00011 | Valid Acc: 0.5919 | Valid loss: 0.6821 | Time: 0.8087\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.6395 | Train Loss: 0.6555 | Time: 0.1192\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.7222 | Train Loss: 0.6241 | Time: 0.1168\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.5521 | Train Loss: 0.6577 | Time: 0.1079\n",
            "Epoch 00012 | Valid Acc: 0.5478 | Valid loss: 0.6876 | Time: 0.8198\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.5319 | Train Loss: 0.6588 | Time: 0.0537\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.7528 | Train Loss: 0.5694 | Time: 0.1292\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.6292 | Train Loss: 0.6628 | Time: 0.1033\n",
            "Epoch 00013 | Valid Acc: 0.5588 | Valid loss: 0.6856 | Time: 0.8229\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.5632 | Train Loss: 0.6235 | Time: 0.0575\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.6923 | Train Loss: 0.5443 | Time: 0.1273\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.7160 | Train Loss: 0.5882 | Time: 0.1188\n",
            "Epoch 00014 | Valid Acc: 0.5772 | Valid loss: 0.6830 | Time: 0.8267\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.6024 | Train Loss: 0.6469 | Time: 0.1207\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.5727 | Train Loss: 0.6514 | Time: 0.1321\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.6709 | Train Loss: 0.5904 | Time: 0.0616\n",
            "Epoch 00015 | Valid Acc: 0.5993 | Valid loss: 0.6800 | Time: 0.8316\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.5116 | Train Loss: 0.6586 | Time: 0.1138\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.6518 | Train Loss: 0.6143 | Time: 0.1185\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.7432 | Train Loss: 0.4921 | Time: 0.0597\n",
            "Epoch 00016 | Valid Acc: 0.5993 | Valid loss: 0.6801 | Time: 0.8340\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.6981 | Train Loss: 0.5029 | Time: 0.0637\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.6754 | Train Loss: 0.6172 | Time: 0.1192\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.5714 | Train Loss: 0.6575 | Time: 0.0528\n",
            "Epoch 00017 | Valid Acc: 0.6140 | Valid loss: 0.6816 | Time: 0.8354\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.6977 | Train Loss: 0.5673 | Time: 0.1099\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.6548 | Train Loss: 0.5535 | Time: 0.0616\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.5882 | Train Loss: 0.6141 | Time: 0.1637\n",
            "Epoch 00018 | Valid Acc: 0.5993 | Valid loss: 0.6830 | Time: 0.8395\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.6234 | Train Loss: 0.6879 | Time: 0.1280\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.6289 | Train Loss: 0.6075 | Time: 0.1251\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.6224 | Train Loss: 0.6445 | Time: 0.0468\n",
            "Epoch 00019 | Valid Acc: 0.6066 | Valid loss: 0.6829 | Time: 0.8415\n",
            "\n",
            "Training time: : 1.7348s | Batch time: : 0.0905s\n",
            "100% 8/8 [00:00<00:00, 13.41it/s]\n",
            "100% 8/8 [00:00<00:00, 13.31it/s]\n",
            "Test Acc: 0.6765\n",
            "\n",
            "Total time: : 2.9372s | Batch time: : 0.0905s\n"
          ]
        }
      ],
      "source": [
        "!python ecm.py -d mutag --l2norm 5e-4 --n-bases 30 --testing --gpu 0 --batch-size=50 --fanout=-1 --num-parts 15 --cluster-size 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcRiG5BZ2Vp9",
        "outputId": "8fc209f6-d244-4460-ba1c-65259e1376c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=100, data_cpu=False, dataset='bgs', dropout=0, fanout=-1, gpu=0, l2norm=0.0005, lr=0.01, model_path=None, n_bases=40, n_epochs=20, n_hidden=16, n_layers=2, use_self_loop=False, validation=False)\n",
            "Done loading data from cached files.\n",
            "Count of subgraphs: 2\n",
            "start training...\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.0000 | Train Loss: 1.1527 | Time: 0.2428\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.5526 | Train Loss: 0.7661 | Time: 0.2311\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "Epoch 00000 | Valid Acc: 0.5726 | Valid loss: 0.7100 | Time: nan\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.8000 | Train Loss: 0.4649 | Time: 0.2410\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.6078 | Train Loss: 1.0371 | Time: 0.2307\n",
            "Epoch 00001 | Valid Acc: 0.6496 | Valid loss: 0.8970 | Time: nan\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.6000 | Train Loss: 1.1476 | Time: 0.2481\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.7647 | Train Loss: 0.6329 | Time: 0.2775\n",
            "Epoch 00002 | Valid Acc: 0.6410 | Valid loss: 0.7813 | Time: nan\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.5000 | Train Loss: 0.6744 | Time: 0.2467\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.6139 | Train Loss: 0.7625 | Time: 0.2361\n",
            "Epoch 00003 | Valid Acc: 0.6496 | Valid loss: 0.7086 | Time: nan\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.8421 | Train Loss: 0.5540 | Time: 0.2831\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.6122 | Train Loss: 0.8617 | Time: 0.2272\n",
            "Epoch 00004 | Valid Acc: 0.5983 | Valid loss: 0.6731 | Time: 2.0185\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.6020 | Train Loss: 0.6253 | Time: 0.2491\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.7895 | Train Loss: 0.4548 | Time: 0.2710\n",
            "Epoch 00005 | Valid Acc: 0.5043 | Valid loss: 0.6891 | Time: 2.0162\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.7895 | Train Loss: 0.5032 | Time: 0.2379\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.4796 | Train Loss: 0.7852 | Time: 0.2230\n",
            "Epoch 00006 | Valid Acc: 0.4530 | Valid loss: 0.7547 | Time: 2.0119\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.7699 | Train Loss: 0.4515 | Time: 0.2532\n",
            "Epoch 00007 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.2679 | Time: 0.2164\n",
            "Epoch 00007 | Valid Acc: 0.4872 | Valid loss: 0.7526 | Time: 2.0106\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.8557 | Train Loss: 0.3436 | Time: 0.2381\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.7500 | Train Loss: 0.5204 | Time: 0.2267\n",
            "Epoch 00008 | Valid Acc: 0.6154 | Valid loss: 0.6889 | Time: 2.0096\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.5841 | Train Loss: 0.7425 | Time: 0.2569\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.5000 | Train Loss: 0.7288 | Time: 0.2656\n",
            "Epoch 00009 | Valid Acc: 0.6068 | Valid loss: 0.6399 | Time: 2.0161\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.6087 | Train Loss: 0.8028 | Time: 0.2457\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.5000 | Train Loss: 0.6831 | Time: 0.2220\n",
            "Epoch 00010 | Valid Acc: 0.5983 | Valid loss: 0.6322 | Time: 2.0151\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.8333 | Train Loss: 0.5197 | Time: 0.2239\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.6757 | Train Loss: 0.5961 | Time: 0.2218\n",
            "Epoch 00011 | Valid Acc: 0.6154 | Valid loss: 0.6301 | Time: 2.0125\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.9109 | Train Loss: 0.1886 | Time: 0.2378\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.8125 | Train Loss: 0.2382 | Time: 0.2314\n",
            "Epoch 00012 | Valid Acc: 0.6410 | Valid loss: 0.6268 | Time: 2.0074\n",
            "Epoch 00013 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.4237 | Time: 0.2257\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.8522 | Train Loss: 0.3242 | Time: 0.2437\n",
            "Epoch 00013 | Valid Acc: 0.6838 | Valid loss: 0.6313 | Time: 2.0065\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.6667 | Train Loss: 0.8891 | Time: 0.2159\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.6316 | Train Loss: 0.7266 | Time: 0.2349\n",
            "Epoch 00014 | Valid Acc: 0.7009 | Valid loss: 0.6594 | Time: 2.0074\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.5686 | Train Loss: 0.7021 | Time: 0.2374\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.7333 | Train Loss: 0.3328 | Time: 0.2763\n",
            "Epoch 00015 | Valid Acc: 0.6496 | Valid loss: 0.7263 | Time: 2.0118\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.8571 | Train Loss: 0.3307 | Time: 0.2375\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.8000 | Train Loss: 0.4306 | Time: 0.2511\n",
            "Epoch 00016 | Valid Acc: 0.6154 | Valid loss: 0.7885 | Time: 2.0177\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.7895 | Train Loss: 0.4695 | Time: 0.2437\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.4898 | Train Loss: 0.9655 | Time: 0.2351\n",
            "Epoch 00017 | Valid Acc: 0.6068 | Valid loss: 0.7916 | Time: 2.0223\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.4732 | Train Loss: 1.1390 | Time: 0.2468\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.8000 | Train Loss: 0.3738 | Time: 0.2805\n",
            "Epoch 00018 | Valid Acc: 0.7009 | Valid loss: 0.6560 | Time: 2.0241\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.8571 | Train Loss: 0.4058 | Time: 0.2356\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.8909 | Train Loss: 0.3535 | Time: 0.2385\n",
            "Epoch 00019 | Valid Acc: 0.6325 | Valid loss: 0.6565 | Time: 2.0243\n",
            "\n",
            "Training time: : 2.7606s | Batch time: : 0.2422s\n",
            "100% 11/11 [00:02<00:00,  4.95it/s]\n",
            "100% 11/11 [00:02<00:00,  4.76it/s]\n",
            "Test Acc: 0.6207\n",
            "\n",
            "Total time: : 7.3021s | Batch time: : 0.2422s\n"
          ]
        }
      ],
      "source": [
        "!python ecm.py -d bgs --l2norm 5e-4 --n-bases 40 --testing --gpu 0 --fanout=-1 --num-parts 10 --cluster-size 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEfTD7duOxPd",
        "outputId": "a343d0d9-2bde-43ce-d172-692a5208abaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=100, cluster_size=3, data_cpu=False, dataset='am', dropout=0, fanout=-1, gpu=0, l2norm=0.0005, lr=0.01, model_path=None, n_bases=40, n_epochs=20, n_hidden=16, n_layers=2, num_parts=27, use_self_loop=False, validation=False)\n",
            "Done loading data from cached files.\n",
            "Count of subgraphs: 9\n",
            "start training...\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.1444 | Train Loss: 2.3959 | Time: 0.2883\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.1783 | Train Loss: 2.3836 | Time: 0.2428\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.2000 | Train Loss: 2.3827 | Time: 0.2383\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.5290 | Train Loss: 2.3109 | Time: 0.2203\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.1935 | Train Loss: 2.3566 | Time: 0.2154\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.3510 | Train Loss: 2.2618 | Time: 0.2428\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.3462 | Train Loss: 2.1103 | Time: 0.2347\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.2632 | Train Loss: 2.2069 | Time: 0.2674\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.3333 | Train Loss: 2.2151 | Time: 0.2177\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "Epoch 00000 | Valid Acc: 0.3466 | Valid loss: 2.0608 | Time: nan\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.3333 | Train Loss: 2.2981 | Time: 0.2079\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.6947 | Train Loss: 1.2887 | Time: 0.2261\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.3619 | Train Loss: 2.1992 | Time: 0.2338\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.1613 | Train Loss: 3.0285 | Time: 0.2885\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.3010 | Train Loss: 2.2144 | Time: 0.2451\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.1905 | Train Loss: 2.5152 | Time: 0.2028\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.4222 | Train Loss: 1.9982 | Time: 0.2464\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.2660 | Train Loss: 2.1233 | Time: 0.2814\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.3077 | Train Loss: 1.7986 | Time: 0.2646\n",
            "Epoch 00001 | Valid Acc: 0.4077 | Valid loss: 2.0206 | Time: nan\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.4286 | Train Loss: 2.0809 | Time: 0.1847\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.2410 | Train Loss: 2.1333 | Time: 0.2597\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.4348 | Train Loss: 2.0660 | Time: 0.2255\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.0909 | Train Loss: 2.8343 | Time: 0.2629\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.2727 | Train Loss: 2.2872 | Time: 0.1965\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.2621 | Train Loss: 2.2134 | Time: 0.2473\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.2759 | Train Loss: 2.0469 | Time: 0.2603\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.1885 | Train Loss: 2.2636 | Time: 0.2498\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.4000 | Train Loss: 2.0736 | Time: 0.2441\n",
            "Epoch 00002 | Valid Acc: 0.3940 | Valid loss: 2.1958 | Time: nan\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.2344 | Train Loss: 2.2216 | Time: 0.2385\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.4583 | Train Loss: 2.0075 | Time: 0.2583\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.8551 | Train Loss: 1.0774 | Time: 0.1979\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.4969 | Train Loss: 1.7439 | Time: 0.2686\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.4062 | Train Loss: 2.0298 | Time: 0.2118\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.3141 | Train Loss: 1.9224 | Time: 0.2497\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.2951 | Train Loss: 1.9765 | Time: 0.2289\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.3922 | Train Loss: 1.6813 | Time: 0.2418\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.2752 | Train Loss: 1.9192 | Time: 0.2432\n",
            "Epoch 00003 | Valid Acc: 0.4152 | Valid loss: 1.9013 | Time: nan\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.2727 | Train Loss: 1.9835 | Time: 0.2358\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.7037 | Train Loss: 0.7942 | Time: 0.2292\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.1667 | Train Loss: 2.4724 | Time: 0.2200\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.3563 | Train Loss: 1.8673 | Time: 0.2726\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.3913 | Train Loss: 2.1992 | Time: 0.2299\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.5000 | Train Loss: 1.4472 | Time: 0.2336\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.4031 | Train Loss: 1.6799 | Time: 0.2442\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.4167 | Train Loss: 1.9198 | Time: 0.1831\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.6147 | Train Loss: 1.0889 | Time: 0.2186\n",
            "Epoch 00004 | Valid Acc: 0.2332 | Valid loss: 1.8678 | Time: 11.0999\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.3404 | Train Loss: 2.0128 | Time: 0.2492\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.3729 | Train Loss: 1.9105 | Time: 0.2562\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.4812 | Train Loss: 1.4385 | Time: 0.3221\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.3636 | Train Loss: 1.6329 | Time: 0.2154\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.2553 | Train Loss: 2.1178 | Time: 0.2324\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.3125 | Train Loss: 1.8890 | Time: 0.2363\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.5641 | Train Loss: 1.4249 | Time: 0.2189\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.6024 | Train Loss: 1.4355 | Time: 0.2635\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.7049 | Train Loss: 0.9555 | Time: 0.2343\n",
            "Epoch 00005 | Valid Acc: 0.4314 | Valid loss: 1.8059 | Time: 11.1189\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.5304 | Train Loss: 1.3394 | Time: 0.2452\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.5374 | Train Loss: 1.7001 | Time: 0.2435\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.6182 | Train Loss: 1.4574 | Time: 0.2382\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.7340 | Train Loss: 0.9381 | Time: 0.3064\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.5370 | Train Loss: 1.5254 | Time: 0.2375\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.3504 | Train Loss: 1.8048 | Time: 0.2485\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.5752 | Train Loss: 1.2422 | Time: 0.2241\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.4808 | Train Loss: 1.4856 | Time: 0.2749\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.1818 | Train Loss: 2.4839 | Time: 0.2384\n",
            "Epoch 00006 | Valid Acc: 0.4401 | Valid loss: 1.6760 | Time: 11.1497\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.7179 | Train Loss: 1.0088 | Time: 0.2346\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.5789 | Train Loss: 1.5719 | Time: 0.1908\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.4194 | Train Loss: 1.8627 | Time: 0.2352\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.3582 | Train Loss: 2.1258 | Time: 0.2623\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.4211 | Train Loss: 1.5559 | Time: 0.2146\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.3224 | Train Loss: 2.5899 | Time: 0.2594\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.2281 | Train Loss: 2.5544 | Time: 0.2613\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.5968 | Train Loss: 1.2969 | Time: 0.2568\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.3739 | Train Loss: 1.9561 | Time: 0.2381\n",
            "Epoch 00007 | Valid Acc: 0.4414 | Valid loss: 1.6309 | Time: 11.0961\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.3636 | Train Loss: 1.9310 | Time: 0.2453\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.5789 | Train Loss: 1.1748 | Time: 0.2702\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.4500 | Train Loss: 1.9724 | Time: 0.1798\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.4107 | Train Loss: 1.9089 | Time: 0.2285\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.4898 | Train Loss: 1.3899 | Time: 0.2234\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.8200 | Train Loss: 0.5470 | Time: 0.2359\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.6092 | Train Loss: 1.2111 | Time: 0.2298\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.5926 | Train Loss: 1.4260 | Time: 0.2516\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.8026 | Train Loss: 0.6953 | Time: 0.2486\n",
            "Epoch 00008 | Valid Acc: 0.4065 | Valid loss: 1.7153 | Time: 11.0900\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.3696 | Train Loss: 1.6506 | Time: 0.2451\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.5270 | Train Loss: 1.4860 | Time: 0.1995\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.5069 | Train Loss: 1.7312 | Time: 0.2339\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.3376 | Train Loss: 2.0489 | Time: 0.2464\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.5981 | Train Loss: 1.1354 | Time: 0.2372\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.3883 | Train Loss: 1.4143 | Time: 0.2002\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.3710 | Train Loss: 1.9446 | Time: 0.2302\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.4000 | Train Loss: 1.6548 | Time: 0.2487\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.3146 | Train Loss: 1.9544 | Time: 0.2624\n",
            "Epoch 00009 | Valid Acc: 0.4875 | Valid loss: 1.6458 | Time: 11.0595\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.5797 | Train Loss: 1.5795 | Time: 0.2419\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.7732 | Train Loss: 0.8849 | Time: 0.2434\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.3770 | Train Loss: 1.8524 | Time: 0.2271\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.3774 | Train Loss: 1.9222 | Time: 0.2854\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.7500 | Train Loss: 1.2033 | Time: 0.1957\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.4776 | Train Loss: 1.8566 | Time: 0.2272\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.4630 | Train Loss: 1.6648 | Time: 0.2649\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.5203 | Train Loss: 1.4588 | Time: 0.2821\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.5786 | Train Loss: 1.3705 | Time: 0.2401\n",
            "Epoch 00010 | Valid Acc: 0.4401 | Valid loss: 1.6880 | Time: 11.0555\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.6818 | Train Loss: 1.1215 | Time: 0.1991\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.5955 | Train Loss: 1.2682 | Time: 0.2350\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.3919 | Train Loss: 2.1459 | Time: 0.2450\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.2867 | Train Loss: 2.8374 | Time: 0.2452\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.5238 | Train Loss: 2.0168 | Time: 0.2032\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.7677 | Train Loss: 0.6916 | Time: 0.2292\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.7143 | Train Loss: 1.0004 | Time: 0.2283\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.5629 | Train Loss: 1.4100 | Time: 0.2725\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.3600 | Train Loss: 1.7688 | Time: 0.2062\n",
            "Epoch 00011 | Valid Acc: 0.5112 | Valid loss: 1.5494 | Time: 11.0800\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.6944 | Train Loss: 0.9115 | Time: 0.2625\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.4259 | Train Loss: 1.6756 | Time: 0.2411\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.4351 | Train Loss: 1.6349 | Time: 0.3141\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.5273 | Train Loss: 1.3099 | Time: 0.2339\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.3778 | Train Loss: 1.8509 | Time: 0.2475\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.6852 | Train Loss: 1.2725 | Time: 0.2279\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.4941 | Train Loss: 1.5251 | Time: 0.3036\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.1955 | Train Loss: 2.2619 | Time: 0.2499\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.7033 | Train Loss: 0.9260 | Time: 0.2378\n",
            "Epoch 00012 | Valid Acc: 0.3055 | Valid loss: 1.9198 | Time: 11.1047\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.2500 | Train Loss: 1.9020 | Time: 0.2767\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.6790 | Train Loss: 0.9049 | Time: 0.2436\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.4892 | Train Loss: 1.1786 | Time: 0.2391\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.3571 | Train Loss: 1.7607 | Time: 0.2428\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.5122 | Train Loss: 1.9173 | Time: 0.2248\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.4878 | Train Loss: 1.9418 | Time: 0.2462\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.5385 | Train Loss: 1.5157 | Time: 0.2273\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.3871 | Train Loss: 1.8024 | Time: 0.2439\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.5222 | Train Loss: 1.3884 | Time: 0.2421\n",
            "Epoch 00013 | Valid Acc: 0.4963 | Valid loss: 1.7041 | Time: 11.1130\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.8293 | Train Loss: 0.5803 | Time: 0.2449\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.5000 | Train Loss: 1.5299 | Time: 0.2051\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.4911 | Train Loss: 1.4606 | Time: 0.2451\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.5750 | Train Loss: 1.3837 | Time: 0.2296\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.5417 | Train Loss: 1.2816 | Time: 0.2544\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.9568 | Train Loss: 0.2380 | Time: 0.2347\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.5200 | Train Loss: 1.6109 | Time: 0.2370\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.5000 | Train Loss: 1.5090 | Time: 0.2160\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.4444 | Train Loss: 1.7910 | Time: 0.2278\n",
            "Epoch 00014 | Valid Acc: 0.4688 | Valid loss: 1.6314 | Time: 11.0964\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.3613 | Train Loss: 2.0144 | Time: 0.2703\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.5301 | Train Loss: 1.3490 | Time: 0.2411\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.4512 | Train Loss: 1.7791 | Time: 0.2480\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.6640 | Train Loss: 1.0415 | Time: 0.2628\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.8632 | Train Loss: 0.3735 | Time: 0.2706\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.5882 | Train Loss: 1.2575 | Time: 0.2107\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.3676 | Train Loss: 1.8849 | Time: 0.2267\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.5695 | Train Loss: 1.4443 | Time: 0.2415\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.7750 | Train Loss: 0.8053 | Time: 0.2189\n",
            "Epoch 00015 | Valid Acc: 0.4900 | Valid loss: 1.5935 | Time: 11.0997\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.8061 | Train Loss: 0.5758 | Time: 0.2137\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.4675 | Train Loss: 1.6234 | Time: 0.2415\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.5208 | Train Loss: 1.4270 | Time: 0.2384\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.7500 | Train Loss: 0.7760 | Time: 0.2538\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.4337 | Train Loss: 2.2033 | Time: 0.2442\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.3947 | Train Loss: 1.7063 | Time: 0.2442\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.6634 | Train Loss: 1.2473 | Time: 0.2458\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.5556 | Train Loss: 1.2053 | Time: 0.2458\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.8673 | Train Loss: 0.6319 | Time: 0.2518\n",
            "Epoch 00016 | Valid Acc: 0.5000 | Valid loss: 1.5491 | Time: 11.0925\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.5641 | Train Loss: 1.2933 | Time: 0.2469\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.8889 | Train Loss: 0.4168 | Time: 0.2674\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.4048 | Train Loss: 1.6463 | Time: 0.3077\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.6333 | Train Loss: 1.0992 | Time: 0.2465\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.5263 | Train Loss: 1.7249 | Time: 0.2256\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.6639 | Train Loss: 0.9079 | Time: 0.2431\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.9182 | Train Loss: 0.5218 | Time: 0.2458\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.3968 | Train Loss: 1.8740 | Time: 0.2298\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.8037 | Train Loss: 0.7466 | Time: 0.2473\n",
            "Epoch 00017 | Valid Acc: 0.5212 | Valid loss: 1.4991 | Time: 11.0899\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.7043 | Train Loss: 0.9954 | Time: 0.2483\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.2439 | Train Loss: 2.5705 | Time: 0.2472\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.4649 | Train Loss: 1.5867 | Time: 0.2464\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.5000 | Train Loss: 1.5509 | Time: 0.2259\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.3662 | Train Loss: 2.0753 | Time: 0.2625\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.7727 | Train Loss: 0.9054 | Time: 0.2179\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.2364 | Train Loss: 2.8346 | Time: 0.2238\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.6545 | Train Loss: 0.9646 | Time: 0.2528\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.6176 | Train Loss: 1.1677 | Time: 0.2130\n",
            "Epoch 00018 | Valid Acc: 0.5224 | Valid loss: 1.4505 | Time: 11.0939\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.6429 | Train Loss: 1.0772 | Time: 0.2013\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.4400 | Train Loss: 1.4825 | Time: 0.2530\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.7426 | Train Loss: 0.8133 | Time: 0.2320\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.4870 | Train Loss: 1.5594 | Time: 0.2742\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.6082 | Train Loss: 1.3247 | Time: 0.2354\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.5357 | Train Loss: 1.3848 | Time: 0.2222\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.4671 | Train Loss: 1.7321 | Time: 0.2456\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.7297 | Train Loss: 0.9031 | Time: 0.2262\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.7636 | Train Loss: 0.7596 | Time: 0.2342\n",
            "Epoch 00019 | Valid Acc: 0.5274 | Valid loss: 1.4392 | Time: 11.0770\n",
            "\n",
            "Training time: : 13.5396s | Batch time: : 0.2406s\n",
            "100% 176/176 [00:28<00:00,  6.08it/s]\n",
            "100% 176/176 [00:31<00:00,  5.58it/s]\n",
            "Test Acc: 0.5051\n",
            "\n",
            "Total time: : 74.1259s | Batch time: : 0.2406s\n"
          ]
        }
      ],
      "source": [
        "!python ecm.py -d am --l2norm 5e-4 --n-bases 40 --testing --gpu 0 --fanout=-1 --num-parts 27 --cluster-size 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lj6d--RojE7Q",
        "outputId": "9972ee82-acbc-4fe6-d9ce-fd2f4b1db219"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=100, cluster_size=3, data_cpu=False, dataset='am', dropout=0, fanout=-1, gpu=0, l2norm=0.0005, lr=0.01, model_path=None, n_bases=40, n_epochs=20, n_hidden=16, n_layers=2, num_parts=27, use_self_loop=False, validation=False)\n",
            "Done loading data from cached files.\n",
            "Count of subgraphs: 9\n",
            "start training...\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.0900 | Train Loss: 2.3997 | Time: 0.2532\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.0840 | Train Loss: 2.3909 | Time: 0.2512\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.1205 | Train Loss: 2.3624 | Time: 0.2653\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.1509 | Train Loss: 2.3840 | Time: 0.2644\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.5960 | Train Loss: 2.1450 | Time: 0.1975\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.3333 | Train Loss: 2.2310 | Time: 0.2302\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.3292 | Train Loss: 2.2300 | Time: 0.2517\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.0984 | Train Loss: 2.3651 | Time: 0.2743\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.1515 | Train Loss: 2.5194 | Time: 0.2196\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "Epoch 00000 | Valid Acc: 0.3466 | Valid loss: 1.9877 | Time: nan\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.2971 | Train Loss: 2.1335 | Time: 0.2462\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.1379 | Train Loss: 2.5713 | Time: 0.2742\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.2500 | Train Loss: 2.1314 | Time: 0.2354\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.5714 | Train Loss: 1.6368 | Time: 0.1872\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.2500 | Train Loss: 2.0955 | Time: 0.2491\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.9000 | Train Loss: 0.7096 | Time: 0.2050\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.3966 | Train Loss: 1.6999 | Time: 0.2488\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.3377 | Train Loss: 2.0446 | Time: 0.2785\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.1959 | Train Loss: 2.0688 | Time: 0.2411\n",
            "Epoch 00001 | Valid Acc: 0.2007 | Valid loss: 2.0474 | Time: nan\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.3333 | Train Loss: 2.0120 | Time: 0.2590\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.4667 | Train Loss: 2.0830 | Time: 0.1808\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.1429 | Train Loss: 1.9776 | Time: 0.2102\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.0960 | Train Loss: 2.1865 | Time: 0.2622\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.3931 | Train Loss: 2.0414 | Time: 0.2431\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.3966 | Train Loss: 1.9374 | Time: 0.2377\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.3375 | Train Loss: 1.9466 | Time: 0.2321\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.2500 | Train Loss: 2.0782 | Time: 0.2691\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.3738 | Train Loss: 1.9776 | Time: 0.2351\n",
            "Epoch 00002 | Valid Acc: 0.3978 | Valid loss: 1.8064 | Time: nan\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.4300 | Train Loss: 1.5304 | Time: 0.2478\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.4237 | Train Loss: 1.7861 | Time: 0.2392\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.6826 | Train Loss: 1.1214 | Time: 0.2504\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.4696 | Train Loss: 1.9044 | Time: 0.3136\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.4079 | Train Loss: 1.5892 | Time: 0.2449\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.5000 | Train Loss: 1.8727 | Time: 0.2208\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.2727 | Train Loss: 2.2318 | Time: 0.2530\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.2823 | Train Loss: 2.4377 | Time: 0.3066\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.5556 | Train Loss: 1.7401 | Time: 0.1820\n",
            "Epoch 00003 | Valid Acc: 0.4377 | Valid loss: 1.8314 | Time: nan\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.5098 | Train Loss: 1.7435 | Time: 0.2574\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.8476 | Train Loss: 0.6123 | Time: 0.2114\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.6111 | Train Loss: 1.7320 | Time: 0.1909\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.3299 | Train Loss: 2.4235 | Time: 0.3062\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.2105 | Train Loss: 2.6557 | Time: 0.2423\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.3400 | Train Loss: 2.7273 | Time: 0.2546\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.3448 | Train Loss: 2.5331 | Time: 0.2377\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.5546 | Train Loss: 1.5361 | Time: 0.2207\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.3846 | Train Loss: 1.7248 | Time: 0.2494\n",
            "Epoch 00004 | Valid Acc: 0.2569 | Valid loss: 1.8017 | Time: 11.1389\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.4673 | Train Loss: 1.7770 | Time: 0.2409\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.5124 | Train Loss: 1.4998 | Time: 0.2472\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.4805 | Train Loss: 1.4933 | Time: 0.2196\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.5517 | Train Loss: 1.3148 | Time: 0.2461\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.2045 | Train Loss: 2.4259 | Time: 0.2403\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.3814 | Train Loss: 1.8222 | Time: 0.2417\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.4714 | Train Loss: 1.5514 | Time: 0.2409\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.5500 | Train Loss: 1.5183 | Time: 0.2695\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.3556 | Train Loss: 1.9523 | Time: 0.2175\n",
            "Epoch 00005 | Valid Acc: 0.4850 | Valid loss: 1.5595 | Time: 11.1468\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.7500 | Train Loss: 0.8556 | Time: 0.2464\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.2881 | Train Loss: 2.0686 | Time: 0.2443\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.4762 | Train Loss: 1.3769 | Time: 0.2036\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.5062 | Train Loss: 1.4548 | Time: 0.2765\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.5281 | Train Loss: 1.4637 | Time: 0.2619\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.3861 | Train Loss: 1.6580 | Time: 0.2461\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.5556 | Train Loss: 1.8624 | Time: 0.2418\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.3000 | Train Loss: 2.2969 | Time: 0.2526\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.8750 | Train Loss: 0.5977 | Time: 0.1954\n",
            "Epoch 00006 | Valid Acc: 0.4152 | Valid loss: 1.6158 | Time: 11.1230\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.3413 | Train Loss: 1.6664 | Time: 0.2606\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.2679 | Train Loss: 2.1223 | Time: 0.2410\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.4778 | Train Loss: 1.5579 | Time: 0.2450\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.9574 | Train Loss: 0.3533 | Time: 0.2357\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.4000 | Train Loss: 1.9275 | Time: 0.2677\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.7097 | Train Loss: 1.2171 | Time: 0.2477\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.6737 | Train Loss: 1.0309 | Time: 0.2273\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.5536 | Train Loss: 1.3852 | Time: 0.2269\n",
            "Epoch 00007 | Batch 000 | Train Acc: 0.6462 | Train Loss: 1.1273 | Time: 0.2355\n",
            "Epoch 00007 | Valid Acc: 0.4489 | Valid loss: 1.7472 | Time: 11.1104\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.4375 | Train Loss: 1.8437 | Time: 0.2408\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.3397 | Train Loss: 1.9960 | Time: 0.2563\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.5238 | Train Loss: 1.5576 | Time: 0.2209\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.6914 | Train Loss: 1.1080 | Time: 0.2477\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.4737 | Train Loss: 1.3815 | Time: 0.1814\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.4600 | Train Loss: 1.6562 | Time: 0.2758\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.6165 | Train Loss: 1.3795 | Time: 0.2404\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.4000 | Train Loss: 1.9774 | Time: 0.3072\n",
            "Epoch 00008 | Batch 000 | Train Acc: 0.6604 | Train Loss: 1.1856 | Time: 0.2419\n",
            "Epoch 00008 | Valid Acc: 0.4863 | Valid loss: 1.5994 | Time: 11.1559\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.5862 | Train Loss: 1.4216 | Time: 0.2082\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.4200 | Train Loss: 1.6402 | Time: 0.2402\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.5395 | Train Loss: 1.3469 | Time: 0.2362\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.4416 | Train Loss: 1.9049 | Time: 0.2787\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.5169 | Train Loss: 1.4295 | Time: 0.2505\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.7895 | Train Loss: 0.7238 | Time: 0.2609\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.3474 | Train Loss: 2.3779 | Time: 0.2518\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.3563 | Train Loss: 2.1189 | Time: 0.2677\n",
            "Epoch 00009 | Batch 000 | Train Acc: 0.6410 | Train Loss: 1.3351 | Time: 0.2386\n",
            "Epoch 00009 | Valid Acc: 0.4751 | Valid loss: 1.5678 | Time: 11.1516\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.8831 | Train Loss: 0.4634 | Time: 0.2436\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.5789 | Train Loss: 1.5842 | Time: 0.2309\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.6842 | Train Loss: 0.9515 | Time: 0.2309\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.2796 | Train Loss: 2.5023 | Time: 0.2473\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.5615 | Train Loss: 1.5029 | Time: 0.2735\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.6549 | Train Loss: 1.0200 | Time: 0.2606\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.7949 | Train Loss: 0.6635 | Time: 0.2425\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.7206 | Train Loss: 0.8480 | Time: 0.2626\n",
            "Epoch 00010 | Batch 000 | Train Acc: 0.6341 | Train Loss: 1.0530 | Time: 0.2332\n",
            "Epoch 00010 | Valid Acc: 0.4713 | Valid loss: 1.5097 | Time: 11.1654\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.7647 | Train Loss: 0.6026 | Time: 0.2359\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.6538 | Train Loss: 1.2765 | Time: 0.2062\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.6400 | Train Loss: 1.3044 | Time: 0.2698\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.6750 | Train Loss: 1.0331 | Time: 0.2825\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.7184 | Train Loss: 0.9447 | Time: 0.2465\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.6122 | Train Loss: 1.0602 | Time: 0.2503\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.4255 | Train Loss: 1.9903 | Time: 0.2419\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.7239 | Train Loss: 1.0924 | Time: 0.2231\n",
            "Epoch 00011 | Batch 000 | Train Acc: 0.3158 | Train Loss: 1.9848 | Time: 0.2243\n",
            "Epoch 00011 | Valid Acc: 0.4127 | Valid loss: 1.6272 | Time: 11.1772\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.7083 | Train Loss: 0.8745 | Time: 0.2442\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.6849 | Train Loss: 0.9524 | Time: 0.2407\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.4762 | Train Loss: 1.7152 | Time: 0.2373\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.1579 | Train Loss: 2.5369 | Time: 0.2489\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.8088 | Train Loss: 0.6494 | Time: 0.2479\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.3462 | Train Loss: 1.9482 | Time: 0.2439\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.7209 | Train Loss: 1.0130 | Time: 0.2375\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.4545 | Train Loss: 1.8521 | Time: 0.2689\n",
            "Epoch 00012 | Batch 000 | Train Acc: 0.8636 | Train Loss: 0.3175 | Time: 0.2622\n",
            "Epoch 00012 | Valid Acc: 0.5100 | Valid loss: 1.5154 | Time: 11.1945\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.4384 | Train Loss: 1.8255 | Time: 0.2485\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.6588 | Train Loss: 1.1110 | Time: 0.2453\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.7449 | Train Loss: 0.8319 | Time: 0.2549\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.5366 | Train Loss: 2.0313 | Time: 0.2647\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.9211 | Train Loss: 0.3068 | Time: 0.2301\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.8242 | Train Loss: 0.5915 | Time: 0.2360\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.5854 | Train Loss: 1.3875 | Time: 0.2419\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.7357 | Train Loss: 0.8582 | Time: 0.3142\n",
            "Epoch 00013 | Batch 000 | Train Acc: 0.8333 | Train Loss: 0.4505 | Time: 0.2416\n",
            "Epoch 00013 | Valid Acc: 0.4713 | Valid loss: 1.5430 | Time: 11.2206\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.7333 | Train Loss: 0.8153 | Time: 0.1975\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.2963 | Train Loss: 1.8554 | Time: 0.2365\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.6901 | Train Loss: 0.9218 | Time: 0.2839\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.6375 | Train Loss: 1.3883 | Time: 0.2417\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.5906 | Train Loss: 1.5498 | Time: 0.2450\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.4000 | Train Loss: 1.7239 | Time: 0.2527\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.3333 | Train Loss: 1.9944 | Time: 0.2298\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.4048 | Train Loss: 1.8119 | Time: 0.2482\n",
            "Epoch 00014 | Batch 000 | Train Acc: 0.6696 | Train Loss: 1.3742 | Time: 0.2311\n",
            "Epoch 00014 | Valid Acc: 0.4825 | Valid loss: 1.5322 | Time: 11.2236\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.6900 | Train Loss: 0.9821 | Time: 0.2458\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.6220 | Train Loss: 1.1006 | Time: 0.2607\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.7836 | Train Loss: 0.6805 | Time: 0.2498\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.4487 | Train Loss: 1.7003 | Time: 0.2795\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.6667 | Train Loss: 1.0351 | Time: 0.2139\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.3036 | Train Loss: 2.5007 | Time: 0.2444\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.7476 | Train Loss: 0.9312 | Time: 0.2884\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.7049 | Train Loss: 0.8020 | Time: 0.2328\n",
            "Epoch 00015 | Batch 000 | Train Acc: 0.6667 | Train Loss: 1.2752 | Time: 0.2240\n",
            "Epoch 00015 | Valid Acc: 0.4589 | Valid loss: 1.8393 | Time: 11.2302\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.5034 | Train Loss: 1.8447 | Time: 0.2671\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.7321 | Train Loss: 0.8410 | Time: 0.2747\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.8861 | Train Loss: 0.4446 | Time: 0.2187\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.3846 | Train Loss: 1.9863 | Time: 0.2457\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.5370 | Train Loss: 2.0311 | Time: 0.2405\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.8400 | Train Loss: 0.7042 | Time: 0.2093\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.7869 | Train Loss: 0.6851 | Time: 0.2793\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.4375 | Train Loss: 1.5252 | Time: 0.1887\n",
            "Epoch 00016 | Batch 000 | Train Acc: 0.6154 | Train Loss: 1.2376 | Time: 0.2541\n",
            "Epoch 00016 | Valid Acc: 0.5362 | Valid loss: 1.5505 | Time: 11.2386\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.6371 | Train Loss: 1.4235 | Time: 0.2532\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.9432 | Train Loss: 0.4396 | Time: 0.2360\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.6667 | Train Loss: 1.5421 | Time: 0.2237\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.6250 | Train Loss: 1.1278 | Time: 0.2946\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.6639 | Train Loss: 1.3007 | Time: 0.2283\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.5455 | Train Loss: 1.3434 | Time: 0.2368\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.9111 | Train Loss: 0.4053 | Time: 0.2357\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.5256 | Train Loss: 1.5213 | Time: 0.2635\n",
            "Epoch 00017 | Batch 000 | Train Acc: 0.6296 | Train Loss: 1.0065 | Time: 0.2653\n",
            "Epoch 00017 | Valid Acc: 0.4825 | Valid loss: 1.6648 | Time: 11.2410\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.5423 | Train Loss: 1.3726 | Time: 0.2620\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.6923 | Train Loss: 1.1033 | Time: 0.2239\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.7273 | Train Loss: 0.8709 | Time: 0.2690\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.5472 | Train Loss: 1.5689 | Time: 0.2342\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.8167 | Train Loss: 0.5408 | Time: 0.2293\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.2437 | Train Loss: 2.3228 | Time: 0.2485\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.6180 | Train Loss: 1.2481 | Time: 0.2699\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.6667 | Train Loss: 1.3471 | Time: 0.2161\n",
            "Epoch 00018 | Batch 000 | Train Acc: 0.4865 | Train Loss: 1.6825 | Time: 0.2247\n",
            "Epoch 00018 | Valid Acc: 0.5000 | Valid loss: 1.5183 | Time: 11.2462\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.7586 | Train Loss: 0.8752 | Time: 0.2149\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.6533 | Train Loss: 1.1606 | Time: 0.2520\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.7076 | Train Loss: 0.8214 | Time: 0.2469\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.5000 | Train Loss: 1.5591 | Time: 0.1810\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.5208 | Train Loss: 1.4138 | Time: 0.2538\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.6783 | Train Loss: 1.0364 | Time: 0.2722\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.4337 | Train Loss: 1.5747 | Time: 0.2355\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.4110 | Train Loss: 2.0981 | Time: 0.2300\n",
            "Epoch 00019 | Batch 000 | Train Acc: 0.8356 | Train Loss: 0.7724 | Time: 0.2092\n",
            "Epoch 00019 | Valid Acc: 0.4776 | Valid loss: 1.4394 | Time: 11.2486\n",
            "\n",
            "Training time: : 14.1198s | Batch time: : 0.2439s\n",
            "100% 292/292 [00:48<00:00,  6.06it/s]\n",
            "100% 292/292 [00:50<00:00,  5.77it/s]\n",
            "Test Acc: 0.5303\n",
            "\n",
            "Total time: : 113.0520s | Batch time: : 0.2439s\n"
          ]
        }
      ],
      "source": [
        "!python ecm.py -d am --l2norm 5e-4 --n-bases 40 --testing --gpu 0 --fanout=-1 --num-parts 27 --cluster-size 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPjxMaqIPr4p",
        "outputId": "622cac1e-c0e0-4080-f4a3-9c038ffda1bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=100, cluster_size=1, data_cpu=False, dataset='am', dropout=0, fanout=-1, gpu=0, l2norm=0.0005, lr=0.01, model_path=None, n_bases=40, n_epochs=20, n_hidden=16, n_layers=2, num_parts=9, use_self_loop=False, validation=False)\n",
            "Done loading data from cached files.\n",
            "Convert a graph into a bidirected graph: 0.358 seconds, peak memory: 13.592 GB\n",
            "Construct multi-constraint weights: 0.000 seconds, peak memory: 13.592 GB\n",
            "[09:43:50] /opt/dgl/src/graph/transform/metis_partition_hetero.cc:87: Partition a graph with 1885136 nodes and 6080376 edges into 9 parts and get 317476 edge cuts\n",
            "Metis partitioning: 2.624 seconds, peak memory: 13.961 GB\n",
            "Count of subgraphs: 9\n",
            "start training...\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.1111 | Train Loss: 2.4037 | Time: 0.1765\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.0791 | Train Loss: 2.4035 | Time: 0.2738\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.4922 | Train Loss: 2.3474 | Time: 0.2292\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.2857 | Train Loss: 2.3376 | Time: 0.2018\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.3616 | Train Loss: 2.3238 | Time: 0.2319\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.0795 | Train Loss: 2.4467 | Time: 0.2117\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.3636 | Train Loss: 2.1026 | Time: 0.1993\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.0222 | Train Loss: 2.3594 | Time: 0.2348\n",
            "Epoch 00000 | Batch 000 | Train Acc: 0.5814 | Train Loss: 1.8234 | Time: 0.2102\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "Epoch 00000 | Valid Acc: 0.3466 | Valid loss: 2.0313 | Time: nan\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.0222 | Train Loss: 2.2089 | Time: 0.2356\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.3333 | Train Loss: 1.7458 | Time: 0.2021\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.3333 | Train Loss: 2.1481 | Time: 0.1705\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.2590 | Train Loss: 2.0838 | Time: 0.2680\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.5000 | Train Loss: 1.2251 | Time: 0.1995\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.2614 | Train Loss: 1.7842 | Time: 0.2186\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.7674 | Train Loss: 1.1129 | Time: 0.2278\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.4972 | Train Loss: 1.5598 | Time: 0.2341\n",
            "Epoch 00001 | Batch 000 | Train Acc: 0.1395 | Train Loss: 1.8687 | Time: 0.2137\n",
            "Epoch 00001 | Valid Acc: 0.1683 | Valid loss: 2.1111 | Time: nan\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.7111 | Train Loss: 1.3121 | Time: 0.2364\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.1860 | Train Loss: 2.0124 | Time: 0.2130\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.7778 | Train Loss: 1.1094 | Time: 0.1739\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.5455 | Train Loss: 1.5501 | Time: 0.1918\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.7159 | Train Loss: 1.0668 | Time: 0.2111\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.4762 | Train Loss: 1.3385 | Time: 0.1971\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.7829 | Train Loss: 0.9368 | Time: 0.2316\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.8705 | Train Loss: 0.5114 | Time: 0.2587\n",
            "Epoch 00002 | Batch 000 | Train Acc: 0.7571 | Train Loss: 1.0162 | Time: 0.2319\n",
            "Epoch 00002 | Valid Acc: 0.4027 | Valid loss: 2.0329 | Time: nan\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.9574 | Train Loss: 0.3032 | Time: 0.2372\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.8095 | Train Loss: 0.5793 | Time: 0.2041\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.8136 | Train Loss: 0.7272 | Time: 0.2400\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.8140 | Train Loss: 0.4958 | Time: 0.2062\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.9568 | Train Loss: 0.1488 | Time: 0.2613\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.9545 | Train Loss: 0.1939 | Time: 0.1941\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.9333 | Train Loss: 0.2548 | Time: 0.2326\n",
            "Epoch 00003 | Batch 000 | Train Acc: 0.8750 | Train Loss: 0.4593 | Time: 0.2152\n",
            "Epoch 00003 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0105 | Time: 0.1699\n",
            "Epoch 00003 | Valid Acc: 0.4002 | Valid loss: 1.9643 | Time: nan\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.9205 | Train Loss: 0.3491 | Time: 0.2184\n",
            "Epoch 00004 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0840 | Time: 0.2144\n",
            "Epoch 00004 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0254 | Time: 0.1960\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.9718 | Train Loss: 0.1460 | Time: 0.2321\n",
            "Epoch 00004 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0157 | Time: 0.2718\n",
            "Epoch 00004 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0670 | Time: 0.2068\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.9612 | Train Loss: 0.1144 | Time: 0.2271\n",
            "Epoch 00004 | Batch 000 | Train Acc: 0.9778 | Train Loss: 0.1051 | Time: 0.2299\n",
            "Epoch 00004 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0025 | Time: 0.1690\n",
            "Epoch 00004 | Valid Acc: 0.4252 | Valid loss: 2.0198 | Time: 11.3612\n",
            "Epoch 00005 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0340 | Time: 0.2371\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.9884 | Train Loss: 0.0339 | Time: 0.2299\n",
            "Epoch 00005 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0102 | Time: 0.1971\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.9886 | Train Loss: 0.0452 | Time: 0.2498\n",
            "Epoch 00005 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0005 | Time: 0.1880\n",
            "Epoch 00005 | Batch 000 | Train Acc: 0.9778 | Train Loss: 0.0495 | Time: 0.2266\n",
            "Epoch 00005 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0092 | Time: 0.2589\n",
            "Epoch 00005 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0065 | Time: 0.1715\n",
            "Epoch 00005 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0010 | Time: 0.2163\n",
            "Epoch 00005 | Valid Acc: 0.3554 | Valid loss: 2.0101 | Time: 11.2916\n",
            "Epoch 00006 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0042 | Time: 0.1752\n",
            "Epoch 00006 | Batch 000 | Train Acc: 0.9961 | Train Loss: 0.0056 | Time: 0.2298\n",
            "Epoch 00006 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0058 | Time: 0.2161\n",
            "Epoch 00006 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0006 | Time: 0.1939\n",
            "Epoch 00006 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0031 | Time: 0.2348\n",
            "Epoch 00006 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0007 | Time: 0.2144\n",
            "Epoch 00006 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0053 | Time: 0.2341\n",
            "Epoch 00006 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0010 | Time: 0.2009\n",
            "Epoch 00006 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0044 | Time: 0.2987\n",
            "Epoch 00006 | Valid Acc: 0.3628 | Valid loss: 2.0035 | Time: 11.2845\n",
            "Epoch 00007 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0002 | Time: 0.2003\n",
            "Epoch 00007 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0024 | Time: 0.2307\n",
            "Epoch 00007 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0038 | Time: 0.2616\n",
            "Epoch 00007 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0011 | Time: 0.2084\n",
            "Epoch 00007 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.2081\n",
            "Epoch 00007 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.2059\n",
            "Epoch 00007 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0004 | Time: 0.2329\n",
            "Epoch 00007 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0005 | Time: 0.2281\n",
            "Epoch 00007 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.2000\n",
            "Epoch 00007 | Valid Acc: 0.3641 | Valid loss: 1.9927 | Time: 11.2706\n",
            "Epoch 00008 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0019 | Time: 0.2724\n",
            "Epoch 00008 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0007 | Time: 0.2295\n",
            "Epoch 00008 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0009 | Time: 0.2163\n",
            "Epoch 00008 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0005 | Time: 0.2347\n",
            "Epoch 00008 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0004 | Time: 0.2354\n",
            "Epoch 00008 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0000 | Time: 0.2078\n",
            "Epoch 00008 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.2304\n",
            "Epoch 00008 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.1694\n",
            "Epoch 00008 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.2076\n",
            "Epoch 00008 | Valid Acc: 0.3566 | Valid loss: 1.9819 | Time: 11.2797\n",
            "Epoch 00009 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.1757\n",
            "Epoch 00009 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0006 | Time: 0.2370\n",
            "Epoch 00009 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.1991\n",
            "Epoch 00009 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0009 | Time: 0.2592\n",
            "Epoch 00009 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0011 | Time: 0.2162\n",
            "Epoch 00009 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0006 | Time: 0.2324\n",
            "Epoch 00009 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0004 | Time: 0.2334\n",
            "Epoch 00009 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.1941\n",
            "Epoch 00009 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0000 | Time: 0.2137\n",
            "Epoch 00009 | Valid Acc: 0.3641 | Valid loss: 1.9669 | Time: 11.2894\n",
            "Epoch 00010 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0012 | Time: 0.2167\n",
            "Epoch 00010 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0007 | Time: 0.2357\n",
            "Epoch 00010 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0000 | Time: 0.1726\n",
            "Epoch 00010 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.2039\n",
            "Epoch 00010 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0009 | Time: 0.2608\n",
            "Epoch 00010 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0000 | Time: 0.2070\n",
            "Epoch 00010 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0010 | Time: 0.2349\n",
            "Epoch 00010 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.1891\n",
            "Epoch 00010 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0005 | Time: 0.2346\n",
            "Epoch 00010 | Valid Acc: 0.3641 | Valid loss: 1.9507 | Time: 11.2815\n",
            "Epoch 00011 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.2011\n",
            "Epoch 00011 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0005 | Time: 0.2300\n",
            "Epoch 00011 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0012 | Time: 0.2084\n",
            "Epoch 00011 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0000 | Time: 0.2520\n",
            "Epoch 00011 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0000 | Time: 0.1711\n",
            "Epoch 00011 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0010 | Time: 0.2346\n",
            "Epoch 00011 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0012 | Time: 0.2269\n",
            "Epoch 00011 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0012 | Time: 0.3030\n",
            "Epoch 00011 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.1973\n",
            "Epoch 00011 | Valid Acc: 0.3666 | Valid loss: 1.9328 | Time: 11.2797\n",
            "Epoch 00012 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0005 | Time: 0.2338\n",
            "Epoch 00012 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0000 | Time: 0.1711\n",
            "Epoch 00012 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0011 | Time: 0.2130\n",
            "Epoch 00012 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0014 | Time: 0.2701\n",
            "Epoch 00012 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0000 | Time: 0.2170\n",
            "Epoch 00012 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.1939\n",
            "Epoch 00012 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0015 | Time: 0.2756\n",
            "Epoch 00012 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0017 | Time: 0.2783\n",
            "Epoch 00012 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.2060\n",
            "Epoch 00012 | Valid Acc: 0.3691 | Valid loss: 1.9151 | Time: 11.2891\n",
            "Epoch 00013 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0017 | Time: 0.2382\n",
            "Epoch 00013 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.1955\n",
            "Epoch 00013 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.1980\n",
            "Epoch 00013 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0000 | Time: 0.1685\n",
            "Epoch 00013 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0006 | Time: 0.2249\n",
            "Epoch 00013 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0014 | Time: 0.2300\n",
            "Epoch 00013 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0013 | Time: 0.2182\n",
            "Epoch 00013 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0018 | Time: 0.2939\n",
            "Epoch 00013 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.2084\n",
            "Epoch 00013 | Valid Acc: 0.4252 | Valid loss: 1.8923 | Time: 11.2920\n",
            "Epoch 00014 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.1989\n",
            "Epoch 00014 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0014 | Time: 0.2205\n",
            "Epoch 00014 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.2037\n",
            "Epoch 00014 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0015 | Time: 0.2727\n",
            "Epoch 00014 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0000 | Time: 0.1685\n",
            "Epoch 00014 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0016 | Time: 0.2366\n",
            "Epoch 00014 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0021 | Time: 0.2754\n",
            "Epoch 00014 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.2223\n",
            "Epoch 00014 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0008 | Time: 0.2307\n",
            "Epoch 00014 | Valid Acc: 0.4401 | Valid loss: 1.8704 | Time: 11.3053\n",
            "Epoch 00015 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.2193\n",
            "Epoch 00015 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.1712\n",
            "Epoch 00015 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0013 | Time: 0.2165\n",
            "Epoch 00015 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.2298\n",
            "Epoch 00015 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0009 | Time: 0.2418\n",
            "Epoch 00015 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.2033\n",
            "Epoch 00015 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0017 | Time: 0.2309\n",
            "Epoch 00015 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0024 | Time: 0.2709\n",
            "Epoch 00015 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0022 | Time: 0.2602\n",
            "Epoch 00015 | Valid Acc: 0.4501 | Valid loss: 1.8535 | Time: 11.3068\n",
            "Epoch 00016 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.1704\n",
            "Epoch 00016 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0009 | Time: 0.2310\n",
            "Epoch 00016 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.2105\n",
            "Epoch 00016 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0024 | Time: 0.2979\n",
            "Epoch 00016 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.2041\n",
            "Epoch 00016 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0013 | Time: 0.2105\n",
            "Epoch 00016 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.1935\n",
            "Epoch 00016 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0023 | Time: 0.2326\n",
            "Epoch 00016 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0037 | Time: 0.2350\n",
            "Epoch 00016 | Valid Acc: 0.4539 | Valid loss: 1.8404 | Time: 11.3039\n",
            "Epoch 00017 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.1954\n",
            "Epoch 00017 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0014 | Time: 0.2663\n",
            "Epoch 00017 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0014 | Time: 0.2175\n",
            "Epoch 00017 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0008 | Time: 0.2654\n",
            "Epoch 00017 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0025 | Time: 0.2289\n",
            "Epoch 00017 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.1676\n",
            "Epoch 00017 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.2126\n",
            "Epoch 00017 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0049 | Time: 0.2335\n",
            "Epoch 00017 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.2070\n",
            "Epoch 00017 | Valid Acc: 0.4489 | Valid loss: 1.8295 | Time: 11.3078\n",
            "Epoch 00018 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0013 | Time: 0.2183\n",
            "Epoch 00018 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.2024\n",
            "Epoch 00018 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0043 | Time: 0.2373\n",
            "Epoch 00018 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.2467\n",
            "Epoch 00018 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0009 | Time: 0.2298\n",
            "Epoch 00018 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0012 | Time: 0.2612\n",
            "Epoch 00018 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.1686\n",
            "Epoch 00018 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0020 | Time: 0.2628\n",
            "Epoch 00018 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.1917\n",
            "Epoch 00018 | Valid Acc: 0.4439 | Valid loss: 1.8129 | Time: 11.3068\n",
            "Epoch 00019 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0028 | Time: 0.2420\n",
            "Epoch 00019 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0009 | Time: 0.2328\n",
            "Epoch 00019 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.1699\n",
            "Epoch 00019 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0015 | Time: 0.2637\n",
            "Epoch 00019 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0023 | Time: 0.2317\n",
            "Epoch 00019 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.1996\n",
            "Epoch 00019 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0012 | Time: 0.2159\n",
            "Epoch 00019 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.2050\n",
            "Epoch 00019 | Batch 000 | Train Acc: 1.0000 | Train Loss: 0.0001 | Time: 0.2071\n",
            "Epoch 00019 | Valid Acc: 0.4252 | Valid loss: 1.7937 | Time: 11.3073\n",
            "\n",
            "Training time: : 14.0026s | Batch time: : 0.2210s\n",
            "100% 440/440 [01:07<00:00,  6.49it/s]\n",
            "100% 440/440 [01:12<00:00,  6.09it/s]\n",
            "Test Acc: 0.4949\n",
            "\n",
            "Total time: : 154.1052s | Batch time: : 0.2210s\n"
          ]
        }
      ],
      "source": [
        "!python ecm.py -d am --l2norm 5e-4 --n-bases 40 --testing --gpu 0 --fanout=-1 --num-parts 9 --cluster-size 1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "DGL-ClusterGCN_Pipeline",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

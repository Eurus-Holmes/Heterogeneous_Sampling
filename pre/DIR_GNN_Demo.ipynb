{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFJ9Zle-N1B2",
        "outputId": "96a75efa-f424-4ebf-eb7b-de53250fbb81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k70Vxlv9lMRZ",
        "outputId": "7e0d6b62-1bf0-4344-d9ce-b4af5a4a2d31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dir-gnn'...\n",
            "remote: Enumerating objects: 138, done.\u001b[K\n",
            "remote: Counting objects: 100% (138/138), done.\u001b[K\n",
            "remote: Compressing objects: 100% (103/103), done.\u001b[K\n",
            "remote: Total 138 (delta 59), reused 88 (delta 27), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (138/138), 6.26 MiB | 17.41 MiB/s, done.\n",
            "Resolving deltas: 100% (59/59), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Wuyxin/dir-gnn.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd dir-gnn\n",
        "%mkdir data\n",
        "%cd data\n",
        "%mkdir MNISTSP\n",
        "%cd MNISTSP\n",
        "%mkdir raw\n",
        "%cd raw\n",
        "%cp /content/drive/MyDrive/CS249/mnist_75sp_color_noise.pt .\n",
        "%cp /content/drive/MyDrive/CS249/mnist_75sp_test.pkl .\n",
        "%cp /content/drive/MyDrive/CS249/mnist_75sp_train.pkl .\n",
        "%cd ../../..\n",
        "#!pip install ml-collections"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_oOYBvHK0VM",
        "outputId": "47f69765-41f2-44de-f72c-e9f11ca74412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/dir-gnn\n",
            "/content/dir-gnn/data\n",
            "/content/dir-gnn/data/MNISTSP\n",
            "/content/dir-gnn/data/MNISTSP/raw\n",
            "/content/dir-gnn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric\n",
        "!pip install torch_sparse\n",
        "!pip install torch_scatter\n",
        "!pip install ogb\n",
        "!pip install texttable"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ss7NND46T-8B",
        "outputId": "5c9267be-9d1a-447d-9fc5-04be663ffb07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.0.4.tar.gz (407 kB)\n",
            "\u001b[?25l\r\u001b[K     |▉                               | 10 kB 32.7 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 20 kB 30.5 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 30 kB 19.3 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 40 kB 16.5 MB/s eta 0:00:01\r\u001b[K     |████                            | 51 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 61 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 71 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 81 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 92 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 102 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 112 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 122 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 133 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 143 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 153 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 163 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 174 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 184 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 194 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 204 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 215 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 225 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 235 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 245 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 256 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 266 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 276 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 286 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 296 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 307 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 317 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 327 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 337 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 348 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 358 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 368 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 378 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 389 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 399 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 407 kB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (1.3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (3.0.8)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (1.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch_geometric) (2.0.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch_geometric) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch_geometric) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch_geometric) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch_geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch_geometric) (1.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.0.4-py3-none-any.whl size=616603 sha256=625a9bd120e43ef626dacebf2a2543389b417197c868b9a87f68b9e3cc89ef67\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/a6/a4/ca18c3051fcead866fe7b85700ee2240d883562a1bc70ce421\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.0.4\n",
            "Collecting torch_sparse\n",
            "  Downloading torch_sparse-0.6.13.tar.gz (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch_sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch_sparse) (1.21.6)\n",
            "Building wheels for collected packages: torch-sparse\n",
            "  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-sparse: filename=torch_sparse-0.6.13-cp37-cp37m-linux_x86_64.whl size=1654973 sha256=9bbbb5f139a7a834ea090f9afef6c299871042469b4c016b5b319a7fe3f38b18\n",
            "  Stored in directory: /root/.cache/pip/wheels/e0/01/be/6b2966e0ff20bb023ae35e5d17903e6e5b4df46dd5892f6be6\n",
            "Successfully built torch-sparse\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.13\n",
            "Collecting torch_scatter\n",
            "  Downloading torch_scatter-2.0.9.tar.gz (21 kB)\n",
            "Building wheels for collected packages: torch-scatter\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-scatter: filename=torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl size=3870734 sha256=a2703a63b8004410a50b559b8c425a01c131e43974d9a7e8f374b3192409adc3\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/57/a3/42ea193b77378ce634eb9454c9bc1e3163f3b482a35cdee4d1\n",
            "Successfully built torch-scatter\n",
            "Installing collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.9\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.3-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 4.5 MB/s \n",
            "\u001b[?25hCollecting outdated>=0.2.0\n",
            "  Downloading outdated-0.2.1-py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.0.2)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.21.6)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.3.5)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.24.3)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.11.0+cu113)\n",
            "Collecting littleutils\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (2.23.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->ogb) (4.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=e79eb10e70d540130cc2d0c55711bfadd67ea74d3e0c9667912fceed0c5b6b03\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/64/cd/32819b511a488e4993f2fab909a95330289c3f4e0f6ef4676d\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.3 outdated-0.2.1\n",
            "Collecting texttable\n",
            "  Downloading texttable-1.6.4-py2.py3-none-any.whl (10 kB)\n",
            "Installing collected packages: texttable\n",
            "Successfully installed texttable-1.6.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m train.mnistsp_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lz_d4wzSzLB",
        "outputId": "32f18ac1-1f2e-40c3-d987-8e816b5ef641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing...\n",
            "Done!\n",
            "Processing...\n",
            "Done!\n",
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n",
            "2022-05-08 21:53:47,236 - /content/dir-gnn/utils/helper.py[line:49] - INFO: +------------+-------+\n",
            "| Parameter  | Value |\n",
            "+------------+-------+\n",
            "| cuda       | 0     |\n",
            "+------------+-------+\n",
            "| datadir    | data/ |\n",
            "+------------+-------+\n",
            "| epoch      | 400   |\n",
            "+------------+-------+\n",
            "| reg        | 1     |\n",
            "+------------+-------+\n",
            "| seed       | [1]   |\n",
            "+------------+-------+\n",
            "| channels   | 32    |\n",
            "+------------+-------+\n",
            "| commit     |       |\n",
            "+------------+-------+\n",
            "| pretrain   | 20    |\n",
            "+------------+-------+\n",
            "| alpha      | 0.000 |\n",
            "+------------+-------+\n",
            "| r          | 0.800 |\n",
            "+------------+-------+\n",
            "| batch_size | 32    |\n",
            "+------------+-------+\n",
            "| net_lr     | 0.001 |\n",
            "+------------+-------+\n",
            "2022-05-08 21:54:59,395 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [  0/400]  all_loss:2.406=[XE:2.406  IL:0.000000]  Train_ACC:0.097 Test_ACC[0.101  0.106]  Val_ACC:0.099  \n",
            "2022-05-08 21:55:59,404 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [  1/400]  all_loss:2.344=[XE:2.344  IL:0.000000]  Train_ACC:0.105 Test_ACC[0.107  0.100]  Val_ACC:0.106  \n",
            "2022-05-08 21:57:00,069 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [  2/400]  all_loss:2.310=[XE:2.310  IL:0.000001]  Train_ACC:0.108 Test_ACC[0.108  0.107]  Val_ACC:0.105  \n",
            "2022-05-08 21:58:00,476 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [  3/400]  all_loss:2.285=[XE:2.285  IL:0.000002]  Train_ACC:0.113 Test_ACC[0.108  0.123]  Val_ACC:0.109  \n",
            "2022-05-08 21:59:00,676 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [  4/400]  all_loss:2.266=[XE:2.266  IL:0.000004]  Train_ACC:0.116 Test_ACC[0.113  0.131]  Val_ACC:0.117  \n",
            "2022-05-08 22:00:01,403 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [  5/400]  all_loss:2.254=[XE:2.254  IL:0.000008]  Train_ACC:0.130 Test_ACC[0.126  0.136]  Val_ACC:0.134  \n",
            "2022-05-08 22:01:01,779 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [  6/400]  all_loss:2.243=[XE:2.243  IL:0.000015]  Train_ACC:0.147 Test_ACC[0.144  0.150]  Val_ACC:0.149  \n",
            "2022-05-08 22:02:02,741 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [  7/400]  all_loss:2.234=[XE:2.234  IL:0.000026]  Train_ACC:0.171 Test_ACC[0.163  0.163]  Val_ACC:0.170  \n",
            "2022-05-08 22:03:03,229 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [  8/400]  all_loss:2.224=[XE:2.224  IL:0.000040]  Train_ACC:0.196 Test_ACC[0.180  0.169]  Val_ACC:0.190  \n",
            "2022-05-08 22:04:03,408 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [  9/400]  all_loss:2.215=[XE:2.215  IL:0.000062]  Train_ACC:0.215 Test_ACC[0.190  0.173]  Val_ACC:0.212  \n",
            "2022-05-08 22:05:03,891 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 10/400]  all_loss:2.206=[XE:2.206  IL:0.000092]  Train_ACC:0.230 Test_ACC[0.202  0.171]  Val_ACC:0.226  \n",
            "2022-05-08 22:06:03,702 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 11/400]  all_loss:2.196=[XE:2.196  IL:0.000138]  Train_ACC:0.239 Test_ACC[0.210  0.166]  Val_ACC:0.234  \n",
            "2022-05-08 22:07:02,059 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 12/400]  all_loss:2.186=[XE:2.186  IL:0.000196]  Train_ACC:0.259 Test_ACC[0.218  0.158]  Val_ACC:0.255  \n",
            "2022-05-08 22:07:59,930 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 13/400]  all_loss:2.175=[XE:2.174  IL:0.000270]  Train_ACC:0.263 Test_ACC[0.219  0.153]  Val_ACC:0.262  \n",
            "2022-05-08 22:08:59,985 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 14/400]  all_loss:2.164=[XE:2.164  IL:0.000383]  Train_ACC:0.270 Test_ACC[0.221  0.153]  Val_ACC:0.267  \n",
            "2022-05-08 22:10:00,802 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 15/400]  all_loss:2.154=[XE:2.153  IL:0.000526]  Train_ACC:0.280 Test_ACC[0.225  0.156]  Val_ACC:0.268  \n",
            "2022-05-08 22:11:01,156 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 16/400]  all_loss:2.141=[XE:2.141  IL:0.000746]  Train_ACC:0.291 Test_ACC[0.228  0.152]  Val_ACC:0.285  \n",
            "2022-05-08 22:12:01,936 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 17/400]  all_loss:2.131=[XE:2.130  IL:0.001000]  Train_ACC:0.295 Test_ACC[0.228  0.154]  Val_ACC:0.285  \n",
            "2022-05-08 22:13:02,427 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 18/400]  all_loss:2.119=[XE:2.117  IL:0.001330]  Train_ACC:0.287 Test_ACC[0.228  0.147]  Val_ACC:0.280  \n",
            "2022-05-08 22:14:02,922 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 19/400]  all_loss:2.108=[XE:2.106  IL:0.001716]  Train_ACC:0.283 Test_ACC[0.225  0.147]  Val_ACC:0.270  \n",
            "2022-05-08 22:15:03,260 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 20/400]  all_loss:2.095=[XE:2.093  IL:0.002212]  Train_ACC:0.278 Test_ACC[0.222  0.142]  Val_ACC:0.266  \n",
            "2022-05-08 22:16:03,340 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 21/400]  all_loss:2.085=[XE:2.082  IL:0.002743]  Train_ACC:0.278 Test_ACC[0.222  0.141]  Val_ACC:0.268  \n",
            "2022-05-08 22:17:03,323 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 22/400]  all_loss:2.073=[XE:2.069  IL:0.003437]  Train_ACC:0.281 Test_ACC[0.227  0.139]  Val_ACC:0.275  \n",
            "2022-05-08 22:18:03,467 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 23/400]  all_loss:2.063=[XE:2.059  IL:0.004176]  Train_ACC:0.290 Test_ACC[0.232  0.137]  Val_ACC:0.289  \n",
            "2022-05-08 22:19:03,478 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 24/400]  all_loss:2.052=[XE:2.047  IL:0.004930]  Train_ACC:0.301 Test_ACC[0.233  0.136]  Val_ACC:0.298  \n",
            "2022-05-08 22:20:03,557 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 25/400]  all_loss:2.042=[XE:2.036  IL:0.005891]  Train_ACC:0.302 Test_ACC[0.232  0.134]  Val_ACC:0.297  \n",
            "2022-05-08 22:21:03,527 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 26/400]  all_loss:2.030=[XE:2.023  IL:0.006938]  Train_ACC:0.301 Test_ACC[0.231  0.131]  Val_ACC:0.294  \n",
            "2022-05-08 22:22:03,399 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 27/400]  all_loss:2.020=[XE:2.012  IL:0.008117]  Train_ACC:0.304 Test_ACC[0.227  0.133]  Val_ACC:0.295  \n",
            "2022-05-08 22:23:03,170 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 28/400]  all_loss:2.011=[XE:2.002  IL:0.009170]  Train_ACC:0.313 Test_ACC[0.235  0.129]  Val_ACC:0.311  \n",
            "2022-05-08 22:24:02,728 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 29/400]  all_loss:2.000=[XE:1.989  IL:0.010931]  Train_ACC:0.328 Test_ACC[0.243  0.132]  Val_ACC:0.327  \n",
            "2022-05-08 22:25:02,579 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 30/400]  all_loss:1.990=[XE:1.977  IL:0.012267]  Train_ACC:0.332 Test_ACC[0.243  0.141]  Val_ACC:0.332  \n",
            "2022-05-08 22:26:02,156 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 31/400]  all_loss:1.979=[XE:1.964  IL:0.014210]  Train_ACC:0.337 Test_ACC[0.244  0.142]  Val_ACC:0.336  \n",
            "2022-05-08 22:27:01,625 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 32/400]  all_loss:1.967=[XE:1.951  IL:0.015725]  Train_ACC:0.339 Test_ACC[0.243  0.146]  Val_ACC:0.340  \n",
            "2022-05-08 22:28:01,744 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 33/400]  all_loss:1.956=[XE:1.939  IL:0.017426]  Train_ACC:0.347 Test_ACC[0.247  0.147]  Val_ACC:0.350  \n",
            "2022-05-08 22:29:01,715 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 34/400]  all_loss:1.946=[XE:1.926  IL:0.019393]  Train_ACC:0.336 Test_ACC[0.240  0.160]  Val_ACC:0.340  \n",
            "2022-05-08 22:30:01,969 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 35/400]  all_loss:1.933=[XE:1.912  IL:0.021369]  Train_ACC:0.347 Test_ACC[0.245  0.157]  Val_ACC:0.352  \n",
            "2022-05-08 22:31:01,734 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 36/400]  all_loss:1.921=[XE:1.898  IL:0.023450]  Train_ACC:0.353 Test_ACC[0.243  0.160]  Val_ACC:0.353  \n",
            "2022-05-08 22:32:01,656 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 37/400]  all_loss:1.912=[XE:1.886  IL:0.025682]  Train_ACC:0.357 Test_ACC[0.244  0.169]  Val_ACC:0.360  \n",
            "2022-05-08 22:33:02,005 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 38/400]  all_loss:1.898=[XE:1.870  IL:0.027250]  Train_ACC:0.367 Test_ACC[0.246  0.167]  Val_ACC:0.370  \n",
            "2022-05-08 22:34:01,930 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 39/400]  all_loss:1.887=[XE:1.856  IL:0.030499]  Train_ACC:0.374 Test_ACC[0.243  0.171]  Val_ACC:0.373  \n",
            "2022-05-08 22:35:00,133 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 40/400]  all_loss:1.873=[XE:1.841  IL:0.031639]  Train_ACC:0.386 Test_ACC[0.243  0.168]  Val_ACC:0.389  \n",
            "2022-05-08 22:35:57,879 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 41/400]  all_loss:1.862=[XE:1.828  IL:0.033645]  Train_ACC:0.383 Test_ACC[0.245  0.171]  Val_ACC:0.387  \n",
            "2022-05-08 22:36:55,695 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 42/400]  all_loss:1.846=[XE:1.811  IL:0.035123]  Train_ACC:0.398 Test_ACC[0.250  0.170]  Val_ACC:0.402  \n",
            "2022-05-08 22:37:53,571 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 43/400]  all_loss:1.834=[XE:1.796  IL:0.037164]  Train_ACC:0.391 Test_ACC[0.243  0.171]  Val_ACC:0.385  \n",
            "2022-05-08 22:38:50,852 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 44/400]  all_loss:1.820=[XE:1.782  IL:0.038476]  Train_ACC:0.406 Test_ACC[0.245  0.171]  Val_ACC:0.406  \n",
            "2022-05-08 22:39:48,182 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 45/400]  all_loss:1.805=[XE:1.764  IL:0.041026]  Train_ACC:0.370 Test_ACC[0.248  0.168]  Val_ACC:0.370  \n",
            "2022-05-08 22:40:45,782 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 46/400]  all_loss:1.792=[XE:1.748  IL:0.044668]  Train_ACC:0.381 Test_ACC[0.228  0.176]  Val_ACC:0.378  \n",
            "2022-05-08 22:41:43,056 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 47/400]  all_loss:1.777=[XE:1.730  IL:0.046764]  Train_ACC:0.434 Test_ACC[0.246  0.167]  Val_ACC:0.433  \n",
            "2022-05-08 22:42:40,634 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 48/400]  all_loss:1.765=[XE:1.712  IL:0.052779]  Train_ACC:0.425 Test_ACC[0.249  0.169]  Val_ACC:0.428  \n",
            "2022-05-08 22:43:38,057 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 49/400]  all_loss:1.748=[XE:1.694  IL:0.053942]  Train_ACC:0.423 Test_ACC[0.230  0.171]  Val_ACC:0.426  \n",
            "2022-05-08 22:44:35,452 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 50/400]  all_loss:1.735=[XE:1.679  IL:0.056243]  Train_ACC:0.372 Test_ACC[0.242  0.165]  Val_ACC:0.367  \n",
            "2022-05-08 22:45:33,221 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 51/400]  all_loss:1.726=[XE:1.663  IL:0.062789]  Train_ACC:0.444 Test_ACC[0.223  0.170]  Val_ACC:0.436  \n",
            "2022-05-08 22:46:30,697 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 52/400]  all_loss:1.708=[XE:1.639  IL:0.069725]  Train_ACC:0.467 Test_ACC[0.236  0.163]  Val_ACC:0.465  \n",
            "2022-05-08 22:47:28,682 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 53/400]  all_loss:1.687=[XE:1.622  IL:0.065392]  Train_ACC:0.415 Test_ACC[0.247  0.162]  Val_ACC:0.414  \n",
            "2022-05-08 22:48:26,152 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 54/400]  all_loss:1.679=[XE:1.606  IL:0.073312]  Train_ACC:0.452 Test_ACC[0.214  0.162]  Val_ACC:0.446  \n",
            "2022-05-08 22:49:23,608 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 55/400]  all_loss:1.664=[XE:1.586  IL:0.078336]  Train_ACC:0.462 Test_ACC[0.230  0.159]  Val_ACC:0.457  \n",
            "2022-05-08 22:50:21,604 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 56/400]  all_loss:1.646=[XE:1.565  IL:0.080350]  Train_ACC:0.445 Test_ACC[0.246  0.153]  Val_ACC:0.444  \n",
            "2022-05-08 22:51:19,160 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 57/400]  all_loss:1.635=[XE:1.560  IL:0.074940]  Train_ACC:0.472 Test_ACC[0.216  0.156]  Val_ACC:0.465  \n",
            "2022-05-08 22:52:16,626 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 58/400]  all_loss:1.618=[XE:1.531  IL:0.086708]  Train_ACC:0.488 Test_ACC[0.228  0.155]  Val_ACC:0.484  \n",
            "2022-05-08 22:53:14,435 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 59/400]  all_loss:1.611=[XE:1.514  IL:0.097327]  Train_ACC:0.482 Test_ACC[0.241  0.143]  Val_ACC:0.484  \n",
            "2022-05-08 22:54:11,833 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 60/400]  all_loss:1.593=[XE:1.503  IL:0.090798]  Train_ACC:0.519 Test_ACC[0.218  0.146]  Val_ACC:0.514  \n",
            "2022-05-08 22:55:09,803 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 61/400]  all_loss:1.581=[XE:1.481  IL:0.099916]  Train_ACC:0.484 Test_ACC[0.222  0.144]  Val_ACC:0.474  \n",
            "2022-05-08 22:56:07,311 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 62/400]  all_loss:1.574=[XE:1.465  IL:0.108989]  Train_ACC:0.495 Test_ACC[0.235  0.134]  Val_ACC:0.500  \n",
            "2022-05-08 22:57:05,122 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 63/400]  all_loss:1.555=[XE:1.453  IL:0.102109]  Train_ACC:0.518 Test_ACC[0.216  0.134]  Val_ACC:0.512  \n",
            "2022-05-08 22:58:03,117 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 64/400]  all_loss:1.549=[XE:1.430  IL:0.118647]  Train_ACC:0.529 Test_ACC[0.230  0.135]  Val_ACC:0.526  \n",
            "2022-05-08 22:59:00,836 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 65/400]  all_loss:1.531=[XE:1.412  IL:0.119406]  Train_ACC:0.492 Test_ACC[0.235  0.133]  Val_ACC:0.485  \n",
            "2022-05-08 22:59:58,502 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 66/400]  all_loss:1.523=[XE:1.410  IL:0.113161]  Train_ACC:0.535 Test_ACC[0.212  0.129]  Val_ACC:0.532  \n",
            "2022-05-08 23:00:56,559 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 67/400]  all_loss:1.510=[XE:1.376  IL:0.134063]  Train_ACC:0.540 Test_ACC[0.224  0.127]  Val_ACC:0.540  \n",
            "2022-05-08 23:01:53,945 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 68/400]  all_loss:1.499=[XE:1.363  IL:0.135443]  Train_ACC:0.528 Test_ACC[0.226  0.131]  Val_ACC:0.519  \n",
            "2022-05-08 23:02:51,769 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 69/400]  all_loss:1.486=[XE:1.357  IL:0.128175]  Train_ACC:0.545 Test_ACC[0.220  0.133]  Val_ACC:0.542  \n",
            "2022-05-08 23:03:49,469 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 70/400]  all_loss:1.478=[XE:1.332  IL:0.145692]  Train_ACC:0.502 Test_ACC[0.234  0.119]  Val_ACC:0.496  \n",
            "2022-05-08 23:04:46,889 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 71/400]  all_loss:1.475=[XE:1.333  IL:0.141535]  Train_ACC:0.537 Test_ACC[0.209  0.131]  Val_ACC:0.535  \n",
            "2022-05-08 23:05:44,709 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 72/400]  all_loss:1.467=[XE:1.305  IL:0.161741]  Train_ACC:0.527 Test_ACC[0.232  0.119]  Val_ACC:0.523  \n",
            "2022-05-08 23:06:42,180 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 73/400]  all_loss:1.453=[XE:1.295  IL:0.158420]  Train_ACC:0.562 Test_ACC[0.229  0.123]  Val_ACC:0.559  \n",
            "2022-05-08 23:07:39,893 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 74/400]  all_loss:1.441=[XE:1.286  IL:0.154951]  Train_ACC:0.583 Test_ACC[0.215  0.125]  Val_ACC:0.575  \n",
            "2022-05-08 23:08:37,614 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 75/400]  all_loss:1.433=[XE:1.266  IL:0.167567]  Train_ACC:0.546 Test_ACC[0.227  0.115]  Val_ACC:0.544  \n",
            "2022-05-08 23:09:35,127 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 76/400]  all_loss:1.426=[XE:1.263  IL:0.163385]  Train_ACC:0.543 Test_ACC[0.218  0.127]  Val_ACC:0.533  \n",
            "2022-05-08 23:10:33,038 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 77/400]  all_loss:1.420=[XE:1.243  IL:0.177071]  Train_ACC:0.604 Test_ACC[0.229  0.122]  Val_ACC:0.595  \n",
            "2022-05-08 23:11:30,542 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 78/400]  all_loss:1.405=[XE:1.229  IL:0.176107]  Train_ACC:0.575 Test_ACC[0.218  0.120]  Val_ACC:0.566  \n",
            "2022-05-08 23:12:28,278 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 79/400]  all_loss:1.405=[XE:1.230  IL:0.174522]  Train_ACC:0.549 Test_ACC[0.217  0.122]  Val_ACC:0.539  \n",
            "2022-05-08 23:13:25,512 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 80/400]  all_loss:1.409=[XE:1.209  IL:0.200517]  Train_ACC:0.529 Test_ACC[0.224  0.118]  Val_ACC:0.523  \n",
            "2022-05-08 23:14:23,046 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 81/400]  all_loss:1.405=[XE:1.208  IL:0.197040]  Train_ACC:0.584 Test_ACC[0.205  0.125]  Val_ACC:0.583  \n",
            "2022-05-08 23:15:20,651 - /content/dir-gnn/train/mnistsp_dir.py[line:231] - INFO: Epoch [ 82/400]  all_loss:1.408=[XE:1.188  IL:0.220387]  Train_ACC:0.521 Test_ACC[0.220  0.120]  Val_ACC:0.513  \n",
            "2022-05-08 23:15:20,651 - /content/dir-gnn/train/mnistsp_dir.py[line:241] - INFO: Early Stopping\n",
            "2022-05-08 23:15:20,658 - /content/dir-gnn/train/mnistsp_dir.py[line:251] - INFO: ====================================================================================================\n",
            "2022-05-08 23:15:20,659 - /content/dir-gnn/train/mnistsp_dir.py[line:257] - INFO: Causal ACC:0.2202-+-nan  Conf ACC:0.1196-+-nan  Train ACC:0.5210-+-nan  Val ACC:0.5128-+-nan  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "i0TqHlVo2eqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import torch\n",
        "import argparse\n",
        "from datasets import MNIST75sp\n",
        "from torch_geometric.data import DataLoader\n",
        "\n",
        "from gnn import MNISTSPNet\n",
        "\n",
        "from torch.utils.data import random_split\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GraphConv, BatchNorm, global_mean_pool\n",
        "from torch_geometric.utils import softmax, degree\n",
        "from utils.mask import set_masks, clear_masks\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import os.path as osp\n",
        "from torch.autograd import grad\n",
        "from utils.logger import Logger\n",
        "from datetime import datetime\n",
        "from utils.helper import random_partition, set_seed, args_print\n",
        "from utils.get_subgraph import split_graph, relabel\n",
        "\n",
        "\n",
        "class CausalAttNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, causal_ratio):\n",
        "        super(CausalAttNet, self).__init__()\n",
        "        self.conv1 = GraphConv(in_channels=5, out_channels=args.channels)\n",
        "        self.conv2 = GraphConv(in_channels=args.channels, out_channels=args.channels)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(args.channels*2, args.channels*4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(args.channels*4, 1)\n",
        "        )\n",
        "        self.ratio = causal_ratio\n",
        "    def forward(self, data):\n",
        "        x = F.relu(self.conv1(data.x, data.edge_index, data.edge_attr.view(-1)))\n",
        "        x = self.conv2(x, data.edge_index, data.edge_attr.view(-1))\n",
        "\n",
        "        row, col = data.edge_index\n",
        "        edge_rep = torch.cat([x[row], x[col]], dim=-1)\n",
        "        edge_score = self.mlp(edge_rep).view(-1)\n",
        "\n",
        "        (causal_edge_index, causal_edge_attr, causal_edge_weight), \\\n",
        "        (conf_edge_index, conf_edge_attr, conf_edge_weight) = split_graph(data,edge_score, self.ratio)\n",
        "\n",
        "        causal_x, causal_edge_index, causal_batch, _ = relabel(x, causal_edge_index, data.batch)\n",
        "        conf_x, conf_edge_index, conf_batch, _ = relabel(x, conf_edge_index, data.batch)\n",
        "\n",
        "        return (causal_x, causal_edge_index, causal_edge_attr, causal_edge_weight, causal_batch),\\\n",
        "                (conf_x, conf_edge_index, conf_edge_attr, conf_edge_weight, conf_batch),\\\n",
        "                edge_score\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Arguments\n",
        "    parser = argparse.ArgumentParser(description='Training for Causal Feature Learning')\n",
        "    parser.add_argument('--cuda', default=0, type=int, help='cuda device')\n",
        "    parser.add_argument('--datadir', default='data/', type=str, help='directory for datasets.')\n",
        "    parser.add_argument('--epoch', default=400, type=int, help='training iterations')\n",
        "    parser.add_argument('--reg', default=1, type=int)\n",
        "    parser.add_argument('--seed',  nargs='?', default='[1,2,3]', help='random seed')\n",
        "    parser.add_argument('--channels', default=32, type=int, help='width of network')\n",
        "    parser.add_argument('--commit', default='', type=str, help='experiment name')\n",
        "    # hyper \n",
        "    parser.add_argument('--pretrain', default=20, type=int, help='pretrain epoch')\n",
        "    parser.add_argument('--alpha', default=1e-4, type=float, help='invariant loss')\n",
        "    parser.add_argument('--r', default=0.8, type=float, help='causal_ratio')\n",
        "    # basic\n",
        "    parser.add_argument('--batch_size', default=32, type=int, help='batch size')\n",
        "    parser.add_argument('--net_lr', default=1e-3, type=float, help='learning rate for the predictor')\n",
        "    args = parser.parse_args()\n",
        "    args.seed = eval(args.seed)\n",
        "\n",
        "    # dataset\n",
        "    num_classes = 10\n",
        "    n_train_data, n_val_data = 20000, 5000\n",
        "    device = torch.device('cuda:%d' % args.cuda if torch.cuda.is_available() else 'cpu')\n",
        "    train_val = MNIST75sp(osp.join(args.datadir, 'MNISTSP/'), mode='train')\n",
        "    perm_idx = torch.randperm(len(train_val), generator=torch.Generator().manual_seed(0))\n",
        "    train_val = train_val[perm_idx]\n",
        "    train_dataset, val_dataset = train_val[:n_train_data], train_val[-n_val_data:]\n",
        "    test_dataset = MNIST75sp(osp.join(args.datadir, 'MNISTSP/'), mode='test')\n",
        "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
        "    n_test_data = float(len(test_loader.dataset))\n",
        "\n",
        "    color_noises = torch.load(osp.join(args.datadir, 'MNISTSP/raw/mnist_75sp_color_noise.pt')).view(-1,3)\n",
        "\n",
        "    # logger\n",
        "    datetime_now = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    all_info = { 'causal_acc':[], 'conf_acc':[], 'train_acc':[], 'val_acc':[], 'test_prec':[], 'train_prec':[], 'test_mrr':[], 'train_mrr':[]}\n",
        "    experiment_name = f'mnistsp.{bool(args.reg)}.{args.commit}.netlr_{args.net_lr}.batch_{args.batch_size}'\\\n",
        "                      f'.channels_{args.channels}.pretrain_{args.pretrain}.r_{args.r}.alpha_{args.alpha}.seed_{args.seed}.{datetime_now}'\n",
        "    exp_dir = osp.join('local/', experiment_name)\n",
        "    os.makedirs(exp_dir, exist_ok=True)\n",
        "    logger = Logger.init_logger(filename=exp_dir + '/_output_.log')\n",
        "    args_print(args, logger)\n",
        "\n",
        "    for seed in args.seed:\n",
        "        set_seed(seed)\n",
        "        # models and optimizers\n",
        "        g = MNISTSPNet(args.channels).to(device)\n",
        "        att_net = CausalAttNet(args.r).to(device)\n",
        "        model_optimizer = torch.optim.Adam(\n",
        "            list(g.parameters()) +\n",
        "            list(att_net.parameters()),\n",
        "            lr=args.net_lr)\n",
        "        conf_opt = torch.optim.Adam(g.conf_mlp.parameters(), lr=args.net_lr)\n",
        "        CELoss = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "\n",
        "        def train_mode():\n",
        "            g.train();att_net.train()\n",
        "            \n",
        "        def val_mode():\n",
        "            g.eval();att_net.eval()\n",
        "\n",
        "        def test_acc(loader, att_net, predictor):\n",
        "            acc = 0\n",
        "            for graph in loader: \n",
        "                graph.to(device)\n",
        "                (causal_x, causal_edge_index, causal_edge_attr, causal_edge_weight, causal_batch),\\\n",
        "                (conf_x, conf_edge_index, conf_edge_attr, conf_edge_weight, conf_batch), edge_score = att_net(graph)\n",
        "                set_masks(causal_edge_weight, g)\n",
        "                out = predictor(x=causal_x, edge_index=causal_edge_index, \n",
        "                        edge_attr=causal_edge_attr, batch=causal_batch)\n",
        "                clear_masks(g)\n",
        "                acc += torch.sum(out.argmax(-1).view(-1) == graph.y.view(-1))\n",
        "            acc = float(acc) / len(loader.dataset)\n",
        "            return acc\n",
        "\n",
        "        cnt, last_val_acc = 0, 0\n",
        "        for epoch in range(args.epoch):\n",
        "                \n",
        "            causal_edge_weights = torch.tensor([]).to(device)\n",
        "            conf_edge_weights = torch.tensor([]).to(device)\n",
        "            alpha_prime = args.alpha * (epoch ** 1.6)\n",
        "            all_loss, n_bw, all_env_loss = 0, 0, 0\n",
        "            all_causal_loss, all_conf_loss = 0, 0\n",
        "            dummy_w = nn.Parameter(torch.Tensor([1.0])).to(device)\n",
        "            train_mode()\n",
        "            for graph in train_loader:\n",
        "                n_bw += 1\n",
        "                graph.to(device)\n",
        "                N = graph.num_graphs\n",
        "                (causal_x, causal_edge_index, causal_edge_attr, causal_edge_weight, causal_batch),\\\n",
        "                (conf_x, conf_edge_index, conf_edge_attr, conf_edge_weight, conf_batch), edge_score = att_net(graph)\n",
        "\n",
        "                set_masks(causal_edge_weight, g)\n",
        "                causal_rep = g.get_graph_rep(\n",
        "                    x=causal_x, edge_index=causal_edge_index, \n",
        "                    edge_attr=causal_edge_attr, batch=causal_batch)\n",
        "                causal_out = g.get_causal_pred(causal_rep)\n",
        "                clear_masks(g)\n",
        "                set_masks(conf_edge_weight, g)\n",
        "                conf_rep = g.get_graph_rep(\n",
        "                    x=conf_x, edge_index=conf_edge_index, \n",
        "                    edge_attr=conf_edge_attr, batch=conf_batch).detach()\n",
        "                conf_out = g.get_conf_pred(conf_rep)\n",
        "                clear_masks(g)\n",
        "                causal_loss = CELoss(causal_out, graph.y)\n",
        "                conf_loss = CELoss(conf_out, graph.y)\n",
        "\n",
        "                env_loss = 0\n",
        "                if args.reg:\n",
        "                    env_loss = torch.tensor([]).to(device)\n",
        "                    for conf in conf_rep:\n",
        "                        rep_out = g.get_comb_pred(causal_rep, conf)\n",
        "                        env_loss = torch.cat([env_loss, CELoss(rep_out, graph.y).unsqueeze(0)])\n",
        "                    causal_loss += min(alpha_prime, 1) * env_loss.mean()\n",
        "                    env_loss = alpha_prime * torch.var(env_loss * conf_rep.size(0))\n",
        "                \n",
        "\n",
        "                # logger\n",
        "                all_conf_loss += conf_loss\n",
        "                all_causal_loss += causal_loss\n",
        "                all_env_loss += env_loss\n",
        "                causal_edge_weights = torch.cat([causal_edge_weights, causal_edge_weight])\n",
        "                conf_edge_weights = torch.cat([conf_edge_weights, conf_edge_weight])\n",
        "\n",
        "            all_env_loss /= n_bw\n",
        "            all_causal_loss /= n_bw\n",
        "            all_conf_loss /= n_bw\n",
        "            all_loss = all_causal_loss + all_env_loss\n",
        "\n",
        "            conf_opt.zero_grad()\n",
        "            all_conf_loss.backward()\n",
        "            conf_opt.step()\n",
        "\n",
        "            model_optimizer.zero_grad()\n",
        "            all_loss.backward()\n",
        "            model_optimizer.step()\n",
        "            val_mode()\n",
        "            with torch.no_grad():\n",
        "\n",
        "                train_acc = test_acc(train_loader, att_net, g)\n",
        "                val_acc = test_acc(val_loader, att_net, g)\n",
        "                # testing acc\n",
        "                noise_level = 0.4\n",
        "                causal_acc, conf_acc = 0., 0.\n",
        "                for graph in test_loader: \n",
        "                    graph.to(device)\n",
        "\n",
        "                    n_samples = 0\n",
        "                    noise = color_noises[n_samples:n_samples + graph.x.size(0), :].to(device) * noise_level\n",
        "                    graph.x[:, :3] = graph.x[:, :3] + noise\n",
        "                    n_samples += graph.x.size(0)\n",
        "\n",
        "                    (causal_x, causal_edge_index, causal_edge_attr, causal_edge_weight, causal_batch),\\\n",
        "                    (conf_x, conf_edge_index, conf_edge_attr, conf_edge_weight, conf_batch), edge_score = att_net(graph)\n",
        "                    \n",
        "                    set_masks(causal_edge_weight, g)\n",
        "                    causal_out = g(\n",
        "                        x=causal_x, edge_index=causal_edge_index, \n",
        "                        edge_attr=causal_edge_attr, batch=causal_batch)\n",
        "                    set_masks(conf_edge_weight, g)\n",
        "                    conf_out = g(x=conf_x, edge_index=conf_edge_index, \n",
        "                            edge_attr=conf_edge_attr, batch=conf_batch)\n",
        "                    clear_masks(g)\n",
        "                    causal_acc += torch.sum(causal_out.argmax(-1).view(-1) == graph.y.view(-1)) / n_test_data\n",
        "                    conf_acc += torch.sum(conf_out.argmax(-1).view(-1) == graph.y.view(-1)) / n_test_data\n",
        "                        \n",
        "                        \n",
        "                logger.info(\"Epoch [{:3d}/{:d}]  all_loss:{:2.3f}=[XE:{:2.3f}  IL:{:2.6f}]  \"\n",
        "                            \"Train_ACC:{:.3f} Test_ACC[{:.3f}  {:.3f}]  Val_ACC:{:.3f}  \".format(\n",
        "                        epoch, args.epoch, all_loss, all_causal_loss, all_env_loss, \n",
        "                        train_acc, causal_acc, conf_acc, val_acc,\n",
        "                        causal_edge_weights.mean(), conf_edge_weights.mean()))\n",
        "            \n",
        "                # activate early stopping\n",
        "                if epoch >= args.pretrain:\n",
        "                    if val_acc < last_val_acc:\n",
        "                        cnt += 1\n",
        "                    else:\n",
        "                        cnt = 0\n",
        "                        last_val_acc = val_acc\n",
        "                if cnt >= 5:\n",
        "                    logger.info(\"Early Stopping\")\n",
        "                    break\n",
        "\n",
        "            \n",
        "        all_info['causal_acc'].append(causal_acc)\n",
        "        all_info['conf_acc'].append(conf_acc)\n",
        "        all_info['train_acc'].append(train_acc)\n",
        "        all_info['val_acc'].append(val_acc)\n",
        "        torch.save(g.cpu(), osp.join(exp_dir, 'predictor-%d.pt' % seed))\n",
        "        torch.save(att_net.cpu(), osp.join(exp_dir, 'attention_net-%d.pt' % seed))\n",
        "        logger.info(\"=\" * 100)\n",
        "\n",
        "    logger.info(\"Causal ACC:{:.4f}±{:.4f}  Conf ACC:{:.4f}±{:.4f}  Train ACC:{:.4f}±{:.4f}  Val ACC:{:.4f}±{:.4f}  \".format(\n",
        "                    torch.tensor(all_info['causal_acc']).mean(), torch.tensor(all_info['causal_acc']).std(),\n",
        "                    torch.tensor(all_info['conf_acc']).mean(), torch.tensor(all_info['conf_acc']).std(),\n",
        "                    torch.tensor(all_info['train_acc']).mean(), torch.tensor(all_info['train_acc']).std(),\n",
        "                    torch.tensor(all_info['val_acc']).mean(), torch.tensor(all_info['val_acc']).std()\n",
        "                ))\n",
        "            "
      ],
      "metadata": {
        "id": "Rw5yB2Ky2Spl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Shortcut"
      ],
      "metadata": {
        "id": "KvTO5tH_26Mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path as osp\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import ModuleList\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Sequential as Seq, ReLU, Tanh, Linear as Lin, Softmax\n",
        "from torch_geometric.nn import GraphConv, BatchNorm, global_max_pool\n",
        "\n",
        "class MNISTSPNet(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, hid_channels=32, num_classes=10, conv_unit=2):\n",
        "        super(MNISTSPNet, self).__init__()\n",
        "\n",
        "        self.node_emb = Lin(in_channels, hid_channels)\n",
        "\n",
        "        self.convs = ModuleList()\n",
        "        self.batch_norms = ModuleList()\n",
        "        self.relus = ModuleList()\n",
        "\n",
        "        for i in range(conv_unit):\n",
        "            conv = GraphConv(in_channels=hid_channels, out_channels=hid_channels)\n",
        "            self.convs.append(conv)\n",
        "            self.batch_norms.append(BatchNorm(hid_channels))\n",
        "            self.relus.append(ReLU())\n",
        "\n",
        "        self.causal_mlp = nn.Sequential(\n",
        "            nn.Linear(hid_channels, 2*hid_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2*hid_channels, num_classes)\n",
        "        )\n",
        "        \n",
        "        self.conf_mlp = torch.nn.Sequential(\n",
        "            nn.Linear(hid_channels, 2*hid_channels),\n",
        "            ReLU(),\n",
        "            nn.Linear(2*hid_channels, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, batch):\n",
        "        node_x = self.get_node_reps(x, edge_index, edge_attr, batch)\n",
        "        graph_x = global_max_pool(node_x, batch)\n",
        "        return self.get_causal_pred(graph_x)\n",
        "    \n",
        "    def get_node_reps(self, x, edge_index, edge_attr, batch):\n",
        "        edge_weight = edge_attr.view(-1)\n",
        "        x = self.node_emb(x)\n",
        "        for conv, batch_norm, ReLU in \\\n",
        "                zip(self.convs, self.batch_norms, self.relus):\n",
        "            x = conv(x, edge_index, edge_weight=edge_weight)\n",
        "            x = ReLU(batch_norm(x))\n",
        "        node_x = x\n",
        "        return node_x\n",
        "    \n",
        "    def get_graph_rep(self, x, edge_index, edge_attr, batch):\n",
        "\n",
        "        node_x = self.get_node_reps(x, edge_index, edge_attr, batch)\n",
        "        graph_x = global_max_pool(node_x, batch)\n",
        "        return graph_x\n",
        "\n",
        "    def get_causal_pred(self, causal_graph_x):\n",
        "        pred = self.causal_mlp(causal_graph_x)\n",
        "        return pred\n",
        "\n",
        "    def get_conf_pred(self, conf_graph_x):\n",
        "        pred = self.conf_mlp(conf_graph_x)\n",
        "        return pred\n",
        "\n",
        "    def get_comb_pred(self, causal_graph_x, conf_graph_x):\n",
        "        causal_pred = self.causal_mlp(causal_graph_x)\n",
        "        conf_pred = self.conf_mlp(conf_graph_x).detach()\n",
        "        return torch.sigmoid(conf_pred) * causal_pred"
      ],
      "metadata": {
        "id": "7IkazQi22vx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# get_subgraph"
      ],
      "metadata": {
        "id": "ynaYVct83JhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "import math\n",
        "import numpy as np\n",
        "from torch_geometric.utils import (remove_self_loops, degree, \n",
        "                                   batched_negative_sampling)\n",
        "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
        "\n",
        "MAX_DIAM=100\n",
        "\n",
        "\n",
        "def get_neg_edge_index(g):\n",
        "    neg_edge_index = batched_negative_sampling(edge_index=g.edge_index,\n",
        "                                               batch=g.batch,\n",
        "                                               num_neg_samples=None,\n",
        "                                               force_undirected=False)\n",
        "    neg_edge_index, _ = remove_self_loops(neg_edge_index)\n",
        "    return neg_edge_index\n",
        "\n",
        "\n",
        "def split_batch(g):\n",
        "    split = degree(g.batch[g.edge_index[0]], dtype=torch.long).tolist()\n",
        "    edge_indices = torch.split(g.edge_index, split, dim=1)\n",
        "    num_nodes = degree(g.batch, dtype=torch.long)\n",
        "    cum_nodes = torch.cat([g.batch.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]])\n",
        "    num_edges = torch.tensor([e.size(1) for e in edge_indices], dtype=torch.long).to(g.x.device)\n",
        "    cum_edges = torch.cat([g.batch.new_zeros(1), num_edges.cumsum(dim=0)[:-1]])\n",
        "\n",
        "    return edge_indices, num_nodes, cum_nodes, num_edges, cum_edges\n",
        "\n",
        "        \n",
        "def split_graph(data, edge_score, ratio):\n",
        "    causal_edge_index = torch.LongTensor([[],[]]).to(data.x.device)\n",
        "    causal_edge_weight = torch.tensor([]).to(data.x.device)\n",
        "    causal_edge_attr = torch.tensor([]).to(data.x.device)\n",
        "    conf_edge_index = torch.LongTensor([[],[]]).to(data.x.device)\n",
        "    conf_edge_weight = torch.tensor([]).to(data.x.device)\n",
        "    conf_edge_attr = torch.tensor([]).to(data.x.device)\n",
        "\n",
        "    edge_indices, _, _, num_edges, cum_edges = split_batch(data)\n",
        "    for edge_index, N, C in zip(edge_indices, num_edges, cum_edges):\n",
        "        n_reserve =  int(ratio * N)\n",
        "        edge_attr = data.edge_attr[C:C+N]\n",
        "        single_mask = edge_score[C:C+N]\n",
        "        single_mask_detach = edge_score[C:C+N].detach().cpu().numpy()\n",
        "        rank = np.argpartition(-single_mask_detach, n_reserve)\n",
        "        idx_reserve, idx_drop = rank[:n_reserve], rank[n_reserve:]\n",
        "\n",
        "        causal_edge_index = torch.cat([causal_edge_index, edge_index[:, idx_reserve]], dim=1)\n",
        "        conf_edge_index = torch.cat([conf_edge_index, edge_index[:, idx_drop]], dim=1)\n",
        "\n",
        "        causal_edge_weight = torch.cat([causal_edge_weight, single_mask[idx_reserve]])\n",
        "        conf_edge_weight = torch.cat([conf_edge_weight,  -1 * single_mask[idx_drop]])\n",
        "\n",
        "        causal_edge_attr = torch.cat([causal_edge_attr, edge_attr[idx_reserve]])\n",
        "        conf_edge_attr = torch.cat([conf_edge_attr, edge_attr[idx_drop]])\n",
        "    return (causal_edge_index, causal_edge_attr, causal_edge_weight), \\\n",
        "        (conf_edge_index, conf_edge_attr, conf_edge_weight)\n",
        "        \n",
        "        \n",
        "def bool_vec(length, r_True, shuffle=True):\n",
        "    n_True = math.ceil(length * r_True)\n",
        "    n_False = length - n_True\n",
        "    vec = np.concatenate([np.zeros(n_False, dtype=np.bool), np.ones(n_True, dtype=np.bool)])\n",
        "    if shuffle:\n",
        "        np.random.shuffle(vec)\n",
        "\n",
        "    return vec\n",
        "\n",
        "\n",
        "def sample(dataset, ratio):\n",
        "    reserve = bool_vec(len(dataset), ratio)\n",
        "    reserve = torch.tensor(reserve).bool()\n",
        "    return dataset[reserve]\n",
        "\n",
        "\n",
        "def relabel(x, edge_index, batch, pos=None):\n",
        "        \n",
        "    num_nodes = x.size(0)\n",
        "    sub_nodes = torch.unique(edge_index)\n",
        "    x = x[sub_nodes]\n",
        "    batch = batch[sub_nodes]\n",
        "    row, col = edge_index\n",
        "    # remapping the nodes in the explanatory subgraph to new ids.\n",
        "    node_idx = row.new_full((num_nodes,), -1)\n",
        "    node_idx[sub_nodes] = torch.arange(sub_nodes.size(0), device=row.device)\n",
        "    edge_index = node_idx[edge_index]\n",
        "    if pos is not None:\n",
        "        pos = pos[sub_nodes]\n",
        "    return x, edge_index, batch, pos\n",
        "\n",
        "\n",
        "def get_broken_graph(g, broken_ratio, connectivity=True):\n",
        "\n",
        "    edge_indices, num_nodes, cum_nodes, num_edges, _ = split_batch(g)\n",
        "    out_edge_ratio = []\n",
        "    broken_masks = []\n",
        "    for edge_index, N, C, E in zip(edge_indices, num_nodes.tolist(),\n",
        "                                cum_nodes.tolist(), num_edges.tolist()):\n",
        "        if connectivity:\n",
        "            flag = 0\n",
        "            node_idx = np.random.choice([i for i in range(N)])\n",
        "            node_idx = torch.tensor([node_idx])\n",
        "            num_edges = int(broken_ratio * E)\n",
        "            for num_hops in range(1, MAX_DIAM):\n",
        "                _, _, _, broken_mask = bid_k_hop_subgraph(\n",
        "                    node_idx=node_idx, \n",
        "                    num_hops=num_hops, \n",
        "                    edge_index=edge_index-C,\n",
        "                    num_nodes=N)\n",
        "                if broken_mask.sum() >= num_edges:\n",
        "                    flag = 1\n",
        "                    break\n",
        "            if flag == 0:\n",
        "                print(\"ERROR!\")\n",
        "        else:\n",
        "            broken_mask = bool_vec(E, r_True=broken_ratio, shuffle=True)\n",
        "            broken_mask = torch.tensor(broken_mask, dtype=torch.float)\n",
        "        \n",
        "        broken_masks.append(broken_mask)\n",
        "        out_edge_ratio.append(broken_mask.sum().float()/E)\n",
        "    broken_masks = torch.cat(broken_masks, dim=0).bool()\n",
        "    broken_edge_index = g.edge_index[:, broken_masks]\n",
        "    broken_edge_attr = g.edge_attr[broken_masks]\n",
        "    out_edge_ratio = torch.tensor(out_edge_ratio).to(g.x.device)\n",
        "\n",
        "    return broken_edge_index, broken_edge_attr, out_edge_ratio\n",
        "\n",
        "\n",
        "# Bidirectional k-hop subgraph\n",
        "# adapted from torch-geometric.utils.subgraph\n",
        "def bid_k_hop_subgraph(node_idx, num_hops, edge_index, relabel_nodes=False,\n",
        "                   num_nodes=None):\n",
        "    r\"\"\"Computes the :math:`k`-hop subgraph of :obj:`edge_index` around node\n",
        "    :attr:`node_idx`.\n",
        "    It returns (1) the nodes involved in the subgraph, (2) the filtered\n",
        "    :obj:`edge_index` connectivity, (3) the mapping from node indices in\n",
        "    :obj:`node_idx` to their new location, and (4) the edge mask indicating\n",
        "    which edges were preserved.\n",
        "\n",
        "    Args:\n",
        "        node_idx (int, list, tuple or :obj:`torch.Tensor`): The central\n",
        "            node(s).\n",
        "        num_hops: (int): The number of hops :math:`k`.\n",
        "        edge_index (LongTensor): The edge indices.\n",
        "        relabel_nodes (bool, optional): If set to :obj:`True`, the resulting\n",
        "            :obj:`edge_index` will be relabeled to hold consecutive indices\n",
        "            starting from zero. (default: :obj:`False`)\n",
        "        num_nodes (int, optional): The number of nodes, *i.e.*\n",
        "            :obj:`max_val + 1` of :attr:`edge_index`. (default: :obj:`None`)\n",
        "        flow (string, optional): The flow direction of :math:`k`-hop\n",
        "            aggregation (:obj:`\"source_to_target\"` or\n",
        "            :obj:`\"target_to_source\"`). (default: :obj:`\"source_to_target\"`)\n",
        "\n",
        "    :rtype: (:class:`LongTensor`, :class:`LongTensor`, :class:`LongTensor`,\n",
        "             :class:`BoolTensor`)\n",
        "    \"\"\"\n",
        "\n",
        "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
        "    row, col = edge_index\n",
        "\n",
        "    node_mask = row.new_empty(num_nodes, dtype=torch.bool)\n",
        "    edge_mask = row.new_empty(row.size(0), dtype=torch.bool)\n",
        "\n",
        "    if isinstance(node_idx, (int, list, tuple)):\n",
        "        node_idx = torch.tensor([node_idx], device=row.device).flatten()\n",
        "    else:\n",
        "        node_idx = node_idx.to(row.device)\n",
        "\n",
        "    subsets = [node_idx]\n",
        "\n",
        "    for _ in range(num_hops):\n",
        "        node_mask.fill_(False)\n",
        "        node_mask[subsets[-1]] = True\n",
        "        if len(subsets) > 1:\n",
        "            node_mask[subsets[-2]] = True\n",
        "        edge_mask1 = torch.index_select(node_mask, 0, row)\n",
        "        edge_mask2 = torch.index_select(node_mask, 0, col)\n",
        "        subsets.append(col[edge_mask1])\n",
        "        subsets.append(row[edge_mask2])\n",
        "\n",
        "    subset, inv = torch.cat(subsets).unique(return_inverse=True)\n",
        "    inv = inv[:node_idx.numel()]\n",
        "\n",
        "    node_mask.fill_(False)\n",
        "    node_mask[subset] = True\n",
        "    edge_mask = node_mask[row] & node_mask[col]\n",
        "\n",
        "    edge_index = edge_index[:, edge_mask]\n",
        "\n",
        "    if relabel_nodes:\n",
        "        node_idx = row.new_full((num_nodes, ), -1)\n",
        "        node_idx[subset] = torch.arange(subset.size(0), device=row.device)\n",
        "        edge_index = node_idx[edge_index]\n",
        "\n",
        "    return subset, edge_index, inv, edge_mask\n",
        "\n",
        "\n",
        "def get_syn_ground_truth_graph(g):\n",
        "\n",
        "    _, _, _, num_edges, cum_edges = split_batch(g)\n",
        "    \n",
        "    nodel_label = np.concatenate(g.z, axis=0)\n",
        "    row, col = g.edge_index.detach().cpu().numpy()\n",
        "    broken_mask = torch.tensor(nodel_label[row] * nodel_label[col] > 0, dtype=torch.bool)\n",
        "    broken_edge_index = g.edge_index[:, broken_mask]\n",
        "    broken_edge_attr = g.edge_attr[broken_mask]\n",
        "    out_edge_ratio = []\n",
        "    for E, C in zip(num_edges.tolist(), cum_edges.tolist()):\n",
        "        out_edge_ratio.append(broken_mask[C: C + E].sum().float()/E)\n",
        "    \n",
        "    out_edge_ratio = torch.tensor(out_edge_ratio).to(g.x.device)\n",
        "    return broken_edge_index, broken_edge_attr, out_edge_ratio\n",
        "\n",
        "\n",
        "def get_single_ground_truth_graph(g):\n",
        "\n",
        "    _, _, _, num_edges, cum_edges = split_batch(g)\n",
        "    nodel_label = np.concatenate(g.z, axis=0)\n",
        "    row, col = g.edge_index.detach().cpu().numpy()\n",
        "    broken_mask = torch.tensor(nodel_label[row] * nodel_label[col] > 0, dtype=torch.bool)\n",
        "    \n",
        "    broken_edge_indices = torch.LongTensor([[],[]]).to(g.x.device)\n",
        "    broken_edge_attrs = torch.LongTensor([]).to(g.x.device)\n",
        "    out_edge_ratio = []\n",
        "    for E, C in zip(num_edges.tolist(), cum_edges.tolist()):\n",
        "        edge_idx = torch.nonzero(broken_mask[C: C + E]).view(-1) + C\n",
        "        edge_index = g.edge_index[:, edge_idx]\n",
        "        node_idx = np.random.choice(np.unique(edge_index.detach().cpu().numpy()))\n",
        "        node_idx = torch.tensor([node_idx]).to(g.x.device)\n",
        "        _, broken_edge_index, _, edge_mask = bid_k_hop_subgraph(node_idx, num_hops=5, edge_index=edge_index)\n",
        "        broken_edge_attr = g.edge_attr[C: C + E][edge_idx - C][edge_mask]\n",
        "        broken_edge_indices = torch.cat([broken_edge_indices, broken_edge_index], dim=1)\n",
        "        broken_edge_attrs = torch.cat([broken_edge_attrs, broken_edge_attr], dim=0)\n",
        "        out_edge_ratio.append(float(broken_edge_index.size(1)) / E)\n",
        "        \n",
        "    out_edge_ratio = torch.tensor(out_edge_ratio).to(g.x.device)\n",
        "    return broken_edge_indices, broken_edge_attrs, out_edge_ratio\n",
        "\n",
        "\n",
        "def get_mnist_ground_truth_graph(g):\n",
        "    \n",
        "    _, _, _, num_edges, cum_edges = split_batch(g)\n",
        "    \n",
        "    nodel_label = torch.tensor(g.x.view(-1) > 0, dtype=torch.bool)\n",
        "   \n",
        "    row, col = g.edge_index.detach().cpu().numpy()\n",
        "    broken_mask = torch.tensor(nodel_label[row] * nodel_label[col] > 0, dtype=torch.bool)\n",
        "    broken_edge_index = g.edge_index[:, broken_mask]\n",
        "    broken_edge_attr = g.edge_attr[broken_mask]\n",
        "    out_edge_ratio = []\n",
        "    for E, C in zip(num_edges.tolist(), cum_edges.tolist()):\n",
        "        out_edge_ratio.append(broken_mask[C: C + E].sum().float()/E)\n",
        "    \n",
        "    out_edge_ratio = torch.tensor(out_edge_ratio).to(g.x.device)\n",
        "    return broken_edge_index, broken_edge_attr, out_edge_ratio\n",
        "\n",
        "\n",
        "def get_ground_truth_graph(args, g):\n",
        "    if args.dataset == 'ba3':\n",
        "        return get_single_ground_truth_graph(g)\n",
        "    elif args.dataset == 'tr3':\n",
        "        return get_syn_ground_truth_graph(g)\n",
        "    elif args.dataset == 'mnist':\n",
        "        return get_mnist_ground_truth_graph(g)"
      ],
      "metadata": {
        "id": "MgLbE3Yh3HH6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DIR_GNN_Demo.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}